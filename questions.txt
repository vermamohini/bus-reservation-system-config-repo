Core Java 
interface and abstarct class difference

marker interface in java
It is an empty interface (no field or methods). Examples of marker interface are Serializable, Cloneable and Remote interface. Cloneable interface : Cloneable interface is present in java.lang package. There is a method clone() in Object class. A class that implements the Cloneable interface indicates that it is legal for clone() method to make a field-for-field copy of instances of that class. 
Invoking Object’s clone method on an instance of the class that does not implement the Cloneable interface results in an exception CloneNotSupportedException being thrown. By convention, classes that implement this interface should override Object.clone() method. 

 What is an immutable object - An object is considered immutable if its state cannot change after it is constructed.
 
 how to create an immutable class in java?
 you have a file ..you have to find out the most occuring word in that, how to do that
 
difference between checked and unchecked Exception in Java.
 checked exception is checked by the compiler at compile time. It's mandatory for a method to either handle the checked exception or declare them in their throws clause. These are the ones which are a sub class of Exception but doesn't descend from RuntimeException. The unchecked exception is the descendant of RuntimeException and not checked by the compiler at compile time.

Read more: https://javarevisited.blogspot.com/2015/10/133-java-interview-questions-answers-from-last-5-years.html#ixzz69ZWJUgBS

 
Difference between throw and throws in Java
Main difference between these two is that one declares exception thrown by a Java method while other is actually used to throw Exception

difference between Vector and ArrayList in Java?
ArrayList is not synchronized its fast compare to Vector

the difference between stack and heap in Java? (answer)
Stack and heap are different memory areas in the JVM and they are used for different purposes. The stack is used to hold method frames and local variables while objects are always allocated memory from the heap. The stack is usually much smaller than heap memory and also didn't shared between multiple threads, but heap is shared among all threads in JVM.
The static variables were stored in the permgen space(also called the method area).
The static variables are stored in the Heap itself.From Java 8 onwards the PermGen Space have been removed and new space named as MetaSpace is introduced which is not the part of Heap any more unlike the previous Permgen Space. 

Read more: https://javarevisited.blogspot.com/2015/10/133-java-interview-questions-answers-from-last-5-years.html#ixzz69ZVPKbPR

Difference between final, finalize and finally? (answer)
The final is a modifier which you can apply to variable, methods and classes. If you make a variable final it means its value cannot be changed once initialized. finalize is a method, which is called just before an object is a garbage collected, giving it last chance to resurrect itself, but the call to finalize is not guaranteed. finally is a keyword which is used in exception handling along with try and catch. the finally block is always executed irrespective of whether an exception is thrown from try block or not.

Read more: https://javarevisited.blogspot.com/2015/10/133-java-interview-questions-answers-from-last-5-years.html#ixzz69ZVZxmA4

difference between Comparator and Comparable in Java? (answer)
The Comparable interface is used to define the  natural order of object while Comparator is used to define custom order. Comparable can be always one, but we can have multiple comparators to define customized order for objects.

Read more: https://javarevisited.blogspot.com/2015/10/133-java-interview-questions-answers-from-last-5-years.html#ixzz69ZVrnTNl

How can we restrict inheritance for a class
can we execute any code, even before the main method?
break out of nested loops in Java

Is it possible to import same package or class twice? Will the JVM load the package twice at runtime?
A:  It is possible to import the same package or class more than one time. Also, it won’t have any effect on compiler or JVM. JVM will load the class for one time only, irrespective of the number of times you import the same class.

Have you worked on Java 8? Can you share major changes in Java 8?
If you have worked on Java 8, you can share major changes such Stream, lambada expression, defaults method in interface etc.

How to make Hashmap thread safe in Java?
When you make map thread safe by using Collection.synchronizedMap(map), it locks whole map object, but ConcurrentHashMap does not lock the whole map, it just locks part of it(Segment).
You can read more about ConcurrentHashMap over here.

If I try to add Enum constants to a TreeSet, What sorting order will it use ?
Ans. Tree Set will sort the Values in the order in which Enum constants are declared.

What if the main() method is declared as private? What happens when the static modifier is removed from the signature of the main() method?
When the main() method is declared as private, the program compiles but during runtime it shows “main() method not public.” Message. When the static modifier is removed from the signature of the main() method, the Program compiles but at runtime throws an error “NoSuchMethodError”.

How do you share an object between threads? or How to pass an object from one thread to another? 
There are multiple ways to do that e.g. Queues, Exchanger etc, but BlockingQueue using Producer Consumer pattern is the easiest way to pass an object from thread to another.

what is a join method in multithreading?
Join is a synchronization method that blocks the calling thread (that is, the thread that calls the method) until the thread whose Join method is called has completed.

What's the difference between map() and flatMap() methods in Java 8?
Both map() and flatMap() takes a mapping function which is applied to each element of a Stream<T>, and returns a Stream<R>. The only difference is that the mapping function in the case of flatMap() produces a stream of new values, whereas for map() it produces a single value for each input element. Arrays.
https://www.geeksforgeeks.org/difference-between-map-and-flatmap-in-java-stream/

How does ConcurrentHashMap achieves its Scalability? 
Sometimes this multithreading + collection interview question is also asked as, the difference between ConcurrentHashMap and Hashtable in Java. The problem with synchronized HashMap or Hashtable was that whole Map is locked when a thread performs any operation with Map. 
The java.util.ConcurrentHashMap class solves this problem by using lock stripping technique, where the whole map is locked at different segments and only a particular segment is locked during the write operation, not the whole map. The ConcurrentHashMap also achieves it's scalability by allowing lock-free reads as read is a thread-safe operation.  See here for more advanced multi-threading and concurrency questions in Java. 


How  do you prevent SQL Injection in Java Code?
This question is more asked to J2EE and Java EE developers than core Java developers, but, it is still a good question to check the JDBC and Security skill of experienced Java programmers.
You can use PreparedStatement to avoid SQL injection in Java code. Use of the PreparedStatement for executing SQL queries not only provides better performance but also shield your Java and J2EE application from SQL Injection attack. 
On a similar note, If you are working more on Java EE or J2EE side, then you should also be familiar with other security issues including Session Fixation attack (https://www.linkedin.com/advice/0/what-best-practices-prevent-session-fixation-attacks) or Cross Site Scripting attack (https://snyk.io/blog/preventing-xss-snyk-code/ using html esacping and input validation like blacklisting some chars, using html escape and encode etc. here In XSS, the attacker tries to execute malicious code in a web application browser) and how to resolve them. These are some fields and questions where a good answer can make a lot of difference on your selection. 


Can we override static method in Java
No, you cannot override static method in Java because method overriding is based upon dynamic binding at runtime and static methods are bonded using static binding at compile time. Though you can declare a method with same name and method signature in sub class which does look like you can override static method in Java but in reality that is method hiding. Java won't resolve method call at runtime and depending upon type of Object which is used to call static method, corresponding method will be called. It means if you use Parent class's type to call static method, original static will be called from patent class, on ther other hand if you use Child class's type to call static method, method from child class will be called. In short you can not override static method in Java. If you use Java IDE like Eclipse or Netbeans, they will show warning that static method should be called using class name and not by using object becaues static method can not be overridden in Java.

If one needs a Set, how do you choose between HashSet vs. TreeSet?If one needs a Set, how do you choose between HashSet vs. TreeSet?
At first glance, HashSet is superior in almost every way: O(1) add, remove and contains, vs. O(log(N)) for TreeSet.

However, TreeSet is indispensable when you wish to maintain order over the inserted elements or query for a range of elements within the set.

Consider a Set of timestamped Event objects. They could be stored in a HashSet, with equals and hashCode based on that timestamp. This is efficient storage and permits looking up events by a specific timestamp, but how would you get all events that happened on any given day? That would require a O(n) traversal of the HashSet, but it’s only a O(log(n)) operation with TreeSet using the tailSet method:

        public class Event implements Comparable<Event> {
            private final long timestamp;
    
            public Event(long timestamp) {
                this.timestamp = timestamp;
            }
    
            @Override public int compareTo(Event that) {
                return Long.compare(this.timestamp, that.timestamp);
            }
        }
       
        ...
	
        SortedSet<Event> events = new TreeSet<>();
        events.addAll(...); // events come in

        // all events that happened today
        long midnightToday = ...;
        events.tailSet(new Event(midnightToday));

If Event happens to be a class that we cannot extend or that doesn’t implement Comparable, TreeSet allows us to pass in our own Comparator:

        SortedSet<Event> events = new TreeSet<>(
                (left, right) -> Long.compare(left.timestamp, right.timestamp));

Generally speaking, TreeSet is a good choice when order matters and when reads are balanced against the increased cost of writes.

What are method references, and how are they useful?
Method references were introduced in Java 8 and allow constructors and methods (static or otherwise) to be used as lambdas. They allow one to discard the boilerplate of a lambda when the method reference matches an expected signature.

For example, suppose we have a service that must be stopped by a shutdown hook. Before Java 8, we would have code like this:

        final SomeBusyService service = new SomeBusyService();
        service.start();

        onShutdown(new Runnable() {
            @Override
            public void run() {
                service.stop();
            }
        });

With lambdas, this can be cut down considerably:

        onShutdown(() -> service.stop());

However, stop matches the signature of Runnable.run (void return type, no parameters), and so we can introduce a method reference to the stop method of that specific SomeBusyService instance:

        onShutdown(service::stop);

This is terse (as opposed to verbose code) and clearly communicates what is going on.

Method references don’t need to be tied to a specific instance, either; one can also use a method reference to an arbitrary object, which is useful in Stream operations. For example, suppose we have a Person class and want just the lowercase names of a collection of people:

        List<Person> people = ...

        List<String> names = people.stream()
                .map(Person::getName)
                .map(String::toLowerCase)
                .collect(toList());

A complex lambda can also be pushed into a static or instance method and then used via a method reference instead. This makes the code more reusable and testable than if it were “trapped” in the lambda.

So we can see that method references are mainly used to improve code organization, clarity and terseness.

what is the difference between import and static import
The import allows the java programmer to access classes of a package without package qualification whereas the static import feature allows to access the static members of a class without the class qualification

table and child table, you want to update the id in parent table, how to do that
diff between primary key and unique key


What is difference between checked and unchecked Exception in Java.
Difference between String, String Builder, and String Buffer.
what is Default access specifier in java?
transient variable in Java

difference between Vector vs ArrayList is that Vector is synchronized and thread-safe while ArrayList is neither Synchronized nor thread-safe. Now, What does that mean? It means if multiple thread try to access Vector same time they can do that without compromising Vector's internal state. Same is not true in case of ArrayList as methods like add(), remove() or get() is not synchronized.

Please explain the various types of garbage collectors in Java?
Answer: The Java programming language has four types of garbage collectors:

Serial Garbage Collector– Using only a single thread for garbage collection, the serial garbage collector works by holding all the application threads. It is designed especially for single-threaded environments. Because serial garbage collector freezes all application threads while performing garbage collection, it is most suitable for command-line programs only. For using the serial garbage collector, one needs to turn on the -XX:+UseSerialGC JVM argument.
Parallel Garbage Collector – Also known as the throughput collector, the parallel garbage collector is the default garbage collector of the JVM. It uses multiple threads for garbage collection and like serial garbage collector freezes all application threads during garbage collection.
CMS Garbage Collector– Short for Concurrent Mark Sweep, CMS garbage collector uses multiple threads for scanning the heap memory for marking instances for eviction, followed by sweeping the marked instances. There are only two scenarios when the CMS garbage collector holds all the application threads:
When marking the referenced objects in the tenured generation space
If there is some change in the heap memory while performing the garbage collection CMS garbage collector ensures better application throughput over parallel garbage collector by using more CPU. For using the CMS garbage collector, the XX:+USeParNewGC JVM argument needs to be turned on.
G1 Garbage Collector – Used for large heap memory areas, G1 garbage collector works by separating the heap memory into multiple regions and then executing garbage collection in them in parallel. Unlike the CMS garbage collector that compacts the memory on STW (Stop The World) situations, G1 garbage collector compacts the free heap space right after reclaiming the memory. Also, the G1 garbage collector prioritizes the region with the most garbage. Turning on the –XX:+UseG1GC JVM argument is required for using the G1 garbage collector.

How will you differentiate HashMap from HashTable?
Answer: HashMap in Java is a Map-based collection class, used for storing key & value pairs. It is denoted as HashMap<Key, Value> or HashMap<K, V> HashTable is an array of a list, where each list is called a bucket. Values contained in a HashTable are unique and depend on the key. Methods are not synchronized in HashMap while key methods are synchronized in HashTable. However, HashMap doesn’t have thread safety while HashTable has the same. For iterating values, HashMap uses iterator and HashTable uses enumerator. HashTable doesn’t allow anything that is null while HashMap allows one null key and several null values. In terms of performance, HashTable is slow. Comparatively, HashMap is faster.

What if the public static void is replaced by static public void, will the program still run?
Answer: Yes, the program would compile and run without any errors as the order of the specifiers don’t matter.

What is a finally block? Is there a case when finally will not execute?
Finally block is a block which always executes a set of statements. It is always associated with a try block regardless of any exception that occurs or not. 
Yes, finally will not be executed if the program exits either by calling System.exit() or by causing a fatal error that causes the process to abort.

What is singleton class in Java and how can we make a class singleton?
Singleton class is a class whose only one instance can be created at any given time, in one JVM. A class can be made singleton by making its constructor private.


How to Create Object without using the keyword “new” in java?

Without new, the Factory methods are used to create objects for a class. For example
Calender c=Calender.getInstance();
Here Calender is a class, and the method getInstance() is a Factory method which can create an object for Calendar class.

What is difference between NoClassDefFoundError and ClassNotFoundException in Java?
Though both of these errors are related to missing classes in the classpath, the main difference between them is their root cause. ClassNotFoundExcpetion comes when you try to load a class at runtime by using Class.forName() or loadClass() and requested class is not present in classpath for example when you try to load MySQL or Oracle driver class and their JAR is not available, while in case of NoClassDefFoundError requested class was present at compile time but not available at runtime. Sometimes due to an exception during class initialization e.g. exception from static block causes NoClassDefFoundError when a failed-to-load class was later referenced by the runtime. 
ClassNotFoundException is a checked Exception derived directly from java.lang.Exception class and you need to provide explicit handling for it while NoClassDefFoundError is an Error derived from LinkageError

What is try-with-resources in java?
One of the Java 7 features is the try-with-resources statement for automatic resource management. Before Java 7, there was no auto resource management and we should explicitly close the resource. Usually, it was done in the finally block of a try-catch statement. This approach used to cause memory leaks when we forgot to close the resource.

From Java 7, we can create resources inside try block and use it. Java takes care of closing it as soon as try-catch block gets finished



***********************************************************************
SQL
***********************************************************************

What is the use of NVL function?
The NVL function is used to replace NULL values with another or given value. Example is –o(Value, replace value)

What is COALESCE function?

COALESCE function is used to return the value which is set to be not null in the list. If all values in the list are null, then the coalesce function will return NULL.

Coalesce(value1, value2,value3,…)

How can we view last record added to a table?

Last record can be added to a table and this can be done by –

Select * from (select * from employees order by rownum desc) where rownum<2;
1
Select * from (select * from employees order by rownum desc) where rownum<2;
 

48. What is the data type of DUAL table?

The DUAL table is a one-column table present in oracle database.  The table has a single VARCHAR2(1) column called DUMMY which has a value of ‘X’.



having, group by, order by
query to find if the date is the last date of the month
first date of the month
difference between primary key and unique key
order by, group by and having clause sequence

What is the difference between DELETE and TRUNCATE commands?
DELETE command is used to remove rows from the table, and WHERE clause can be used for conditional set of parameters. Commit and Rollback can be performed after delete statement.
TRUNCATE removes all rows from the table. Truncate operation cannot be rolled back.

What is the difference between TRUNCATE and DROP statements?
TRUNCATE removes all the rows from the table, and it cannot be rolled back. DROP command removes a table from the database and operation cannot be rolled back.


What is a constraint?
Constraint can be used to specify the limit on the data type of table. Constraint can be specified while creating or altering the table statement. Sample of constraint are.
NOT NULL.
CHECK.
DEFAULT.
UNIQUE.
PRIMARY KEY.
FOREIGN KEY.

How to select unique records from a table?
Select unique records from a table by using DISTINCT keyword.
Select DISTINCT StudentID, StudentName from Student.

Difference between UNION and UNION ALL
The UNION operator combines and returns the result-set retrieved by two or more SELECT statements.
SELECT name FROM Students 	 /* Fetch the union of queries */
UNION
SELECT name FROM Contacts;

SELECT name FROM Students 	 /* Fetch the union of queries with duplicates*/
UNION ALL
SELECT name FROM Contacts;

What is the difference between CHAR and VARCHAR2 datatype in SQL?
Both Char and Varchar2 are used for characters datatype but varchar2 is used for character strings of variable length whereas Char is used for strings of fixed length. For example, char(10) can only store 10 characters and will not be able to store a string of any other length whereas varchar2(10) can store any length i.e 6,8,2 in this variable.

How can you insert NULL values in a column while inserting the data?
NULL values can be inserted in the following ways:
Implicitly by omitting column from column list.
Explicitly by specifying NULL keyword in the VALUES clause

What is the need of MERGE statement?
This statement allows conditional update or insertion of data into a table. It performs an UPDATE if a row exists, or an INSERT if the row does not exist.


***********************************************************************
Hibernate
***********************************************************************
What is cascading and how to have cascading in hibernate what are different types of cascading?
what is lazy and eager loading

What are different states of an entity bean?
An element bean occurrence can exist is one of the three states.
Transient: When a object is never continued or related with any session, it’s in transient state. Transient cases might be influenced diligent by calling save(), persist() or saveOrUpdate(). Steady examples might be influenced transient by calling to delete().
Persistent: When a question is related with a one of a kind session, it’s in persevering state. Any occurrence returned by a get() or load() technique is persevering.
Detached: When a object is already relentless yet not related with any session, it’s in disengaged state. Disengaged occasions might be made determined by calling refresh(), saveOrUpdate(), lock() or replicate(). The condition of a transient or disengaged example may likewise be influenced relentless as another constant case by calling to merge().

Is it possible to connect multiple database in a single Java application using Hibernate?
Yes. Practically it is possible to connect a single Java application to multiple databases using two separate hibernate configuration files and two separate session factories. These hibernate configuration files contain different dialect configuration pertaining to the respective database. The entities are exclusively mapped to the relevant database configuration. Thus, with two different parallel SessionFactory objects, it is possible to have multiple databases connected.

What is lazy loading?
Lazy loading is defined as a technique in which objects are loaded on an on-demand basis. It has been enabled by default since the advent of Hibernate 3 to ensure that child objects are not loaded when the parent is.

Difference between first level and second level cache in hibernate
1. First level cache is enabled by default whereas Second level cache needs to be enabled explicitly.
2. First level Cache came with Hibernate 1.0 whereas Second level cache came with Hibernate 3.0.
3. First level Cache is Session specific whereas Second level cache is shared by sessions that is why First level cache is considered local and second level cache is considered global.

What are different types of second level cache ?
1. EHCache ( Easy Hibernate )
2. OSCache  ( Open Symphony )
3. Swarm Cache ( JBoss )
4. Tree Cache ( JBoss ) 

How can we see hibernate generated SQL on console?
We need to add following in hibernate configuration file to enable viewing SQL on the console for debugging purposes:

What’s the use of session.lock() in hibernate?
session.lock() method of session class is used to reattach an object which has been detached earlier. This method of reattaching doesn’t check for any data synchronization in database while reattaching the object and hence may lead to lack of synchronization in data.

What’s the difference between load() and get() method in hibernate?
Load() methods results in an exception if the required records isn’t found in the database while get() method returns null when records against the id isn’t found in the database.
So, ideally we should use Load() method only when we are sure about existence of records against an id.

42. What’s the use of version property in hibernate?
Version property is used in hibernate to know whether an object is in transient state or in detached state.

 What is meant by a Named SQL Query in hibernate and how it’s used?
Named SQL queries are those queries which are defined in mapping file and are called as required anywhere.
For example, we can write a SQL query in our XML mapping file as follows:

Then this query can be called as follows:
List students = session.getNamedQuery(&amp;amp;quot;studentdetails&amp;amp;quot;)
.setString(&amp;amp;quot;TomBrady&amp;amp;quot;, name)
.setMaxResults(50)
.list();
1
2
3
4
	
List students = session.getNamedQuery(&amp;amp;quot;studentdetails&amp;amp;quot;)
.setString(&amp;amp;quot;TomBrady&amp;amp;quot;, name)
.setMaxResults(50)
.list();

How can we reattach any detached objects in Hibernate?

Objects which have been detached and are no longer associated with any persistent entities can be reattached by calling session.merge() method of session class.
31. What are different ways to disable hibernate second level cache?

Hibernate second level cache can be disabled using any of the following ways:
a. By setting use_second_level_cache as false.
b. By using CACHEMODE.IGNORE
c. Using cache provider as org.hibernate.cache.NoCacheProvider

What is transient instance state in Hibernate?
If an instance is not associated with any persistent context and also, it has never been associated with any persistent context, then it’s said to be in transient state.

19. How can we reduce database write action times in Hibernate?
Hibernate provides dirty checking feature which can be used to reduce database write times. Dirty checking feature of hibernate updates only those fields which require a change while keeps others unchanged.

20. What’s the usage of callback interfaces in hibernate?
Callback interfaces of hibernate are useful in receiving event notifications from objects. For example, when an object is loaded or deleted, an event is generated and notification is sent using callback interfaces.

21. When an instance goes in detached state in hibernate?
When an instance was earlier associated with some persistent context (e.g. a table) and is no longer associated, it’s called to be in detached state.

***********************************************************************
JDBC
***********************************************************************
What is the difference between execute, executeQuery, executeUpdate?
Statement execute(String query) is used to execute any SQL query and it returns TRUE if the result is an ResultSet such as running Select queries. The output is FALSE when there is no ResultSet object such as running Insert or Update queries. We can use getResultSet() to get the ResultSet and getUpdateCount() method to retrieve the update count.

Statement executeQuery(String query) is used to execute Select queries and returns the ResultSet. ResultSet returned is never null even if there are no records matching the query. When executing select queries we should use executeQuery method so that if someone tries to execute insert/update statement it will throw java.sql.SQLException with message “executeQuery method can not be used for update”.

Statement executeUpdate(String query) is used to execute Insert/Update/Delete (DML) statements or DDL statements that returns nothing. The output is int and equals to the row count for SQL Data Manipulation Language (DML) statements. For DDL statements, the output is 0.

You should use execute() method only when you are not sure about the type of statement else use executeQuery or executeUpdate method.


Statement: Used for general purpose access to the database and executes a static SQL query at runtime.
PreparedStatement: Used to provide input parameters to the query during execution.
CallableStatement: Used to access the database stored procedures and helps in accepting runtime parameters.

***********************************************************************
Spring
***********************************************************************
What are different ways to configure a class as Spring Bean?
What are the minimum configurations needed to create Spring MVC application?
Can we have multiple Spring configuration files?
caching in spring mvc
When to use Setter and Constructor Injection in Dependency Injection pattern?

Which DI would you suggest Constructor-based or setter-based DI?
You can use both Constructor-based and Setter-based Dependency Injection. The best solution is using constructor arguments for mandatory dependencies and setters for optional dependencies.

what is method injection and when can it be used?
it is a another kind of injection used for dynamically overridding a class and its abstract methods to create instances every time the bean is injected
spring lookup-method annotation used for the method injection. It is one of the good feature to help when we inject the beans of different scopes.

Explain Bean in Spring and List the different Scopes of Spring bean.
Beans are objects that form the backbone of a Spring application. They are managed by the Spring IoC container. In other words, a bean is an object that is instantiated, assembled, and managed by a Spring IoC container.

There are five Scopes defined in Spring beans.

SpringBean - Java Interview Questions - Edureka

Singleton: Only one instance of the bean will be created for each container. This is the default scope for the spring beans. While using this scope, make sure spring bean doesn’t have shared instance variables otherwise it might lead to data inconsistency issues because it’s not thread-safe.
Prototype: A new instance will be created every time the bean is requested.
Request: This is same as prototype scope, however it’s meant to be used for web applications. A new instance of the bean will be created for each HTTP request.
Session: A new bean will be created for each HTTP session by the container.
Global-session: This is used to create global session beans for Portlet applications.

Explain inner beans in Spring.

A bean can be declared as an inner bean only when it is used as a property of another bean. For defining a bean, the Spring’s XML based configuration metadata provides the use of <bean> element inside the <property> or <constructor-arg>. Inner beans are always anonymous and they are always scoped as prototypes. For example, let’s say we have one Student class having reference of Person class. Here we will be creating only one instance of Person class and use it inside Student.

Here’s a Student class followed by bean configuration file:

Student.java

public class Student
{
private Person person;
//Setters and Getters
}
public class Person
{
private String name;
private String address;
//Setters and Getters
}

studentbean.xml
	
<bean id=“StudentBean" class="com.edureka.Student">
<property name="person">
<!--This is inner bean -->
<bean class="com.edureka.Person">
<property name="name" value=“Scott"></property>
<property name="address" value=“Bangalore"></property>
</bean>
</property>
</bean>
21. Define Bean Wiring.

When beans are combined together within the Spring container, it’s called wiring or bean wiring. The Spring container needs to know what beans are needed and how the container should use dependency injection to tie the beans together, while wiring beans.

bean wiring - Spring Framework Interview Questions - Edureka!
22. What do you understand by auto wiring and name the different modes of it?

The Spring container is able to autowire relationships between the collaborating beans. That is, it is possible to let Spring resolve collaborators for your bean automatically by inspecting the contents of the BeanFactory.
Different modes of bean auto-wiring are:

    no: This is default setting which means no autowiring. Explicit bean reference should be used for wiring.
    byName: It injects the object dependency according to name of the bean. It matches and wires its properties with the beans defined by the same names in the XML file.
    byType: It injects the object dependency according to type. It matches and wires a property if its type matches with exactly one of the beans name in XML file.
    constructor: It injects the dependency by calling the constructor of the class. It has a large number of parameters.
    autodetect: First the container tries to wire using autowire by constructor, if it can’t then it tries to autowire by byType.

23. What are the limitations with auto wiring?

Following are some of the limitations you might face with auto wiring:

    Overriding possibility: You can always specify dependencies using <constructor-arg> and <property> settings which will override autowiring.
     Primitive data type: Simple properties such as primitives, Strings and Classes can’t be autowired.
    Confusing nature: Always prefer using explicit wiring because autowiring is less precise.
	
	What do you understand by @Qualifier annotation?

When you create more than one bean of the same type and want to wire only one of them with a property  you can use the @Qualifier annotation along with @Autowired to remove the ambiguity by specifying which exact bean should be wired.

For example, here we have two classes, Employee and EmpAccount respectively. In EmpAccount, using @Qualifier its specified that bean with id emp1 must be wired.

Employee.java

public class Employee
{
private String name;
@Autowired
public void setName(String name)
{ this.name=name; }
public string getName()
{ return name; }
}

EmpAccount.java

public class EmpAccount
{
private Employee emp;
@Autowired
@Qualifier(emp1)
public void showName()
{
System.out.println(“Employee name : ”+emp.getName);
}
}

***********************************************************************
Spring Boot
***********************************************************************
What are the Spring Boot Starters?
Starters are a set of convenient dependency descriptors which we can include in our application.
Spring Boot provides built-in starters which makes development easier and rapid. For example, if we want to get started using Spring and JPA for database access, just include the spring-boot-starter-data-jpa dependency in your project.

Spring Boot starters are a set of convenient dependency management providers which can be used in the application to enable dependencies. These starters, make development easy and rapid. All the available starters come under the org.springframework.boot group. Few of the popular starters are as follows:

Programming & Frameworks Training
spring-boot-starter: – This is the core starter and includes logging, auto-configuration support, and YAML.
spring-boot-starter-jdbc – This starter is used for HikariCP connection pool with JDBC
spring-boot-starter-web – Is the starter for building web applications, including RESTful, applications using Spring MVC
spring-boot-starter-data-jpa – Is the starter to use Spring Data JPA with Hibernate
spring-boot-starter-security – Is the starter used for Spring Security
spring-boot-starter-aop: This starter is used for aspect-oriented programming with AspectJ and  Spring AOP
spring-boot-starter-test: Is the starter for testing Spring Boot applications

What does it mean that Spring Boot supports relaxed binding?
Relaxed binding in Spring Boot is applicable to the type-safe binding of configuration properties.
With relaxed binding, the key of an environment property doesn’t need to be an exact match of a property name. Such an environment property can be written in camelCase, kebab-case, snake_case, or in uppercase with words separated by underscores.
For example, if a property in a bean class with the @ConfigurationProperties annotation is named myProp, it can be bound to any of these environment properties: myProp, my-prop, my_prop, or MY_PROP.

How to deploy Spring Boot web applications as JAR and WAR files

Which Embedded Containers Are Supported by Spring Boot?

Whenever you are creating a Java application, deployment can be done via two methods:

    Using an application container that is external.
    Embedding the container inside your jar file.

Spring Boot contains Jetty, Tomcat, and Undertow servers, all of which are embedded.

    Jetty – Used in a wide number of projects, Eclipse Jetty can be embedded in framework, application servers, tools, and clusters.

    Tomcat – Apache Tomcat is an open source JavaServer Pages implementation which works well with embedded systems.

    Undertow – A flexible and prominent web server that uses small single handlers to develop a web server.

	
How Can You Set Up Service Discovery?

There are multiple ways to set up service discovery. I’ll choose the one that I think to be most efficient, Eureka by Netflix. It is a hassle free procedure that does not weigh much on the application. Plus, it supports numerous types of web applications.
Eureka configuration involves two steps – client configuration and server configuration.
Client configuration can be done easily by using the property files. In the clas spath, Eureka searches for a eureka-client.properties file. It also searches for overrides caused by the environment in property files which are environment specific.
For server configuration, you have to configure the client first. Once that is done, the server fires up a client which is used to find other servers. The Eureka server, by default, uses the Client configuration to find the peer server.


Why Would You Opt for Microservices Architecture?

This is a very common microservices interview question which you should be ready for! There are plenty of pros that are offered by a microservices architecture. Here are a few of them:
    Microservices can adapt easily to other frameworks or technologies.
    Failure of a single process does not affect the entire system.
    Provides support to big enterprises as well as small teams.
    Can be deployed independently and in relatively less time.
	
How does Microservice Architecture work?

    Topic: Microservices
    Difficulty: ??? 

    Clients – Different users from various devices send requests.
    Identity Providers – Authenticates user or clients identities and issues security tokens.
    API Gateway – Handles client requests.
    Static Content – Houses all the content of the system.
    Management – Balances services on nodes and identifies failures.
    Service Discovery – A guide to find the route of communication between microservices.
    Content Delivery Networks – Distributed network of proxy servers and their data centers.
    Remote Service – Enables the remote access information that resides on a network of IT devices.

	transaction management in microservcies
	


	
	
What does the @SpringBootApplication annotation do internally?

As per the Spring Boot doc, the @SpringBootApplication annotation is equivalent to using @Configuration, @EnableAutoConfiguration, and @ComponentScan with their default attributes. Spring Boot enables the developer to use a single annotation instead of using multiple. But, as we know, Spring provided loosely coupled features that we can use for each individual annotation as per our project needs.

2) How to exclude any package without using the basePackages filter?

There are different ways you can filter any package. But Spring Boot provides a trickier option for achieving this without touching the component scan. You can use the exclude attribute while using the annotation  @SpringBootApplication. See the following code snippet:

@SpringBootApplication(exclude= {Employee.class})
public class FooAppConfiguration {}


3) How to disable a specific auto-configuration class?

You can use the exclude attribute of@EnableAutoConfiguration, if you find any specific auto-configuration classes that you do not want are being applied.

//By using "exclude"
@EnableAutoConfiguration(exclude={DataSourceAutoConfiguration.class})


On the other foot, if the class is not on the classpath, you can use the excludeName attribute of the annotation and specify the fully qualified name instead.

//By using "excludeName"
@EnableAutoConfiguration(excludeName={Foo.class})


Also, Spring Boot provides the facility to control the list of auto-configuration classes to exclude by using the spring.autoconfigure.exclude property. You can add into the application.properties. And you can add multiple classes with comma separated.

//By using property file
spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration


4) What is Spring Actuator? What are its advantages?

This is one of the most common interview questions in Spring Boot. As per the Spring doc:

"An actuator is a manufacturing term that refers to a mechanical device for moving or controlling something. Actuators can generate a large amount of motion from a small change."
As we know, Spring Boot provides lots of auto-configuration features that help developers quickly develop production components. But if you think about debugging and how to debug, if something goes wrong, we always need to analyze the logs and dig through the data flow of our application to check to see what's going on. So, the Spring Actuator provides easy access to those kinds of features. It provides many features, i.e. what beans are created, the mapping in the controller, the CPU usage, etc. Automatically gathering and auditing health and metrics can then be applied to your application.

It provides a very easy way to access the few production-ready REST endpoints and fetch all kinds of information from the web. But by using these endpoints, you can do many things to see here the endpoint docs. There is no need to worry about security; if Spring Security is present, then these endpoints are secured by default using Spring Security’s content-negotiation strategy. Or else, we can configure custom security by the help of RequestMatcher.

Essentially, Actuator brings Spring Boot applications to life by enabling production-ready features. These features allow us to monitor and manage applications when they're running in production.

Integrating Spring Boot Actuator into a project is very simple. All we need to do is to include the spring-boot-starter-actuator starter in the pom.xml file:
	
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>

Spring Boot Actuator can expose operational information using either HTTP or JMX endpoints. Most applications go for HTTP, though, where the identity of an endpoint and the /actuator prefix form a URL path.

Here are some of the most common built-in endpoints Actuator provides:

    auditevents: Exposes audit events information
    env: Exposes environment properties
    health: Shows application health information
    httptrace: Displays HTTP trace information
    info: Displays arbitrary application information
    metrics: Shows metrics information
    loggers: Shows and modifies the configuration of loggers in the application
    mappings: Displays a list of all @RequestMapping paths
    scheduledtasks: Displays the scheduled tasks in your application
    threaddump: Performs a thread dump

5) How to enable/disable the Actuator?

Enabling/disabling the actuator is easy; the simplest way is to enable features to add the dependency (Maven/Gradle) to the spring-boot-starter-actuator, i.e. Starter. If you don't want the actuator to be enabled, then don't add the dependency.

Maven dependency:

<dependencies>
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
</dependencies>

How can we create a custom endpoint in Spring Boot Actuator?
To create a custom endpoint in Spring Boot 2.x, you can use the @Endpoint annotation. Spring Boot also exposes endpoints using @WebEndpointor, @WebEndpointExtension over HTTP with the help of Spring MVC, Jersey, etc.

6) What is the Spring Initializer?

This may not be a difficult question, but the interviewer always checks the subject knowledge of the candidate. It's often that you can't always expect questions that you have prepared. However, this is a very common question asked almost all of the time.

The Spring Initializer is a web application that generates a Spring Boot project with everything you need to start it quickly. As always, we need a good skeleton of the project; it helps you to create a project structure/skeleton properly. You can learn more about the Initializer here.

7) What is a shutdown in the actuator?

Shutdown is an endpoint that allows the application to be gracefully shutdown. This feature is not enabled by default. You can enable this by using management.endpoint.shutdown.enabled=true in your application.properties file. But be careful about this if you are using this.

8) Is this possible to change the port of Embedded Tomcat server in Spring boot?

Yes, it's possible to change the port. You can use the application.properties file to change the port. But you need to mention "server.port" (i.e. server.port=8081). Make sure you have application.properties in your project classpath; REST Spring framework will take care of the rest. If you mention server.port=0 , then it will automatically assign any available port.

9) Can we override or replace the Embedded Tomcat server in Spring Boot?

Yes, we can replace the Embedded Tomcat with any other servers by using the Starter dependencies. You can use spring-boot-starter-jetty  or spring-boot-starter-undertow as a dependency for each project as you need.

10) Can we disable the default web server in the Spring Boot application?

The major strong point in Spring is to provide flexibility to build your application loosely coupled. Spring provides features to disable the web server in a quick configuration. Yes, we can use the application.properties to configure the web application type, i.e.  spring.main.web-application-type=none.

What is thymeleaf?
It is a server side Java template engine for web application. It's main goal is to bring elegant natural templates to your web application.

It can be integrate with Spring Framework and ideal for HTML5 Java web applications.

For more information click here.

14) How to use thymeleaf?
In order to use Thymeleaf we must add it into our pom.xml file like:

<dependency>    
<groupId>org.springframework.boot</groupId>    
<artifactId>spring-boot-starter-thymeleaf</artifactId>    
</dependency>    

Do you think, you can use jetty instead of tomcat in spring-boot-starter-web?
Yes, we can use jetty instead of tomcat in spring-boot-starter-web, by removing the existing dependency and including the following:

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
    <exclusions>
        <exclusion>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-tomcat</artifactId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-jetty</artifactId>
</dependency>

What is the way to use profiles to configure the environment-specific configuration with Spring Boot?
Since it is a known fact that a Profile is nothing but a key to identify an environment lets consider the following two profiles in the example:

dev
prod
Consider the following properties present in the application properties file:
example.number: 100
example.value: true
example.message: Dynamic Message

Now, say you want to customize the application.properties for dev profile, then you need to create a file with name application-dev.properties and override the properties that you want to customize. You can mention the following code:

example.message: Dynamic Message in Dev
Similarly, if you want to customize the application.properties for prod profile, then you can mention the following code snippet:

example.message: Dynamic Message in Prod
Once you are done with the profile-specific configuration, you have to set the active profile in an environment. To do that, either you can

Use -Dspring.profiles.active=prod in  arguments
Use spring.profiles.active=prod in application.properties file


 What do you think is the need for Profiles?
Profiles are used to provide a way to segregate the different parts of the application configuration and make it available for various environments. So, basically, any @Component or a @Configuration can be marked with a @Profile to limit as it is loaded. Consider you have multiple environments,

Dev
QA
Stage
Production
Now, let’s say, you want to have different application configuration in each of the environments, you can use profiles to have different application configurations for different environments. So, basically, Spring and Spring Boot provide features through which you can specify:

The active profile for a specific environment
The configuration of various environments for various profiles.

***********************************************************************
Struts
***********************************************************************
interceptor in struts
Interceptors are the backbone of Struts2 Framework. Struts2 interceptors are responsible for most of the processing done by the framework, such as passing request params to action classes, making Servlet API request, response, session available to Action classes, validation, i18n support, etc.

ActionInvocation is responsible to incapsulate Action classes and interceptors and to fire them in order. The most important method for use in ActionInvocation is invoke() method that keeps track of the interceptor chain and invokes the next interceptor or action. This is one of the best example of Chain of Responsibility pattern in Java EE frameworks.

What is ValueStack and OGNL?
ValueStack is the storage area where the application data is stored by Struts2 for processing the client requests. The data is stored in ActionContext objects that use ThreadLocal to have values specific to the particular request thread.

Object-Graph Navigation Language (OGNL) is a powerful Expression Language that is used to manipulate data stored on the ValueStack. As you can see in architecture diagram, both interceptors and result pages can access data stored on ValueStack using OGNL.

Does Struts2 action and interceptors are thread safe?
Struts2 Action classes are thread safe because an object is instantiated for every request to handle it.

Struts2 interceptors are singleton classes and a new thread is created to handle the request, so it’s not thread safe and we need to implement them carefully to avoid any issues with shared data.

Which design pattern is implemented by Struts2 interceptors?
Struts2 interceptors are based on intercepting filters design pattern. The invocation of interceptors in interceptor stack closely resembles Chain of Responsibility design pattern.

Which interceptor is responsible for mapping request parameters to action class Java Bean properties?
com.opensymphony.xwork2.interceptor.ParametersInterceptor interceptor is responsible for mapping request parameters to the Action class java bean properties. This interceptor is configured in struts-default package with name “params”. This interceptor is part of basicStack and defaultStack interceptors stack.

What is the difference in using Action interface and ActionSupport class for our action classes, which one you would prefer?
We can implement Action interface to create our action classes. This interface has a single method execute() that we need to implement. The only benefit of using this interface is that it contains some constants that we can use for result pages, these constants are SUCCESS, ERROR, NONE, INPUT and LOGIN.

ActionSupport class is the default implementation of Action interface and it also implements interfaces related to Validation and i18n support. ActionSupport class implements Action, Validateable, ValidationAware, TextProvider and LocaleProvider interfaces. We can override the validate() method of ActionSupport class to include field level validation login in our action classes.

Depending on the requirements, we can use any of the approaches to creating Struts 2 action classes, my favorite is ActionSupport class because it helps in writing validation and i18n logic easily in action classes.


***********************************************************************
JSF
***********************************************************************
Is it possible to have more than one Faces Configuration file?
Can we have a JSF application without faces-config.xml?- yes, sing only annotations
What are the different types of Page Navigation supported in JSF?
how can you use a messgae bundel or implemnent internatilization in jsf - declare in faces-config.xml or use f:loadbundle
what is difference between jsp experssion language and jsf el? - value expressions are delimited by ${?}, in jsp el but #{?} in jsf el
diffreence between backing bena and managed bean - backing bean is any beam refenced in form, managed bean is backing bean registred in faces-config.xml and created by jsf when needed. backing bena can only have request scope, while managed can have request, session or application scope



***********************************************************************
Git
***********************************************************************
 difference between git pull and git fetch
 git stash



***********************************************************************
Web services
***********************************************************************
whcih Jax-rs Implementations have you used?

what is HATEOAS in REST?
HATEOAS stand for Hypermedia As The Engine Of Application State. It provides links to resources so that client does not have to manually bookmark the links. Below is an example.

{

"id":1,

"message":"Hello World",

"author":"Dhiraj",

"href":"/messages/1"

}

What tools do you use to test your Web Services?
SoapUI tool for SOAP WS & RESTFul web service testing and on the browser the Firefox “poster” plugin or Google Chrome “Postman” extension for RESTFul services or Advanced Rest Client provided by Google.


How can you implemenet HTTP basic Authentication for the web service?

How can you do the versioning of the Interface you have implemented?


What does HTTp status code 400 means, and 500 means?

***********************************************************************
Batch jobs
***********************************************************************
What are the three key components to consider when using Java quartz?
The scheduler, job and trigger. The scheduler coordinates the execution of the job and the trigger sets up the interval and frequency with which the job will run.

1.2. How do you begin a quartz process so that it will start executing jobs?
To begin a quartz process you must initiate all the components, scheduler, job and trigger, and then call the start method on the scheduler.

Trigger trigger = TriggerBuilder.newTrigger().withIdentity("jcgTriggerName", "group1")
  .withSchedule(CronScheduleBuilder.cronSchedule("0/5 * * * * ?")).build();
Scheduler scheduler;
scheduler = new StdSchedulerFactory().getScheduler();
scheduler.start();
JobKey jobKey = new JobKey("jcgJobName", "group1");
JobDetail job = JobBuilder.newJob(MyJob.class).withIdentity(jobKey).build();
scheduler.scheduleJob(job, trigger);


1.3. How do I check the status of a running job?
The JobListener will allow you to check the status of a running job.

How is the CronTrigger used?
The CronTrigger is used to execute a job using a cron expression. A cron expression is a notation that represents a second, minute, hour, day, month, or year as well as the ways to express special characters like wildcards or ranges for the schedule.

1.7. How do you store job state?
The JobDataMap is used to store state that will be provided to the job when it executes.

1.8. How do you handle or prevent concurrent execution of jobs?
The DisallowConcurrentExecution annotation is used to prevent concurrent execution of that same instance of jobs. The instance definition is controlled by the identifier in JobDetail.

1.10. What types of exceptions are allowed from the job execute method?
The JobExecutionException is the only allowable exception from the job execute method

1.11. What happens when a scheduled job fails to trigger?
The misfire conditions is specific to each trigger. For the cron trigger, a misfire condition is specified in the job creation or will default to the smart policy. To customize the misfire condition specific to the cron trigger there are three options; withMisfireHandlingInstructionDoNothing, withMisfireHandlingInstructionFireAndProceed, withMisfireHandlingInstructionIgnoreMisfires.

You can also create a custom trigger by implementing the TriggerListener interface and define the triggerMisfired method.

1
public void triggerMisfired(Trigger trigger);

1.13. What are different types of Job Stores?
There are three different types of job stores provided by quartz; RAMJobStore, JDBCJobStore and TerracottaJobStore that provided persistent job data to quartz components.

1.14. How would you stop a running job?
You can stop a running job by calling interrupt on the scheduler and providing the job key. The job that you are interrupting must implement the InterruptableJob interface.

1
sched.interrupt(job.getKey());
1.15. What is the JobExecutionContext?
The JobExecutionContext is passed to the execute method of an invoked job by the scheduler and it contains a handle to the scheduler, a handle to the trigger and the JobDetail.

How do I chain Job execution? Or, how do I create a workflow?
There currently is no “direct” or “free” way to chain triggers with Quartz. However there are several ways you can accomplish it without much effort. Below is an outline of a couple approaches:

One way is to use a listener (i.e. a TriggerListener, JobListener or SchedulerListener) that can notice the completion of a job/trigger and then immediately schedule a new trigger to fire. This approach can get a bit involved, since you’ll have to inform the listener which job follows which – and you may need to worry about persistence of this information.

Another way is to build a Job that contains within its JobDataMap the name of the next job to fire, and as the job completes (the last step in its execute() method) have the job schedule the next job. Several people are doing this and have had good luck. Most have made a base (abstract) class that is a Job that knows how to get the job name and group out of the JobDataMap using special keys (constants) and contains code to schedule the identified job. Then they simply make extensions of this class that included the additional work the job should do.

In the future, Quartz will provide a much cleaner way to do this, but until then, you’ll have to use one of the above approaches, or think of yet another that works better for you.

***********************************************************************
Apache POI
***********************************************************************
difference between HSSF and XSSF
HSSF (Horrible Spreadsheet Format) - It is used to read and write xls format of MS-Excel files.

XSSF (XML Spreadsheet Format) - It is used for xlsx file format of MS-Excel.
can you hide a sheet using apache POI
password protect a sheet using apache POI?

***********************************************************************
Design pattern and advanced
***********************************************************************
how to do concurreny management
caching
which design pattern you have used?
how to include singleton pattern?
What is the difference between the Factory and the Abstract Factory pattern

How can you secure the password being used in a spring aplication?

What things you would care about to improve the performance of Application if its identified that its DB communication that needs to be improved ?
1. Query Optimization ( Query Rewriting , Prepared Statements )
2. Restructuring Indexes.
3. DB Caching Tuning ( if using ORM )
4. Identifying the problems ( if any ) with the ORM Strategy ( If using ORM )

if you have find execution time of a method, different ways?
using spring AOP
start and end log

Which Design Pattern Will You Use to Shield Your Code From a Third Party library Which Will Likely to be Replaced by Another in Couple of Months?
This is just one example of  the scenario-based design pattern interview question. In order to test the practical experience of Java developers with more than 5 years experience, companies ask this kind of questions.  You can expect more real-world design problems in different formats, some with more detail explanation with context, or some with only intent around. 
One way to shield your code from third party library is to code against an interface rather than implementation and then use dependency injection to provide a particular implementation. This kind of questions is also asked quite frequently to experienced and senior Java developers with 5 to 7 years of experience.

 Do you know about Open Closed Design Principle or Liskov Substitution Principle?
 
 What is decorator design pattern in Java?

·          Decorator design pattern is used to enhance the functionality of a particular object at run-time or dynamically.
·          At the same time other instance of same class will not be affected by this so individual object gets the new behavior.
·          Basically we wrap the original object through decorator object.
·          Decorator design pattern is based on abstract classes and we derive concrete implementation from that classes,

Read more: https://javarevisited.blogspot.com/2011/11/decorator-design-pattern-java-example.html#ixzz69154rReV

best practice to prevent duplicate form submit
disabling the button
the cookie sounds like a good approach, basically you need something that the server is going to pass to the client, and then check on submit. once that has been authorized the server will prevent processing of further requests with the same parameter.

What do you understand by OutOfMemoryError in Java?
Answer: Typically, the OutOfMemoryError exception is thrown when the JVM is not able to allocate an object due to running out of memory. In such a situation, no memory could be reclaimed by the garbage collector. There can be several reasons that result in the OutOfMemoryError exception, out of which most notable ones are:

Holding objects for too long
Trying to process too much data at the same time
Using a third-party library that caches strings
Using an application server that doesn’t perform a memory cleanup post the deployment
When a native allocation can’t be satisfied

What is difference between dependency injection and factory design pattern? (answer)
Though both patterns help to take out object creation part from application logic, use of dependency injection results in cleaner code than factory pattern. By using dependency injection, your classes are nothing but POJO which only knows about dependency but doesn't care how they are acquired. In the case of factory pattern, the class also needs to know about factory to acquire dependency. hence, DI results in more testable classes than factory pattern. Please see the answer for a more detailed discussion on this topic.


113) Difference between Adapter and Decorator pattern? (answer)
Though the structure of Adapter and Decorator pattern is similar, the difference comes on the intent of each pattern. The adapter pattern is used to bridge the gap between two interfaces, but Decorator pattern is used to add new functionality into the class without the modifying existing code.


114) Difference between Adapter and Proxy Pattern? (answer)
Similar to the previous question, the difference between Adapter and Proxy patterns is in their intent. Since both Adapter and Proxy pattern encapsulate the class which actually does the job, hence result in the same structure, but Adapter pattern is used for interface conversion while the Proxy pattern is used to add an extra level of indirection to support distribute, controlled or intelligent access.


115) What is Template method pattern? (answer)
Template pattern provides an outline of an algorithm and lets you configure or customize its steps. For examples, you can view a sorting algorithm as a template to sort object. It defines steps for sorting but let you configure how to compare them using Comparable or something similar in another language. The method which outlines the algorithms is also known as template method.


116) When do you use Visitor design pattern? (answer)
The visitor pattern is a solution of problem where you need to add operation on a class hierarchy but without touching them. This pattern uses double dispatch to add another level of indirection.


117) When do you use Composite design pattern? (answer)
Composite design pattern arranges objects into tree structures to represent part-whole hierarchies. It allows clients treat individual objects and container of objects uniformly. Use Composite pattern when you want to represent part-whole hierarchies of objects.

What is Law of Demeter violation? Why it matters? (answer)
Believe it or not, Java is all about application programming and structuring code. If  you have good knowledge of common coding best practices, patterns and what not to do than only you can write quality code.  Law of Demeter suggests you "talk to friends and not stranger", hence used to reduce coupling between classes.

According to Law of Demeter, a method M of object O should only call following types of methods:
Methods of Object O itself
Methods of Object passed as an argument
Method of object, which is held in instance variable
Any Object which is created locally in method M
More importantly method should not invoke methods on objects that are returned by any subsequent method calls specified above and as Clean Code says "talk to friends, not to strangers".

 // as per rule 1, this method invocation is fine, because o is a argument of process() method         Message msg = o.getMessage();         // this method call is a violation, as we are using msg, which we got from Order.         // We should ask order to normalize message, e.g. "o.normalizeMessage();"         msg.normalize();         // this is also a violation, instead using temporary variable it uses method chain.         o.getMessage().normalize();

Read more: https://javarevisited.blogspot.com/2014/05/law-of-demeter-example-in-java.html#ixzz6H8jrbBi2



108) What is Adapter pattern? When to use it?
Another frequently asked Java design pattern questions. It provides interface conversion. If your client is using some interface but you have something else, you can write an Adapter to bridge them together. This is good for Java software engineer having 2 to 3 years experience because the question is neither difficult nor tricky but requires knowledge of OOP design patterns.

Read more: https://javarevisited.blogspot.com/2015/10/133-java-interview-questions-answers-from-last-5-years.html#ixzz6H8eXGTJY

 Can you explain Liskov Substitution principle? 
According to the Liskov Substitution Principle, Subtypes must be substitutable for supertype, i.e. methods or functions which use superclass type must be able to work with the object of subclass without any issue".
 LSP is closely related to the Single responsibility principle and Interface Segregation Principle. If a class has more functionality, than the subclass might not support some of the functionality and does violate LSP.In order to follow LSP SOLID design principle, derived class or subclass must enhance functionality, but not reduce them. LSP represents "L" on the SOLID acronym.

 
 Single Responsibility Principle (SRP)
Single Responsibility Principle is another SOLID design principle, and represent  "S" on the SOLID acronym. As per SRP, there should not be more than one reason for a class to change, or a class should always handle single functionality.


Open Closed Design Principle
Classes, methods, or functions should be Open for extension (new functionality) and Closed for modification. This is another beautiful SOLID design principle, which prevents someone from changing already tried and tested code.
Ideally, if you are adding new functionality only, then your code should be tested, and that's the goal of the Open Closed Design principle. By the way, the Open-Closed principle is "O" from the SOLID acronym.


DRY (Don't repeat yourself)
Our first object-oriented design principle is DRY, as the name suggests DRY (don't repeat yourself) means don't write duplicate code, instead use Abstraction to abstract common things in one place. If you have a block of code in more than two places, consider making it a separate method, or if you use a hard-coded value more than one time, make them public final constant.
The benefit of this Object-oriented design principle is in maintenance. It's important not to abuse it, duplication is not for code, but for functionality. It means if you used standard code to validate OrderID and SSN, it doesn’t mean they are the same, or they will remain the same in the future.
By using standard code for two different functionality or thing, you tightly couple them forever, and when your OrderId changes its format, your SSN validation code will break.So beware of such coupling and just don’t combine anything which uses similar code but is not related.


Read more: https://javarevisited.blogspot.com/2018/07/10-object-oriented-design-principles.html#ixzz6H8l1F7Le

How to swap two integers without using temp variable?
a = a + b; 
b = a - b; // actually (a + b) - (b), so now b is equal to a 
a= a - b; // (a + b) -(a), now a is equal to b

How do you test static method? (answer)
You can use PowerMock library to test static methods in Java.

Is it possible for two unequal objects to have the same hashcode?
Yes, two unequal objects can have same hashcode that's why collision happen in a hashmap.
the equal hashcode contract only says that two equal objects must have the same hashcode it doesn't say anything about the unequal object.

Read more: https://javarevisited.blogspot.com/2015/10/133-java-interview-questions-answers-from-last-5-years.html#ixzz6H8o2FWe1


***********************************************************************
Jenkins
***********************************************************************
What are Triggers in jenkins?

***********************************************************************
Agile
***********************************************************************
What do you know about “Planning Poker” technique?
Answer: Planning poker, also known as Scrum Poker, is a card-based agile technique that is used for planning and estimation. To start a session of planning poker technique, the agile user story is read by the product owner. The steps performed in the poker planning technique are –

    Each estimator has a deck of poker cards with the values such as 0, 1, 2, 3, 5, and so on, to denote story points, ideal days or something else that the team uses for estimation.
    Each estimator has a discussion with the product owner and then privately selects a card on the basis of their independent estimation.
    If the cards with same value are selected by all estimators, it is considered as an estimate. If not, the estimator discusses the high and low value of their estimates.
    Then again, each estimator privately selects a card and reveals. This process of poker planning is repeated to reach a general agreement.


What is the scrum of scrums?
Answer: Suppose there are  7 teams working on a project and each team has  7 members. Each team leads its own particular scrum meeting. Now to coordinate among the teams a separate meeting has to be organized, that meeting is called Scrum of Scrums.

What is the Release candidate?
Answer: The release candidate is a code /version /build released to make sure that during the last development period, no critical problem is left behind. It is used for testing and is equivalent to the final build.

Explain what is a story point in the scrum?
Answer: It can be considered as a unit to estimate the total efforts required to complete or to do the particular task or implementing a backlog.

Why aren't user stories simply estimated in man-hours?
Answer: Estimation of user stories on the basis of man-hours can be done but preferably not. You won't be able to concentrate on the quality product to be delivered to the customer. Moreover, you will concentrate on the cost and budget of the management while using man-hours.

Instead of that, one can use story points, as it provides the complete idea about both the complexity of work and required efforts

***********************************************************************
Maven
***********************************************************************
What is a transitive dependency in Maven ? Can we override Transitive Dependency version and If Yes, how ?
Transitive dependency is the dependencies not defined directly in the current POM but the POM of the dependent projects. 
Yes we can override transitive dependency version by specifying the dependency in the current POM. 

***********************************************************************
Machine learning
***********************************************************************

https://www.springboard.com/blog/machine-learning-interview-questions/
which machine learning you used- supervised or unsupervised?
what is meant by precision and recall?
how did you perform the evaluation for your algorithm? - What evaluation approaches would you work to gauge the effectiveness of a machine learning model?
Type I error is a false positive, while Type II error is a false negative. 
What’s the difference between Type I and Type II error?
What’s the F1 score? How would you use it?
How would you handle an imbalanced dataset?
 What is classifier in machine learning?
 
 How is feature extraction done in NLP
The features of a sentence can be used to conduct sentiment analysis or document classification. For example if a product review on Amazon or a movie review on IMDB consists of certain words like ‘good’, ‘great’ more, it could then be concluded/classified that a particular review is positive.
Bag of words is a popular model which is used for feature generation. A sentence can be tokenized and then a group or category can be formed out of these individual words, which further explored or exploited for certain characteristics(number of times a certain word appears etc).

difference betweehn lemmatization and stemming
inverse document frequency?
The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient): 

Levenshtein distance?
In information theory, linguistics and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after the Soviet mathematician Vladimir Levenshtein, who considered this distance in 1965.[1]
Levenshtein distance may also be referred to as edit distance, although that term may also denote a larger family of distance metrics.[2]:32 It is closely related to pairwise string alignments. 
Levenshtein Distance (LD) is a measure to quantify how different two stings by counting the number of character edits that turns one string into another.
For example, distance between "cats" and "rats" is 1, since you need to replace the "c" with the "r"; distance between "house" and "host" is 2 (remove "u" and replace "e" with "t".
Applications of LD are in spelling correction (find the closest word from the vocabulary) and in all applications that benefit from soft matching of words, e.g. information retrieval, machine translation etc.

HITS algorithm
Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg. The idea behind Hubs and Authorities stemmed from a particular insight into the creation of web pages when the Internet was originally forming; that is, certain web pages, known as hubs, served as large directories that were not actually authoritative in the information that they held, but were used as compilations of a broad catalog of information that led users direct to other authoritative pages. In other words, a good hub represented a page that pointed to many other pages, and a good authority represented a page that was linked by many different hubs.[1]

The scheme therefore assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages. 
***********************************************************************
***********************************************************************
JBPM
***********************************************************************
***********************************************************************
What is BPM?
BPM stands for Business Process Management. It is a set of activities which follows the number of steps in a specific order to fulfill the organizational goals. The order of these goals is depicted using a flowchart.

It involves:


Understanding the values which the organization delivers.
How those are achieved by analyzing, documenting and improving the way that people and systems work together.
2) What is jBPM?
jBPM stands for Java Business Process Management. It is a flexible Business Process Management (BPM) Suite which is written in the Java language. It filled the gap between the business analysts and the developers. It is light-weight and fully open-source in nature. It allows us to create, deploy, execute and monitor business processes throughout their life cycle. It is distributed under the Apache license and was released under the ASL by the JBoss Company.

Traditional BPM process engines focused only non-technical people who had insufficient functionality.

For more detail:jBPM Introduction

3) What does jBPM do?
JBPM is a business management suite which is used to model our business goals. It describes the number of steps that need to be executed to achieve the business goal and the order, using a flowchart. It improves the visibility and agility of our business logic. It can be understood by the business users and the developers both. It is easier to monitor the business processes.

The core of jBPM is a light-weight, extensible workflow engine written in pure Java. It allows us to execute business processes using the latest BPMN 2.0 specification. It can be run in any Java environment and can be easily embedded in our application or as a service.

4) What are the advantages of jBPM?
jBPM allows us to use different computational model for business process and rule. This model is based on a knowledge-oriented approach. jBPM application is not process-oriented or rules-oriented, but the end users can use different strategies to represent their business logic.

the advantages of jBPM are:

jBPM is lightweight, fully open-source and written in Java language.
jBPM makes possible to model complex workflows using a graphical designer. The graphical designer helps non-developers to design business processes and provides a much better view of the state of a process at runtime.
jBPM Workflows can also create tasks for human users. For example- manual testing or signing off on releases.
jBPM Workflow definitions contain the workflow graph along with the Java code which performs the actions triggered by the workflow. New workflows definitions do not affect the existing processes.

The list of the jBPM components are:

Core Engine
Human Task Services
Persistence
CDI/REST/JMS
Process Designer
Data Modeler
Form Modeler
Rules Authoring
Guvnor Repository
Process Management
Eclipse Developer Tools

Definition Service
It is used to scan process definition that parses the process and extracts important information from it. This information provides the input to the system to inform users about what is expected. Definition Service provides information about:

Process Definition
Process Variable
Service Task
User Task
Input and Output Information
Process Services
Process Services focused on runtime operations so use it only when there is need to alter process instance. It is used to give access to the execution environment that allows:

Start new process instances
Work with existing process
Runtime Data Services
This service refers to the runtime information of Process Instances. It is used as the main source of information.

Start Process Instances
Execute Node Instances
User Task Services
This service is used to manage the individual user task from start to end. User Task Service allows:

Modify Selected Properties
Access to Task Variables
Access to Task Attachments
Access to Task Comments

***********************************************************************
***********************************************************************
 
 what all you have worked
 role
 technical code
 design pattern

 jsf - richfaces, prime faces
 
 -----------------------------------------
 syed
 
 
 git -
 jsf
 spring
 
 immediate onsite
 notice period - last week resigend - 85 days left
 
 spring mvc , factory, singleton
 dependency injection - 
 
 spring - good
 
 orm - not worked 
 unstructred binary json into database
 
 relational database - not worked
 sql database
 
 jackson library 
 
 not sure about that
 
 core java - ok
 
 jsf - not in jsf
 
 jsf, hibernate, spring
 
 read some documentation
 3.5 experience
 
 caching - not part
 eureka and zool was used
 
 active profiles in spring-
 same jar - we can pass the value of client in parameter
 
 which clients it was 
 
 ----------------------------------------------
 
 9015196738
priyanka srivastava
11.92 years

dev work - 

 java and j2ee
 migration project - oracle toplink to eclipse orm
 
 struts mvc framework
 testing and support
 
 web based appl- 
 not worked scratch 
 support and enhacements
 
 jsf- no
 spring - no 
 
 struts - both 1 and 2
 interceptors - not used
 last project - migration ...already developed ..only migration 
 
 orm
 hibernate - but on eclipse link orm..open source
 queries - group by having order by
 
 java - 
 good
 
 singleton
 caching - did not know how it was implemented
 -----------------------------------------

core java concepts 
khud kaam
code

first level evaluation - 
neutral 

hibernate
spring
jsf
java
db

framework not known
 
 after 6 PM
 12 to 9 PM - not suitable
 
 technical lead past projects
 2 months leave
 CVS account
 wallgreens - 6 years
 GE
 
 sap - spring we applications with web and sap
 all the changes 
 
 technical design
 changes
 technical demos
 
 oracle as DB
 jsp, jquery
 jsf not worked on - not handson
 
 jsp and jquery
 hibernate - not worked on
 
 spring 
 jdbc template
 queries writing done
 
 java batches - generating files
 core java
 
 spring 
 
 batches - quartz
 java files - jobs configured in unix, cron jobs 
 
 enhancements - not stracth
 spring mvc application - not confident
 creating simple application, depend on the requirements
 
 web.xml - dispatcher servlet
 spring bean 
 
 interface and abstarct class - 
 
 java 8 - used
 

tushar shukla

elily client
core java
j2ee
hibernate
spring

support and maintenace + development--enhancement
assessment at offshore

rajeev

bean factory-lazy
application context- lazy + eager

spring - fine
webservices - ? restful web services--jersey


RDBMS
hql query
criteria query

oracle


sagar kale
experience
technology
what all worked on


sagar kale
15th may 2019

4 years exp
spring
sprint boot
java
hibernate
rest services
drool
git
angular
node js

java
migration project
80% development

spring 3 - to latest
spring mvc to spring boot
jpa 

concurrency mgmt
caching - encache

not much sql
basic queries

git not much
git 

3 months 

worked on javascript
jsf things

calculator approach

one of n numbers...

java 8

https://www.java2novice.com/java-interview-programs/duplicate-number/ 
https://www.java2novice.com/java-interview-programs/ 

------------------

Caching implementation

decotar design pattern
singlton design pattern
difference between factory and abstarct factory design pattern

prevent duplicate form submissions
most frequenctly occuring word

way to find method execution time
interceptor and purpose of that

order of ordr by, having, group by


meghna jain
2.7
data staging project

spring, hibernate, java 8 features
 
 ritu tyagi
 ng, deep learning specialization ngeong - coursera..standford 
 tcs - 6 weeks course - web scrapping - data science
 
 

--------------------------------------
java 8
use of optional
diff between findfirst and findany

permgen space...metaspace?

predicate and function - A Predicate can only return a boolean (result of the test()) while Function does a transformation and can return anything (result of apply(). Predicate can take one type parameter which represents input type or argument type. Function can take 2 type parameters First one represents input type argument type and second one represents return type. Predicates and Functions are functional interfaces introduced in Java 8's java. util. function package


default methods - Interfaces can have default methods with implementation in Java 8 on later.
Interfaces can have static methods as well, similar to static methods in classes. Java interface static method is similar to default method except that we can't override them in the implementation classes. 
Default methods were introduced to provide backward compatibility for old interfaces so that they can have new methods without affecting existing code. In case both the implemented interfaces contain default methods with same method signature, the implementing class should explicitly specify which default method is to be used or it should override the default method.


functional interface - An Interface that contains exactly one abstract method is known as functional interface. It can have any number of default, static methods but can contain only one abstract method. It can also declare methods of object class. Functional Interface is also known as Single Abstract Method Interfaces or SAM Interfaces. If we use a functional interface, we can reduce the lines of code that we write and increase its readability.  the main benefit of using functional interfaces is that they can be instantiated using lambda expressions rather than lengthy anonymous classes.isnce a lambda function can only provide the implementation for 1 method it is mandatory for the functional interface to have ONLY one abstract method.
carryOutWork(new SimpleFuncInterface() {
      @Override
      public void doWork() {
        System.out.println("Do work in SimpleFun impl...");
      }
    });
    carryOutWork(() -> System.out.println("Do work in lambda exp impl..."));

What is the use of accept and content type headers in HTTP request?
accept tells what kind of response client is accepting
content tells server what is the format of data being sent in request


what is payload?
the request data which is present in body part of http message

what is the diffrenrce between get and load in hibernate - get() loads the data as soon as it's called whereas load() returns a proxy object and loads data only when it's actually required, so load() is better because it support lazy loading. Since load() throws exception when data is not found, we should use it only when we know data exists.

https://stackoverflow.com/questions/4343202/difference-between-super-t-and-extends-t-in-java

https://stackoverflow.com/questions/50817843/difference-between-recursive-task-and-recursive-action-in-forkjoinpool

https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html
readAllLines vs lines in Files class
createDirectories vs createDirectory in Files class
walk mthod

https://docs.oracle.com/javase/8/docs/api/java/nio/file/Paths.html
https://docs.oracle.com/javase/8/docs/api/java/nio/file/Path.html

AtomicInteger java
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicInteger.html

https://www.geeksforgeeks.org/enum-in-java/
constructor in enum private
enum can implement interface but not extend class
enum can be defined in class or outside class, but not inside method
enum constant public static final
enum can have main method
All java enums implicitly extend from java.lang.Enum class and not from java.util.Enum class.

enums JobStatus and TestResult are siblings as both implicitly extend from java.lang.Enum class.
Siblings cannot be compared using double equals operator (==).
equals(Object) method accepts any instance which Object can refer to, which means all the instances.

clone method is protected in Enum class so it cannot be accessed in com.udayan.ocp package using reference variable

https://stackoverflow.com/questions/9077055/how-does-the-skip-method-in-inputstream-work

https://www.baeldung.com/java-8-double-colon-operator

https://www.geeksforgeeks.org/singleton-class-java/

https://www.geeksforgeeks.org/java-util-concurrent-cyclicbarrier-java/
https://stackoverflow.com/questions/21808814/whats-the-difference-between-cyclicbarrier-countdownlatch-and-join-in-java

RecursiveTask is a generic class which extends from ForkJoinTask<V> class. 
RecursiveTask<V> declares compute() method as: 'protected abstract V compute();' 
In the given code overriding method returns Long, so classes from which class Task can extend are: RecursiveTask<Long>, RecursiveTask<Object> [co-variant return type in overriding method] and RecursiveTask [co-variant return type in overriding method].

RecursiveAction is a non-generic class which extends from ForkJoinTask<Void> class.
RecursiveAction declared compute() method as: 'protected abstract void compute();'
In the given code overriding method returns Long, hence RecursiveAction can't be used as super class of Task.

https://www.geeksforgeeks.org/stream-peek-method-in-java-with-examples/

https://www.baeldung.com/java-8-localization

https://www.geeksforgeeks.org/assertions-in-java/

https://www.geeksforgeeks.org/path-resolve-method-in-java-with-examples/
https://stackoverflow.com/questions/50551920/where-to-use-resolve-and-relativize-method-of-java-nio-file-path-class
https://www.baeldung.com/java-nio-2-path
Resolve, resolve sibling and relativize
https://dzone.com/articles/getting-to-know-javaniofilepath-part-2

https://www.geeksforgeeks.org/path-normalize-method-in-java-with-examples/
https://www.geeksforgeeks.org/path-relativize-method-in-java-with-examples/

https://bugs.openjdk.org/browse/JDK-8066943
https://stackoverflow.com/questions/63872767/why-does-path-relativize-behave-differently-on-java-8-and-java-11
https://stackoverflow.com/questions/37747896/understanding-java-nio-file-path-relativizepath-other

https://www.logicbig.com/how-to/code-snippets/jcode-java-8-date-time-api-localdate-until.html

https://stackoverflow.com/questions/50076815/singlethreadexecutor-vs-plain-thread
The major difference is in task execution policy.
By creating a Thread instance or subclassing Thread you are basically executing a single task.
Using Executors.newSingleThreadExecutor() on the other hand allows you to submit multiple tasks. Since those tasks are guaranteed not to be executed concurrently, this allows you to exploit the following thread confinement benefits:
No synchronization required when accessing objects that are not thread-safe
Memory effects of one task are guaranteed to be visible to the next task

https://docs.oracle.com/javase/8/docs/api/java/util/Deque.html\diff 
between push, add, offer, pop, remove, poll, peek

https://www.geeksforgeeks.org/instant-truncatedto-method-in-java-with-examples/

https://beginnersbook.com/2013/05/inner-class/
Unlike other inner classes, an anonymous inner class can either extend from one class or can implement one interface.
It cannot extend and implement at the same time and it cannot implement multiple interfaces. Has no constructor and can not be static

Only two modifiers are allowed for method-local inner class which are abstract and final.The inner class can use the local variables of the method (in which it is present), only if they are marked final

A static nested class cannot access non static members of outer class.static nested class can use all 4 access modifiers (public, protected, default and private) and 2 non-access modifiers (final and abstract).static nested class can contain all type of members, static as well as non-static. This behavior is different from other inner classes as other inner classes don't allow to define anything static, except static final variables. This is the reason static nested class is not considered as inner class.

Regular inner class can have modifiers final, abstract, private, public and protected. It can not have anything static except static final varibales.

interface can be nested inside a class. Class Outer is top-level class and interface I1 is implicitly static.
Static nested interface can use all 4 access modifiers (public, protected, default and private).

interface can be nested inside an interface. class can be nested inside an interface. By default, these are public and static.You cannot explicitly specify protected, default and private for nested classes and nested interfaces inside an interface.

https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html

https://docs.oracle.com/javase/tutorial/jdbc/basics/transactions.html

https://docs.oracle.com/javase/8/docs/api/java/util/Random.html
nextInt, ints

https://www.geeksforgeeks.org/stream-max-method-java-examples/


https://www.examtopics.com/exams/oracle/1z0-809/view/23/

https://www.certification-questions.com/java-dumps/1z0-809.html

https://docs.oracle.com/javase/8/docs/api/java/lang/doc-files/ValueBased.html

https://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html

autowried anotaion constructor or setter when defined over constructor or setter else property injection

spring boot actuator for monitoring of services, getting all beans registered, all servlets , all auto configuration, all env variables etc..need to enable..management exposure we endpoints to * for getting beans end point etc. By default only health and monitoring end points are exposed..useful for liveness and readiness check in kubernetes world, application is up or down, dependencies are up or down..application si up only when all the dependencies are up

Centralized configuration server - spring cloud config server..to store configuration for different environment and diff project in a central location..no need to go and change the configuration on each server.. store conf in git repository..with application name and profile like limits-service.prop, limits-service-qa.prop..the default one is called when no profiles are active..in case of active profile, the apporpriate file is used..specific profile conf has priority over default conf..need to restart service using cong in case of conf change..to solve this we need Spring Cloud Bus and refresh scope can be used to dynamically propagate changes?.....problem of centralized failure
Externalized configuration is needed for portability and scalibility. They are needed for the 12 factor methodology which syas that configuration should be external. There are limit to number of env vriables which can be set. application should be able to pick change to cinfiguration dynamically without restart. Conf should be cloud/platform independent (if we use heroku or cloud foundry we are bound to one cloud provider implementaion for configuration). Changes to configuration if needed should be traceable.
Config server - version control for configuration through git, centralized configuration, allows easier maintainance and staging, get rid of env variables
Spring cloud config server - loads config to startup and exposes it via REST, is a spring boot starter project tied to Git repository, provides support for branching and profile
we can get config through rest end point or spring cloud config client which loads config on startup
config client makes uses of spring.application.name property and it should be the same as name of the configuration file in git. this should be present in bootstrap.properties file.. in the rest url, default provides the default file property while, - dev or -qa provides env specific property file details in rest api..spring-cloud-config-client dependency needed

For spring cloud config server, add dependency for spring-cloud-starter-server, in application.properties, include git.uri for the path where the config file is present and in the main class add the annotation @EnableConfigServer. Spring cloud config server uses EnvironmentRepository  which has 2 implementations available for Git and local file. If we have some other source, we need to provide our implemntation for EnvironmentRepository. The configuration file should be named spring.application.name-profile.properties

For spring cloud config client, add dependency for spring-cloud-starter-config, in bootstrap.properties, include spring.application.name and spring.cloud.config.uri for the config server url. the client will call http://<server>:<port>/spring.application.name/profile
With YAML file, we can put multiple profiles in one file. With properties file, we need to have two. YAML file is preferred first by config server compared to properties file.
Spring apps have an Environment object which contains multiple PropertySources populated from env variables, system properties, JNDI, developer specified properties file. Spring cloud config client library simply adds another PropertySource by connecting to server over HTTP http://<server>:<port>/spring.application.name/profile and those properties become part of client application and are prioritized over ohter properties.

If config server is down, 3 ways to handle :
multiple insatnces of config server and load balanaced
client app can hanve a policy on how to handle application if config server is down. In the client, have something like spring.cloud.config.failFast=true : This property tells the Spring Cloud Config Client to fail fast if it cannot connect to the configuration server. Default is false.
in the client provide local fallback configuration settings.

ContollerAdvice annotation for handling exceptions globally...definging a expectiontype and what rewpsonse status should be retruned for that acroos multiple controllers

ResponseStatus annotation to return different response code from a rest api for example 204 for entity created instead of 200 success etc.

RestTemplate and urivariables - for calling rest apis, need lot of code and also duplicate code..and hard coding of urls

spring jpa defer initlaization - to stop the data.sql from running before the tables are created

jparepository -inteface extends this to define crud operations on entity, needs two things- entity name  and id data type..methods like findall, findbyid, save etc are available by default, custom method on two columns etc need to be explicitly defined with property name as mentioned in entityclass like findByFromAndTo
PagingAndSortingRepository - interface extends this to have paging and sorting by default..return Page instead of List and add pageable attribute to the method param like below. In the rest api, we can have params for size=20(default, number of elements per page), page = 0 (default), sort= parar, asc/desc
    /**
     * Find Tours associated with the Tour Package.
     *
     * @param code tour package code
     * @return List of found tours.
     */
    Page<Tour> findByTourPackageCode(@Param("code")String code, Pageable pageable);
spring data rest provides hypermedia driven rest apis by default based on the methods presnet in repository class. There is search method available, there are methods for findById, findByName(if included in repository)
@RestResource(exported = false) annotation can be set in repository method to not expose it..like to prevent users from saving or deleting data through rest api
hal browser to expolore microservices exposed through spring data rest
spring data rest vs spring mvc restcontroller- restcontroller useful when you are not using spring jpa repository, when you have algrotihms in your service, when you need specific business logic in service..when you do not want to expose entities because of some privacy reasons

feign for calling rest apis..declarative rest client..needs proxy interface to declare rest api method signature...needs hard coding of url like localhost:8080, localhost:8081

naming server need for removing hard coding of urls when calling rest apis..eureka namine server with client side load balancing component spring cloud loadbalancer..earlier ribbon was used...all the microservices register with eureka by including eureka-client dependnecy and definign eureka url in application properties..load is balaned bwteen multiple instances of mciroservices through feign
why eureka - easy integration with spring, rest based easy registration and discovery, default round robin load balancing in client, all services are not publicly exposed
eureka server has easy built in dashboard with replication and helps in not becoming single point of failure..spring-cloud-starter-netflix-eureka-server dependency and @EnableEurekaServer annotation and server.port=8761 and eureka.client.register-with-eureka=false (do not register naming server with eureka) and eureka.client.fetch-registry=false?
eureka client much like config client makes uses of spring.application.name and registers the application with naming server - spring-cloud-starter-netflix-eureka-client dependency and @EnableDiscoveryClient annotation
To consume load balnaced services registered in eureka, need to define eureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka

api gateway or edge server- earlier zuul was used..now no longer supported..for handling cross cutting concerns like logging, security etc..now spring cloud api gateway is used...we need to enable client discovery in api gateway to be able to call the urls with api gateway...also..we need to use service name as registered with eureka

unique identifier for web service geenration using spring cloud starter sleuth

distributed tracing using zipkin or elk stack

spring web flux?

circuti breaker pattern for fault tolerance if one of the microservice is down then a default behavior can be returned- earlier hystrix used and a fallbackmethod provided..now resilience4j is used..we can configure no. of retries for a service and interval/wait period between the retries and a exponentialbackoff between subsequent retries to true and fallback method which will be called after that...we can also configure a circuit breaker which is just return the fallback response insteda of actually calling the service. Cicruit breaker can be in 3 states - closed, open and half open..when the application starts , then the curcuit brekaer is in closed state, but when the service is down for all the requests, it goes to open state and after a configured or default wait period which try to call the service method for some percentage of requests like 10% by going in half open state . if those 10% request are successful, and then all request are successful, then it will go to closed state again.
using resilience4j, we can also do these:
we can configure ratelimit of how many requests per second can be allowed to a service. any number of rerquestrs after that will fail.
we can define a bulk head with max concurent requests allowed

Spring boot 2 -> Spring cloud starter sleuth (tracing configuration) -> brave (tracing library) -> zipkin
Spring boot 3 -> Micrometer ( for metrics, traces, logs) -> Opentelementry -> Zipkin 


Docker commands  : 
docker build imagename:version
docker run -p -d containerport:hostport --name containername imagename 
docker stats - to check the statistics of mermore, cpu in use
docker top - to check the top process running in the container
docker kill - issues sigkill - no graceful shutdown
docker stop - issues sigterm - graceful shutdown
docker push - to push to docker repository docker hub
docker tag - to associate tags with container
docker pull

spring boot maven plugin to build images with pullpolicy configuration of only_if_present
with maven build -> spring boot:build-image -DskipTests


********************************************************
Kubernetes - provides load balancing and service discovery (through environment variables) by default
kubectl command
kubectl create deployment deploymentname --image=imagename:versionname
kubectl expose deployment deploymentname --type=LoadBalancer --port=8080 
kubectl get pods / kubectl get po
kubectl get services / kubectl get svc
kubectl get replicaset / kubectl get rs
kubectl get events
kubeclt get deployments
kubeclt set image ?
kubectl get pod -o wide
kubectl get service -o wide
kubectl get deployment -o wide >> deployment.yaml
kubectl get service -o wide >> service.yaml
kubectl diff -f deployment.yaml
kubectl apply -f deployment.yaml
kubectl logs podname
kubectl create configmap svcname --from-literal=CURRENCY_EXCHANGE_URI=http://currency-exchange
kubectl rollout history deployment servicename
kubectl rollout undo deployment servicename --to-revision=1
kubectl scale deployment deploymentname --replicase=2
kubectl autoscale deployment deploymentname --min=1 --max=3 --cpu-percent=5 -> This creates horizontalpod autoscaler (hpa)
kubectl get hpa



Cluster has master nodes (use to manage the worker nodes) and worker nodes (which actually contain the application)
master node has 4 componenets (all components together are known as control plane)- 
distributed database ectd containing the desired state of kebernetes cluster like we want one deployment for helloword, 2 replicas for app2 etc. It is key value store. etcd
etcd is a highly reliable and distributed key-value store which is used to store the data regarding cluster state. It can be part of the master node or can be external in which case the master node connects to it.

api server - ? You can communicate with master node via kubectl or api server. kube-apiserver
The kube-apiserver connects to the Kubernetes API and helps to perform all the administrative tasks given by the user and stores the cluster state in etcd key-value store after all the executions are done . It is the front end on Kube control plane. It is scaled horizontally i.e it scales by increasing the number of instances.

scheduler - to schedule on which node which pod will be deployed, assigns work to worked pods

kubernetes controller - ensures that the actual state of the kuberntes cluseter is same as desired state. It includes node controller, replication controller
Kubernetes master can run on any machine but all master components must run on the same machine and no nodes should run in that machine.

worker node has 4 componnets
pods - which contains the containers where the applications are running..to have multiple instances of the application running we have replicaset which checks if a pod is down then it ups another pod. if we want to scale the deployment to use 3 replicas, replicaset ensures 3 pods are created. if we change the imgae of the deployment, then replicaset first creates a pod with the new image and if that is successful, then an existing pod is deleted and then another pod is created with new image and so on. this approach is called rolling update and is the default approach. services are used in kubernetes to make sure that there a single url exposed to the outside world for the application. Any change to pod and its corresponding ip change of the pod makes no change to the ezxternal url of the service. Service has external IP and pod has a internal ip. service is created when we expose the deployment. Service type can be load balanacer to balance load amongst pods, clusterIP from communication withing the kuberneters cluser and Nodeport?

proxy server/ kube-proxy
kube-proxy It is a kubernetes network proxy service that runs on every node, it is used to connect the application to the external world/environment. Instead of directly connecting to the pods to interact with the application Services are used.
A service groups related pods and is used for TCP and UDP load balancing.

kubelet
kubelet is a worker node component that runs on every worker node in a cluster and is used to communicate with the master node. It runs containers inside a pod according to pod-spec.
A pod-spec is YAML or JSON file that contains information regarding which containers should be run in the pod.
Pods are group of similar type of containers.
A kubelet interacts with the container-runtime with the help of CRI(container runtime interface). A CRI consists of two parts Image Service and Runtime Service. The Image service deals with the container images and the runtime service deals with the running of containers in a pod.

container runtime
Container Runtime Every container must have a container runtime, it is used to run and maintain containers in a node. Container runtime are tools or software that are used to create and run containers. Eg: dockers and rkt, containerd


Nodes are the worker machines in the kubernetes cluster that are controlled and managed by the Master. Nodes can be local machines or the VMs. Nodes contain Pods.The Services provided by the nodes are
Node Status
Management
API objects

Node Status contains the information about the node such as 1.Addresses: The addresses contain the
 **Hostname**: The hostname provided by the node's host kernel.
** External IP**: the IP address that is used to communicate with the node outside of its cluster.
 **Internal IP**: the IP address that is used for communication within the cluster.
Condition: The condition field is used to describe the status of the running nodes.
Capacity: The capacity field describes the resources available to the nodes and the maximum number of pods that can be scheduled onto the node.
Info: This field provides the generic information about the node, such as kernel version, Kubernetes version (kubelet and kube-proxy version), Docker version (if used), OS name. The information is gathered by Kubelet from the node.

Node Management
Management: Nodes are not directly created by Kubernetes, they are externally created or are already existing in the local machine. So Kubernetes just creates an object representing the node. after creation, Kubernetes checks whether the node is valid or not based on the metadata.namefield. If the node is valid it is eligible to run Pods and other cluster activities.

API Object: Node is the top level resource in the Kubernetes REST API.

There are two types of container networking specifications
CNM(Container Network Model)
CNI(Container NEtwork Interface).
Kubernetes uses the CNI specification for container networking and communication.
The CNI assigns IP address to the pods.

Inter Container communication
Container runtime along with the host OS creates an isolated network entity called as network namespace for every container. Containers inside Pod share the network namespace and communicate via localhost.

Inter Pod Communication
For pods to communicate with each other within the same node and with pods in different nodes, kubernetes doesn't allow Network address Translation(NAT). This is achieved by
creating routable pods and nodes, using the Google Kubernetes Engine.
Using Software Defined Networking, like Flannel, Weave, etc.



Kubernetes Objects
Objects are used to maintain the desired state of a cluster.

Object Fields: Objects have two fields
Specs: Define the desired state of the object.
Status: Defines the actual state of the object.

Describing Object:
Objects are described using a .yaml or .json file.
It contains the following requirements
apiversion.
kind/type of object
metadata
spec, the desired state of the object
$kubectl create command is used to create an object.
Every Object has an UID, a system generated string that uniquely identifies the object.

Pods are logical group of similar type of containers. It is the smallest unit of deployment in Kubernetes.
Pods have a well-defined lifecycle, they are recreated to maintain the desired state.
Every container outside the pod has a unique IP address.
Pods allow data sharing and communication within the containers as they have a shared network namespace. Pods are transient in nature and cannot replicate automatically so we use controller manager components like replication controller with them.

Service in Kubernetes is used to defines a logical set of Pods and a policy by which to access them. Services are used for communications among different pods. Service is defined using YAML or JSON file. The set of Pods targeted by a Service is usually determined by a LabelSelector. Services are used to expose the pods outside the cluster. Services are exposed in different ways using different ServiceSpecs such as
ClusterIP
NodePort
Load Balancer
ExternalName

Kube-proxy
Every Worker node runs a daemon process called kube-proxy
The kube - proxy keeps track of API server of the master node for addition and deletion of Service endpoints.
For every Service created in a worker node, the kube-proxy configures the IP tables rules to handle the traffic for its ClusterIP and forwards it to service endpoints
When the service is removed, kube-proxy removes the iptables rules on all worker nodes as well.

Service Discovery
Service discovery at runtime is an important concept because all communications from external world to Kubernetes occurs through Services.
Environment Variables: When the Pod starts on a worker node, the kubelet adds a set of variables called Environment Variables in the Pod for all active Services. For example, if we have an active Service called redis-master, having Cluster-IP 172.10.0.12 and exposing Port -7307, the environment variables created will be as shown
REDIS_MASTER_SERVICE_HOST=172.10.0.12
REDIS_MASTER_SERVICE_PORT=7307
REDIS_MASTER_PORT=tcp://172.10.0.12:7307
REDIS_MASTER_PORT_7307_TCP=tcp://172.10.0.12:7307
REDIS_MASTER_PORT_7307_TCP_PROTO=tcp
REDIS_MASTER_PORT_7307_TCP_PORT=7307
REDIS_MASTER_PORT_7307_TCP_ADDR=172.10.0.12
DNS : There are addon components in Kubernetes called DNS which creates and manages the DNS names for all the Services created. The format looks like my-svc.my-namespace.svc.cluster.local.

Cluster IP: Cluster IP is the default Service type, in this type of Service there is virtual IP that can be used for communication only within the cluster.This is default configuration for a Service. If type is set to ClusterIP in a service, which can only be accessed within the cluster.
NodePort: In NodePort Service there is a virtual IP address along with a port number in the range of 30000-32767, that maps to the respective Service from all the worker nodes.
A default port number is selected by the Kubernetes while creating this type of Service. NodePort type Service is used when we want to communicate with the external world. NodePort lets you access your service from outside the cluster. You use NodeIP along with NodePort to access pods.
Load Balancer: This type of Service uses external load balancer, the Cluster IP and NodePort for this type of Service is automatically created and the load balancing is done by external load balancer.This option is used to expose service to external load balancer provided by your cloud provider.
External IP: This type of Services are mapped to External IP address if it can route worker nodes. The External IPs are not managed by Kubernetes.This option maps service to provided external name (e.g., abc.xyz.com)

Service Type Example
The yaml given below shows how you can configure pods (my-app) to be accessed from your browser using NodePort.
kind: Service
apiVersion: v1
metadata:
  name: my-test-service
spec:
  selector:
    app: my-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8003
    nodePort:30080
  type: NodePort
The above yaml exposes port 30080 in host/node to access pod/s my-app.
You need to query http://YOUR_IP:30080 to access my-app running at port 8003.

Labels are key-value pair, that are used to organise and select attributes of an object. Ex
"metadata":{
   "labels":{
       "key1":"value1"
       "key2":"value2"
}}
Labels can be used to map organisation structure onto the Kubernetes Object. Ex:
"environment":"dev","environment":"qa"

Selectors are used to identify a sub-set of Objects. The kubernetes API supports two types of selectors.
equality based
set based
Empty label selectors select all the objects in the collection. A null label selector selects no objects
Equality-based requirements are used for identifying sub-sets of objects based on keys and values The =,==,!= are used for equality based selectors.
Ex: environment = production
Set based Requirments are used for filtering objects based on a set of values. There are three kinds of operators used in, not in and exists
Ex: environment in (production, dev)

Replication controller is a part of the controller- manager and it is used to produce replicas of the pods because pods are transient in nature and cannot replicate them selves.
If the number of Replicas are less they generate more replicas of the pods to maintain the desired state.
If the number of replicas are more they are destroyed

Deployment Object
A deployment object is used to define the desired state of an application.
Deployments can create new replica sets, delete other deployments and update deployments.

Deployment Controller
Deployment controller is a a part controller-manager of the Master node.
It is used to create and update pods and replica sets.
Deployment Controller converts the actual state to desired state

Namespaces: Namespaces are virtual clusters within physical clusters. They are used when users are spread across multiple teams or projects. The names inside a namespace must be unique but can be similar across namespaces.
Namepaces are created using YAML files. Ex:
apiVersion: v1
kind: Namespace
metadata:
  name: <insert-namespace-name-here>
the $kubectl create -f <namespace -name-here> command is run in CLI to create namespace. 

Volumes are the persistent state data that can be stored on-disk or on other containers.
Why use Volumes in Kubernetes?
You know that Pods are transient in nature and when the containers in a pod crash all the data inside themare deleted and new containers are created. The Data inside them is lost, to overcome this problem Volumes are used.
Volumes can store data on a storage medium or in othe containers.
Volumes are placed inside the Pods and are connected to the containers.

Persistent Volumes
A PersistentVolume (PV) is storage space created in the cluster by the administrator.
It is similar to other cluster resources like the nodes. PVs are volume plugins like Volumes, but unlike volumes inside a container, they have a lifecycle independent that of the lifecycle of the pod.
The API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.
PersistentVolumeClaim (PVC) are storage requests created by the user. Users request for Persistent Volumes based on their requirments. Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim.
Like Pods consume node resources, the PVCs consume Persistent Volume resources. Pods can request specific levels of resources (CPU and Memory).

A pod is the smallest unit of Kubernetes that can be deployed and managed. A pod is a collection of similar kind of containers that share storage and network and specification how to run them. A pod’s contents are always co-located and co-scheduled and run in a shared context.
Containers inside a pod share the same IP address and port space and can communicate with each other via localhost. They can also communicate with each other using standard inter-process communications like semaphores or POSIX shared memory. Containers in different pods have distinct IP addresses and cannot communicate by IPC without special configuration. These containers usually communicate with each other via Pod IP addresses.

Pod Lifecycle
Pods are transient, pods are created, assigned a unique ID (UID), and scheduled to nodes where they remain until termination or deletion. If a node dies, the pods scheduled to that node are scheduled for deletion, after a timeout period. A pod is never rescheduled to a node instead, a new pod with a new UID is assigned to the node. the following are the terms used to define a Pod Lifecycle.
Pod Phase : The Pod phase is a simple yet high-level summary of where the pod is in its lifecycle. The different values of Pod phase are Running, Succeeded, failed and Unknown.
Pod Condition : is an array through which the Pod has to pass (PodScheduled, Ready, Initialized, Unscheduled, ContainersReady) . Each element in Pod Condition has six possible fields.
lastProbeTime:
lastTransitionTime:
message:
reason:
status:
type

STATUS field indicating the current status of the pod.
This STATUS field is PodStatus object which in turn has phase field.
Possible Values for STATUS are :
Pending: Pod is ready to be deployed, but container images are being downloaded.
Running: Pod is assigned a node, and at least one container is running inside the pod.
Succeeded: All containers inside a pod are successfully terminated, and will not be rescheduled again.
Failed: A container inside the pod has exited with a non-zero exit code.
Unknown: Error in communicating with Pod's host.

Container Probes : is a health check performed by the kubelet on a container. Handler is used to perform the check. There are three types of handlers
ExecAction:
TCPSocketAction
HTTPGetAction
Pod Lifetime: Pods are not deleted and destroyed automatically unless they are destroyed manually or by a controller but there is an exception to this when the pod with a phase succeeded or failed for more than a specific duration it will expire and automatically destroyed.

Deployment and Deployment Controller
A Deployment controller is used to create and update Pods and ReplicaSets.
Deployment controller changes the actual state to the desired state as described in the deployment object. It can create new RepilcaSets, remove existing deployments and create new deployments.
Deployment Controller uses:
Creating deployments for new Replicasets
Declaring desired states of the pods
Rolling back deployment to an earlier version
Scaling up the deployment
To pause or stop deployments
To check the status of deployment
Clearing old ReplicaSets

Creating Deployments: A deployment is created using a YAML file an then running the following command on Terminal.
$kubectl create -f <file_name>.yaml
To see the created deployments
$kubectl get deployments

ConfigMaps are used to define the configuration of a container image.
They are used to separate the configuration details from the container image, so that you do not have to create multiple images for similar applications, instead you can create multiple ConfigMaps.
ConfigMaps can be used by Pods and replication controllers to create multiple containers.
There are two ways in which configMaps can be created
from literals
from files

Creating configMaps files using literals
To create configMaps using literals we use the kubectl CLI. Execute the following command to create a configMap.
$ kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2
configmap "my-config" created 
You can get the configuration information by executing the following command
$ kubectl get configmaps my-config -o yaml

Creating the configMap using YAML file
This method of creating a configMap is more preferred because it gives you full control to change the different components of the configMap. The YAML file structure looks like this
apiVersion: v1
kind: ConfigMap
metadata:
  name: imageName
data:
        image data
To create the configMap file create a yaml file and run the kubectl create command Ex: $ kubectl create -f imageName-configmap.yaml

Secrets
Secrets are volume spaces that are used to store sensitive information of the application.
They are created using the kubectl create command
 $ kubectl create secret generic userpassword --from-literal=password=loginpassword
This creates a secret called userpassword that stores the login password.

Secrets can be created manually by using YAMl files and encoding the information. -Ex:
$ echo loginpassword | base64
bG9naW5wYXNzd29yZAo=
The YAML file
apiVersion: v1
kind: Secret
metadata:
  name: userpassword
type: Opaque
data:
  password: bXlzcWxwYXNzd29yZAo=
  
  
Create a pod object using kubectl run command with google's sample image: gcr.io/google-samples/kubernetes-bootcamp:v1 and expose it on port 8080, name the pod as firstapp.
kubectl run firstapp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080
kubectl create deployment firstapp --image=gcr.io/google-samples/kubernetes-bootcamp:v1

Check if the pod creation is successful by running the command: 
kubectl get pod firstapp

Expose the application to the local VM by creating a Service object of type NodePort.
kubectl expose pod firstapp --port=8080 --type=NodePort

Check if the service is created by running the command: 
kubectl get svc firstapp

Create another deployment using a 'YAML' file, create a deployment.yaml file that contains information of the number of replicas and the images to be used. Use an nginx image to deploy. Name the deployment as 'nginx'

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
		
kubectl apply -f deployment.yaml

Check if the deployment is created by running the command: 
kubectl get deployment nginx

Create a NodePort type service using a 'YAML' file, create a service.yaml file that contains information of the type of service and the port numbers. Name the Service nginx-svc and use port 30080 for nodePort.

apiVersion: apps/v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30080

Check if the service is created by running the command: 
kubectl get svc nginx-svc

Use kubectl exec command to get inside the running nginx pod and write the string 'Welcome to fresco nginx pod' to /usr/share/nginx/html/index.html file
kubectl exec --stdin --tty nginx -- /bin/bash
echo 'Welcome to fresco nginx pod' > /usr/share/nginx/html/index.html

You can specify how and when kubernetes should pull images by specifying imagePullPolicy in Pod description yaml. Accepted values are:
IfNotPresent: This is the default policy which specifies images should only be pulled if it is not already present in the node.
Always: No matter whether the node has the image or not, pull the image from the repository each time.
Even though you can use : latest tag with image and can omit imagePullPolicy, it is not recommended, because any change to the image could disturb your application functionality.

Images can be stored in private repositories to maintain the confidentiality of your code.
For Kubernetes (kubelet) to access these repositories, cluster admin explicitly needs to provide these credential information to Kubernetes.
There are various registries and ways to configure kubelet to pull the image from private repositories. They are:
Using Google Container Registry: Authentication can be done using Google service account.
Using AWS EC2 Container Registry (ECR): Kubelet automatically fetches the image using ECR credentials.
Using Azure Container Registry (ACR): Standard Docker authentication is used.

Configuring Nodes to Authenticate to a Private Registry. - First login to your private repository by running.
docker login [repository_url]
Above step updates $HOME/.docker/config.json
Get all Nodes in your cluster.
Get by Names
nodes=$(kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}')
Get by IPs
nodes=$(kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type=="ExternalIP")]}{.address} {end}')
Copy the docker config to each node.
for n in $nodes; do 
 scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json;
 done
 Now you can verify the above steps by creating a Pod that pulls an image from a private repo.
kubectl create -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: private-image-testing
spec:
  containers:
    - name: private-image-test
      image: $YOUR_PRIVATE_IMAGE_NAME
      imagePullPolicy: Always
      command: [ "echo", "SUCCESS" ]
EOF

Specifying ImagePullSecrets on a Pod.
Image Pull Secret in Pod - Approach 1
It is also possible to specify image pull secrets at pod level. This is a recommended approach if you are using cloud provider like GKE (Google Kubernetes Engine) where node creations are automated.
Follow these steps to specify secret at pod level:
Create a secret with Registry credentials.
kubectl create secret docker-registry myregistrysecret --docker-server=DOCKER_REGISTRY_SERVER_URL --docker-username=DOCKER_USER_NAME --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL_ID
This will output secret/myregistrysecret created . If you have multiple registries to access, You can always create a new secret and Kubernetes will take care the rest.
Create Pod deployment yaml.
apiVersion: v1
kind: Pod
metadata:
  name: app_name
  namespace: name_space
spec:
  containers:
    - name: app_name
      image: YOUR_PRIVATE_IMAGE
  imagePullSecrets:
    - name: myregistrysecret
Now deploy Pod with Image Secret.
kubectl create -f <above_pod_spec.yaml>

Image Pull Secret in Pod - Approach 2
If you are not comfortable with creating secrets by kubectl or you want to have more control over your secrets, you can use yaml or json to deploy the secret to pods.
Follow these steps to deploy secret using yaml and use it with Pod.
Encode .docker/config.json content to base64.
cat $HOME/.docker/config.json | base64
The above command will give base64 version of docker config. save it somewhere. 2. Create secret yaml file.
apiVersion: v1
kind: Secret
metadata:
  name: myregistrysecret
  namespace: name_space
data:
  .dockerconfigjson: COPIED_BASE64_STRING   
type: kubernetes.io/dockerconfigjson
Deploy the secret.
kubectl create -f <above_secret_file.yaml>
Create Pod deployment yaml.
apiVersion: v1
kind: Pod
metadata:
  name: app_name
  namespace: name_space
spec:
  containers:
    - name: app_name
      image: YOUR_PRIVATE_IMAGE
  imagePullSecrets:
    - name: myregistrysecret
Now deploy Pod with Image Secret.
kubectl create -f <above_pod_spec.yaml>


Container Hooks
Kubernetes exposes two event hooks to containers they are:
PostStart: This hook gets executed as soon as the container starts. However, it may not get executed before its ENTRYPOINT, and also Kubernetes will not pass any parameter to this hook.
PreStop: This hook is invoked by kubernetes just before the container is removed. It is a blocking call (Synchronous). Hence, a container will not be terminated until it executes completely.
Follow these steps to implement container lifecycle hooks:

Add hook property to pod description yaml.
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-hook-demo
spec:
  containers:
  - name: lifecycle-hook-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from Fresco Team> /usr/share/message"]
      preStop:
        exec:
          command: ["/usr/sbin/nginx","-s","quit"]
Create the Pod.
kubectl create -f <above_config.yaml>
To verify the step, get inside the container.
kubectl exec -it lifecycle-hook-demo -- /bin/bash
Use cat the file to see a message
 cat /usr/share/message
You should see Hello from Fresco Team message.


When to Use Multiple Namespaces
When your project is big and consists of hundreds of developers, it is advisable to create a new space for each team so that they can work in isolation.
Namespaces provide scope to names (labels/pods), Names of pods need not be the same across the namespaces.
Resource quotas can be applied to namespaces at one shot.
It is not a good practice to use namespaces just because they are provided by Kubernetes. Namespaces should only be used if it is necessary.

Creating a NameSpace
Create yaml file of kind: namespace.
apiVersion: v1
kind: Namespace
metadata:
  name: testns
Create it by using 'kubectl`.
kubectl create -f  <above_file.yaml>
Verify namespace creation by executing
kubectl get namespaces

Whenever you deploy a pod to Kubernetes, it gets deployed to default namespace.
So if you want to deploy a pod to any other namespace, you explicitly need to specify the same.
You can use the following command to deploy new pod to your namespace.
kubectl run nginx --image=nginx --namespace=testns

You can also use pod's yaml to specify to which namespace it should get deployed.
apiVersion: v1
kind: Pod
metadata:
  name:lifecycle-hook-demo
  namespace: testns
spec:
  containers:
  - name: lifecycle-hook-container
    image: nginx

The difference between ReplicaSet and Replication Controller is that ReplicaSet supports set based selectors whereas Replication Controller supports equality-based selectors.

Kubernetes Object Management
There are three ways to manage Kubernetes Objects.

Imperative Commands: Operator Operate directly on live objects in the cluster.
Run a single instance of Nginx
kubectl run nginx --image nginx
Even though It is simple to operate, it is not recommended for production because commands do not provide an audit trail or source control over deployments.

Imperative object configuration: Here we specify operations like create, replace, delete and at least one yaml/json file with configurations.
Create a Deployment
kubectl create -f nginx-depl.yaml

Declarative object configuration: In this method, the user does not specify the operation to be performed on a file.
Apply changes
kubectl apply -f some-depl.yaml
Deployment Controller uses Declarative object configuration method.

Deployment Controller provides a declarative way of updates to ReplicaSets and Pods.
You can specify the desired state in Deployment Object, and it is the Deployment Controller's responsibility to maintain that state at any cost.
You should not manage ReplicaSets created by deployment controller. Deployment Controllers internally manage them.


Updating a Deployment
Once your Pods are deployed, if any changes to the deployed pods are required, you can achieve them either by changing yaml file and redeploying it or using following commands: You can fetch deployments by running
kubectl get deploy
To Update the image
$ kubectl set image deployment <deplyoment name> <container-name>=nginx:1.9.1 --record
You can run kubectl rollout status <deployment name> to check status of rollout.
Scaling Deployment
kubectl scale deployment <deployment name> --replicas=10
Pausing a Deployment
kubectl rollout pause deployment <deployment name>
Resume a Deployment
kubectl rollout resume deployment <deployment nm>
You can use kubectl rollout status deployment <deployment name> to check status of your change. You can use kubectl rollout history deployment <deployment name> to check history of your change.

Complete Deployment
Kubernetes tags a deployment as complete if:
All updates provided have been completed.
All Replicas are up and running.
No old replica of deployment is running.
You can run kubectl rollout status on your deployment to check the current status of your deployment. If all rollouts are complete, it returns 0 exit code.

Failed Deployment
A deployment may fail because of:
Insufficient quota
Error with image pull
Readiness probe failures
Insufficient permissions
Runtime Configuration Error

Failed Deployment
You can detect deployment failure by specifying .spec.progressDeadlineSeconds in your deployment yaml file. It specifies the number of seconds the Deployment Controller has to wait before indicating that deployment is stalled.
You can use patch command of kubectl to acheive this functionality as well.
kubectl patch <deplyment name> -p '{"spec":{"progressDeadlineSeconds":600}}'
The above command sets progressDeadlineSeconds to 10 minutes.

Usually, services access pods. However, they can also be configured to access other kinds of backends.
The need could be:
You would want to keep an external DB cluster in production, but to test you want to use your private DB.
To point your service to another service which is in another namespace or cluster.
You want to migrate workload to Kubernetes and some backends still run outside the Kubernetes.
This is how you can define a Service without any Selectors.
kind: Endpoints
apiVersion: v1
metadata:
  name: my-test-service
subsets:
  - addresses:
      - ip: 1.2.3.4
    ports:
      - port: 8003
Above Service routes every traffic to 1.2.3.4:8003

Sometimes, Services need to expose multiple ports, Kubernetes Service Definition supports this as well.
kind: Service
apiVersion: v1
metadata:
  name: my-test-service
spec:
  selector:
    app: my-app
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 8003
  - name: https
    protocol: TCP
    port: 443
    targetPort: 8004
	
Headless Services
If you don't want load balancing or single service IP, you can go with Headless services. This can be achieved by setting NONE value to .spec.clusterIP in service yaml.
With selectors
If a headless service defines selectors, Endpoints records in API is created, and DNS returns set of IP addresses to point a set of pods.
Without selectors
If no Selectors are defined with service, no EndPoints records will be created. However, DNS looks for any ExternalName-type services.

StatefulSets
Kubernetes provides a workload API to manage Stateful Applications, which is known as StatefulSet.
StatefulSets manage deployments and pods scaling along with guarantying ordering and uniqueness of pods.
StatefulSets maintain a sticky identity of each pod. Even though pods are created with the same specs, they are not interchangeable according to StatefulSets.
StatefulSets works just like any other Controllers. You need to define the desired state in a Stateful Object. StatefulSet Controller takes responsibility of maintaining that.

A StatefulSet consists of these components:
A headless Service created to handle network domain.
A spec field that specifies the replicas to be created.
A volumeClaimTemplates used for stable storage (uses PersistentVolumes ).

First we create a Headless Service.
# Headless Service
apiVersion: v1
kind: Service
metadata:
  name: nginx-test
  labels:
    app: nginx-test
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None 
  selector:
    app: nginx-test
Service is made headless by adding clusterIP: None.

After creating Headless Service create a StatefulSet.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx-test # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 2 # by default is 1
  template:
    metadata:
      labels:
        app: nginx-test # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image:nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
Above StatefulSet creates two replications of Nginx pod.
Persistent Volume Claim is used to maintain a stable state.

Kubernetes provides another Controller named DaemonSet, which ensures each pod (or some) is running a copy of a pod.
As and when nodes are added to our cluster, DaemonSet adds Pods to them. As Nodes are destroyed/removed, Pods are garbage collected.
This is a sample DeamonSet looks like:
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: test-daemon
  namespace: kube-system
  labels:
    k8s-app: node-logging
spec:
  selector:
    matchLabels:
      name:node-daemon
  template:
    metadata:
      labels:
        name: node-daemon
    spec:
      containers:
      - name:node-daemon
        image:some-image
To run pods only on some nodes use .spec.template.spec.nodeSelector.
A DaemonSet is a controller that ensures that the pod runs on all the nodes of the cluster.Some typical use cases of a DaemonSet is to run cluster level applications like:
Monitoring Exporters: You would want to monitor all the nodes of your cluster so you will need to run a monitor on all the nodes of the cluster like NodeExporter.
Logs Collection Daemon: You would want to export logs from all nodes so you would need a DaemonSet of log collector like Fluentd to export logs from all your nodes.
However, Daemonset automatically doesn’t run on nodes which have a taint e.g. Master. You will have to specify the tolerations for it on the pod.

Both Deployments and DaemonSets are similar to each other. Both take responsibility of managing Pods and try to maintain the desired state.
Use Deployments for Stateless Services (front ends), where scaling up and down won't affect Application's functionality.
Use DaemonSets when it is essential to run pods on each node (or some nodes), and they have to start before any other pods.

https://semaphoreci.com/blog/replicaset-statefulset-daemonset-deployments

Kube DNS
Kubernetes Schedules a DNS Service and a DNS pod and tells kubelet to query DNS IP for any name resolutions.
Every service created/defined in a cluster is assigned a new domain name.
If a Pod is running in the same cluster and namespace that of a service, it can directly call service using its name. Otherwise, it has to add the namespace to its lookup.
E.g., let us assume Service is named foo and it belongs to bar namespace. If a pod wants to connect to foo and also belongs to bar namespace, it can address the service directly using foo. If Pod is running in xyz namespace, it has to use foo.bar to access the service.
Kube DNS watches Kubernetes API for any kind of changes.
As soon as a new service is created, Kube DNS makes a DNS Entry in the form of,
<service_name>.<namespace_name>.svc.<domain_name>
Each of these DNS entries will be mapped to the IP address of the service.
When another service/pod wants to call this service, it has to either use this service name (if both are in the same namespace) or it has to use a fully qualified name (if namespaces are different).

Ingress is an API Object that is used to manage external requests to cluster's service.
Ingress can provide SSL Termination, Load Balancing, and Name-based Virtual Hosting.
we already have LoadBalancer as service type, why do we even need Ingress? if you want to expose more than one service to the internet, If you use service type LoadBalancer, Your cloud provider will create one load balancer for each service, and that's too expensive.So we use ingress as a wrapper to those services and use only one cloud load balancer.
Ingress sits between service and Internet and redirects every request to service based on ingress rules.

You can use Single Service Ingress if you want to expose only one service to the outside world.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  backend:
    serviceName: test-svc
    servicePort: 80
The above ingress redirects every request to test-svc running at port 80.

Fanout Ingress routes traffic from single IP to multiple services based on filtering rules specified.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: simple-fanout-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    http:
      paths:
      - path: /testpath
        backend:
          serviceName: test-svc
          servicePort: 80
      - path: /anotherpath
        backend:
          serviceName: another-svc
          servicePort: 8001
nginx.ingress.kubernetes.io/rewrite-target: / indicates path matching will start from the root path.
path: /testpath indicates filtering to be applied after the domain name.
backend object specifies to which service the path should be redirected to.
The above ingress routes all http://DOMAIN_NAME/testpath to test-svc at port 80, and http://DOMAIN_NAME/anotherpath to another-svc running at port 8001.


apiVersion: v1
kind: Namespace
metadata:
  name: frescons
  
kubectl create -f  ns_frescons.yaml
kubectl get namespaces

kubectl run nginx --image=nginx --namespace=testns

Let's Create a Service for MongoDB
apiVersion: v1
kind: Service
metadata:
  name: mongo
  namespace: frescons
spec:
  ports:
    - port: 27017
      targetPort: 27017
  selector:
    app: mongo

kubectl create -f  mongo_service.yaml
	
Once the service is created we can create mongoDB Deployment.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  namespace: frescons
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        image: mongo:4.1
        ports:
        - containerPort: 27017
          protocol: TCP
		  
kubectl create -f  mongo_deployment.yaml
		  
NodeJs Server Service
Let's create a Service First.
apiVersion: v1
kind: Service
metadata:
  name: node-mongo-page-hit
  namespace: frescons
spec:
  selector:
    app: node-mongo-page-hit
  type: NodePort
  ports:
    - port: 8000
      targetPort: 8000
      nodePort: 30800
The above Service opens NodePort at 30800 and forwards requests to our nodeJS app running at port 8000.
kubectl create -f  nodejs_service.yaml

Now Create NodeJs Server deployment descriptor.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-mongo-page-hit
  namespace: frescons
spec:
  replicas: 1
  selector:
    matchLabels:
      app: node-mongo-page-hit
  template:
    metadata:
      labels:
        app: node-mongo-page-hit
    spec:
      containers:
      - name: node-mongo-page-hit
        image: fresco/node-mongo-page-hit:latest
		imagePullPolicy: Never
        ports:
        - containerPort: 8000
          protocol: TCP
        env:
        - name: PORT
          value: '8000'
The above Deployment Descriptor creates one pod of our nodejs service which listens at port 8000.
kubectl create -f  nodejs_deployment.yaml

Nginx Service
Let's create a Service First.
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: frescons
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30080
The above Service opens NodePort at 30080 and forwards requests to our nginx running at port 80.
kubectl create -f  nginx_service.yaml

Now Create Nginx Server deployment descriptor.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: frescons
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          protocol: TCP
		lifecycle:
		  postStart:
			exec:
			  command: ["/bin/sh", "-c", "echo Hello from Fresco Team> /usr/share/nginx/html/index.html"]
The above Deployment Descriptor creates one pod of our nodejs service which listens at port 8000.
kubectl create -f  nginx_deployment.yaml

Minikube installation

sudo apt install docker.io -y 
sudo systemctl unmask docker
sudo service docker restart
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
sudo dpkg -i minikube_latest_amd64.deb
rm -rf minikube_latest_amd64.deb
Follow the steps to complete the handson

Your task is to spin up three applications/pods inside a kubernetes cluster. A node-mongo-page-hit application that connects to a mongo db pod and an independent nginx pod.

Step - 1

Check whether docker & minikube are properly installed and configured. Start Minikube and execute this command to sync host docker with minikube docker eval $(minikube docker-env)
Step - 2

Create a folder named kubernetes-page-count in Desktop and build a docker mongo image with name fresco/node-mongo-page-hit:latest

Enable ingress addon in minukube. Create an Ingress router that will route the following paths to respective apps
name: fresco-ingress, namespace: frescons
path: /nginx, backend: {serviceName: nginx, servicePort: 80}
path: /app, backend: {serviceName: node-mongo-page-hit, servicePort: 8000}

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: fresco-ingress
  namespace: frescons
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    http:
      paths:
      - path: /nginx
        backend:
          serviceName: nginx
          servicePort: 80
      - path: /app
        backend:
          serviceName: node-mongo-page-hit
          servicePort: 8000

Step - 7
Hit http:localhost/nginx and http:localhost/app and see if everything works fine.
Use curl with below commands to check.
curl -kL http:localhost/nginx should return Hello from Fresco Team
curl -kL http:localhost/app should return page hit count is: 1
********************************************************

service registration and service discovery needed for miscorservices
Service registration, in simple terms, is the location of a service that is stored in a registry accessible to all the client applications.
Service discovery is the process of querying the service registry and discover the needed service.

Service registry contains host, ip, authentication, version, protocol. A service registry is useful because it enables client-side load-balancing and decouples service providers from service clients without the need for DNS. Eureka, Zookeeper, Etcd, SmartStack, NSQ, Serf, Spotify DNS, and SkyDNS are some of the service registries in use.

service registrtaion types - 
self registration - service itself registers with the registry when it goes up or down..code becomes complex ..suitable for simple architecture
third party registration - 3rd party registration where a third party tool registers service instances with the service registry. Every microservice instance is polled for the service status and are updated in the service registry automatically. Additional information from the microservice may also be updated in the registry using config files.ex - Netflix Prana and AWS Auto Scaling Groups

Service discovery types- 
Client side discovery - This pattern involves having the service clients query the service registry, determine the network locations of the service instances. The client then uses a load-balancing algorithm to select one of the available service instances and make a request.With a client side load balancer, the client is aware of the multiple possible instances and chooses an instance based on a policy, such as preferring an instance that's in the same data center to reduce request latency. Netflix Ribbon is an example for client side discovery tool that works with Eureka to load balance requests across the available service instances.

Server side Service discovery - In this pattern, the service client makes a request to the load balancer, which in turn, queries the service registry for the service instance location. The load balancer then routes the client request to the appropriate service instance. AWS Elastic Load Balancer (ELB) is an example following this pattern with ELB acting as the router as well as service registry.

Types of distributes computing/ system -
Master slave
Peer to peer

CAP Therorem -
Consistency - Every read would get you the most recent write
Availability - Every node (if not failed) always executes queries
Partition-tolerance - Even if the connections between two nodes are down, the other two nodes promises, are kept.

Service discovery in distributes system -
In distributed environment, service registry is not just used to identify the available services and its instances, but is also critical in maintaining consistency of information across all the instances of a particular service type. In other words, when there is a 'write' request that has to happen on a service instance, the same information has to be written on the other instances of the same service. Else, inconsistency among the instances (aka nodes) creep in.To handle this, distributed systems often resort to identifying a leader node. The leader node then ensures that every 'write' entry is passed to its other follower nodes (service instances). The leader has the responsibility of ensuring that all entries are at least written by a majority of nodes. This is done by consensus among nodes. 
How is leader election done - assumptions : each node has a unique number 2 alogrithms - 
bully algortihm (the node which notices failure sens ELECTIOn request to other nodes higher in number. Other nodes respond with OK and then the node with the highest number is elected and sends WINNER message to other nodes
ring algortihm (the node which notices failure sens ELECTIOn request to successor nodes appending its node number and finally the request recahes the initiater node and then the node with the highest number is elected
consensus achieved - regular consensus and unfirm consensus - properties of consensus are Validity , Agreement, Termination, Integrtity..paxos algorithm is used


High availability of service registry is done using
Service Registry / Service Discovery setup in a cluster - When the Service Registry is deployed in a cluster, the servers in the cluster exchange the registry information while striving for consistency.
client side caching - Client pulls discovery information regularly from the registry and caches it locally. It basically has the same view on the system as the Server. In case, if all Servers go down or the Client is isolated from the Server by a network partition, it can still behave properly until its cache becomes obsolete.

https://ecd-plugin.github.io/ecd/

Microservices - small independent deployable applications..small services which are loosely coupled with bounded context..each service handles one task like fetching customer data from backend..each service can be developed and deployed independently..each service communicate using lightweight protocols... standard HTTP methods like GET, POST, DELETE, PUT
ployglot persistenece - not being limited to RDMS for services, like using MongoDB for search service, SAP for some service, Paypal for payment service etc.
Microservices are not orchestrated but choreographed
nanoservices and teraservices - microservcies should not be too small and should not be too big
advantage of monolithic is code reusability, less overhead in deployment, easy to extend features and apis
disadvantage of monolithic - horizontal scaling not possible, rollback of partial deployment not possible, entire system can go down if one service down (circuit breaker pattern not possible), locked to language and farmeowrk and if sharepoint is to be used and app is in java it is not easily possible
advantage of microservice - suitable for agile development, can use diff technology for development, easy maintainance in terms of onboarding new devlopers as they can learn one service and then proceed, developed and deployed independently, entire system does not go down if one service down, scaling of one service possible
challenges of microservice - overhead of configuration management, monitoring of microservices, transaction management across microservcies, flow visulatization of application, logging of microservices, automation of deployment, 
drawbacks - possible code duplicacy, new version deployment harder, harder to test, complexity at network layer

netflix oss - open software suite from netfix for mciroservices - eureka for service registration and discovery, hystrix for circuit breaker, ribbon for load balancing, zuul for api gateway, hystrix dashboard for monitoring

domain driven design artifcats- entities, value objects, services, aggregates, repository, factory, module

service mesh - when kubernetes is managing the scaling up and down of servcies/ containers, it manages which services are there and their addresses. in that case, kubernetes engine manages service discovery through service mesh most commonly used are ignio / envoy. In the case when we do not have control on infrastructure and servcies are scaled up and down separately, then separate service registration and discovery server is used.

An API gateway is like a moderator.It offers security, governance, integration, and acceleration for API. An API call will be received by the gateway and will route it to the appropriate microservice. There can be separate api gateway for each client device - desktop/ mobile. An API Proxy will simply add Transport Security and Monitoring capabilities to an existing API.It does not add anything new, just re-exposes the existing API with some additional capabilities.A gateway has more functionality on top of API proxy, including content mediation and transformation, rate-limiting, caching, etc.Even though API Proxy is lightweight and offers the same functionality as Gateway, a well-built gateway will offer lightweight proxy with more capabilities.An API Manager can include an API Gateway as part of its features but will include other things like OAuth, caching, role checking, etc.API Gateway is just one tool amongst many in an API Managers package.
adv-encapsulates internal structure of application, client does not need to know about location of services, logic for calling service is moved from client to api gateways
diadv- additional overhead to maintain highly availabel service, need to update api gateway with microservice end point, additional response time due to network hop

12 factor principle:
codebase - must be version controlled and can be deployed many times - we push source code to repo..automated build kicks in and tests are run..then image is built from code and pushed to repository- docker hub or artifactory...each application should have a single code base witha root commit but can have many deploys (individual developer build and eploys from feature branch..when it is tested there, it is moved to non prod branch and then ultimately to prod branch).
dependencies - must be declared and isolated...normally managed via maven, gradle etc..in kubernetes world, the containes which are dependent are run in the same pods and this is known as sidecar pattern...applicatiions should not share common dependency..this will help when updated version of dependency is needed by one application but not others, otherwise dependency changing might break other applications. Same can also happen if underlying OS is upgraded or changed. Each application should run in its own sandbox. application should not rely on underlying run time environment and should be able to run in its own container.
configuration - must be stored in a env via config files or env variables or through user provided services..in kuberntets stored in config maps (insensitive data) or secrets (sensitive date) and then provided to pods through env variables or files. for more securing sensitive data, hashicorp vault is used. Configuration is anything which is specific to the environment and not the application. Software should be free of credentials, internal urls, internal information about environments. Spring provides externalized configuration through bootstrap files. It is bit tricky when migrating legacy to cloud native as if you miss to externalize config, the code still runs. But if you miss a dependency, the code could not be built.
backing services - whether DB., SMTP or any other service like twitter bot must be treated like internal service. It is any service which is communicated over network, be it database connections, cache providers, file systems like SFTP or S3, email services. The key is to treat such remote services like local services. These remote services are bound to the application through some urls mentioned in the configuration mentioned above. This allows in swapping services in different environment. Instead of creating connections to the service manually, it should be injected through the container. Binding these backing services to the application at run time instead of compile team is needed.
build , release and run - ability to have multiple builds, versioning, rollback..tag container image with build number or version number and containers can be grouped and deployed using deployemtn, replicateset, daoemonset. helm is used for version control. Application code should have a build. Code change will trigger a build and build or configuration change will trigger a release. each release should have a timestamp associated with it. it is possible to trigger new release based on only config change and no code change. CI CD should be used to not repeate steps when doing build, release and run multiple times
processes - execute application as stateless process, challenges in handling sticky sessions and caching. Mermory usage should be single threaded and short lived. Instead of sticky sessions, should cache session information in some database. instead of in memory systems or file systems, use databases. Use caches for short term memory usage
port bindings - expose services via port bindings..so we can have multiple services running on a machine by specfying port through env variables or configuration etc. in distributed environment, application routing framework handles the port for us, so we need not to worry
concurrency - horizontal scaling instead of vertical
disposability - quick appn startup and shutdown to minimize time against crashes, short lived processes for security. application startup should be quick. Anything which can be delegated to after startup should be moved to other routine. application shutdown should be graceful. This involves writing objects to database and not accepting new database connections etc. very good error handling and crash mechanism
dev/prod parity - appn is treated in the same way in dev, test and prod. there whould not be wide gap between prod and non prod environments. it means that they should differ by only a few features max and code should be deployed to prod more fast.
log management - treate logs as stream of events..use a log router and then route logs to elastic search or splunk...a standard log format across applications helps in log aggregation with respect to timestamp and other things. For ex- log output in json format makes it easier to read in ELK
admin tasks - should be handled like rest of appn

Liveness Probe—indicates if the container is operating. If so, no action is taken. If not, the kubelet kills and restarts the container. Readiness Probe—indicates whether the application running in the container is ready to accept requests.Failing liveness probe will restart the container, whereas failing readiness probe will stop our application from serving traffic.

Helm is package manager for kubernetes much like maven for java projects. It is used to manage deployments as they get bigger and more complex and it becomes difficult to identify the issues. In there, you can inject some varaibles. There are some values with .Release, .Values etc and those are picked from different yml files. You need to install helm and then helm has a client component which is the cmd line utitlity we run and server side component which is run in kuberneted (tiller-?). 

While ingresses and load balancers have a lot of overlap in functionality, they behave differently. The main difference is ingresses are native objects inside the cluster that can route to multiple services, while load balancers are external to the cluster and only route to a single service.
https://www.baeldung.com/ops/kubernetes-ingress-vs-load-balancer#:~:text=While%20ingresses%20and%20load%20balancers,route%20to%20a%20single%20service.

Prometheus for metrics like cpu usage, product calls in Kubernetes
Fluentd for log monitoring with efk stack - elstaci search, fluentd and kibana
jaegar for log tracing


springdoc-openapi library provides swagger (open api specification tool) integration with string boot..we need to include dependencies for springdoc-openapi-ui for restcontoillers and springdoc-open-api-data-rest for apis provided through spring data rest
other libraries to include documentation includes spring-fox and swagger

to have spring-security, need to include spring-security-starter dependency..by deafult all the apis are secured and need login with username and password. If we want to have public apis, we need to have a custom WebSecurityConfig class extending from WebSecurityConfigurerAdapter and we ned to override the configure method to authorize requests matching some ant patterns to be permitted.
By deafult, when you call new UserNamePasswordAuthentication, it expects a bean extending UserDetailsService from spring and you need to override public UserDetail loadUserByUserName method. A class implementing UserDetailsService is invoked by the AuthenticationManager.

JWT - Json Web token - used for restful apis as these are stateless and we do not want to pass username and password with each api call
first api call to request authentication and get a JWT token (from the identity server by passing appid, client id, client secret)
subsequent api call with jwt token in http header
jwt token consists of header, payload and signature in the format header.payload.signature
header consists of type (jwt token) and hashing algorithm
payload contains exp, iat, subject and issuer and additonal tokens specifc to the application in key/value pairs like
{
  "sub": "FAST",
  "iss": "awa-rest-api",
  "iat": 1566498600,
  "exp": 1609439400
}
signature consists of hash of header and payload using a secret string embedded in the application
jwtauthfilter - invoked before controller and repository is initialized, its dofilter method checks if the token in header is valid or not using jjwt library included in pom.xml
https://www.viralpatel.net/java-create-validate-jwt-token/
@Preauthorize annotation marks certain methods with roles that an authenticated user may access.

to build docker images through maven build process, spotify maven plugin can be used. You can provide the base image, entry point with -D options as provided in dockerfile. There are options to overwrite images (in case of change, no need to remove and recreate the image) and also there can be tags to push image to docker repository like docker hub.


The @Repository annotation (introduced in Spring 2.0) and @Service annotation (introduced in Spring 2.5) are specialization of the @Component annotation. The main advantage of using @Repository or @Service over @Component is that it's easy to write an AOP pointcut that targets, for instance, all classes annotated with @Repository to include logging.

When using @Autowired on property names, it is difficult to provide mock implementation for testing. When using @Autowired on setter, it tightly couples the class creation with setting the dependencies. When using constructor injection, we need not define autowired annotation of the class has a single constructor. If there are multiple constructor, then we need to use annotation. Constructor injection is considered the best way in OOPs along with making the property as final.

The main difference between @Controller and @RestController is that in effect every method that responds to a @RequestMapping url and verb gets the @ResponseBody annotation added which allows JSON marshalling of the return object. Controller annotation class will have methods that would return a string which corresponds to the view

Spring Boot @SpringBootApplication annotation is used to mark a configuration class that declares one or more @Bean methods and also triggers auto-configuration and component scanning. It's same as declaring a class with @Configuration, @EnableAutoConfiguration and @ComponentScan annotations.
@EnableAutoConfiguration : enable Spring Boot's auto-configuration mechanism. The @EnableAutoConfiguration annotation enables Spring Boot to auto-configure the application context. Therefore, it automatically creates and registers beans based on both the included jar files in the classpath and the beans defined by us. Allows for configuration classes to be scanned dynamically, often based on jars loaded in classpath, driven off of spring.factories, AutoConfigureBefore and @AutoConfigureAfter annotation.
It automatically creates MVC beans - dispatcherServlet, adapters, ViewResolvers, HandlerMappings
@ComponentScan : enable @Component scan on the package where the application is located. Spring Boot application scans all the beans and package declarations when the application initializes. You need to add the @ComponentScan annotation for your class file to scan your components added to your project
@Configuration : allow to register extra beans in the context or import additional configuration classes.

@Configuration is a spring framework annotation and not strictly bound to spring-boot. It was introduced when spring started to allow programmatic creation of spring-beans as to move forward from xml definitions of beans.

@AutoConfiguration is a spring-boot specific annotation not commonly available in spring framework. The reason it exists, is for external providers that cooperate with spring-boot to be able to mark some classes in some libraries they provide with this annotation as to inform spring-boot that those classes could be parsed and make some initializations during start up of spring application automatically.
So if some regular programmer that develops some application happens to have kafka in dependencies then some beans will automatically be created and added in application context and will be ready for the programmer to use, although he has not defined any configuration for them. Spring-boot already knows this as the kafka provider has already informed by marking some class in the jar they provide with the annotation @AutoConfiguration.
For this reason @AutoConfiguration has some more powerful configurations available as before, after, beforeName, afterName as to allow the provider to specify when the configuration is applied during application startup if some order is necessary.
So this annotation is not to be used from some regular programmer that develops an application using spring-boot. It is for someone that develops a library that other users might use with spring-boot. One such example is kafka library.
For this to work in a spring-boot project @EnableAutoConfiguration is needed as well, to enable auto configuration.

From spring documentation
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added. For example, if HSQLDB is on your classpath, and you have not manually configured any database connection beans, then Spring Boot auto-configures an in-memory database.

@Configuration instead is to be used from some regular programmer that develops an application using spring-boot or spring-framework as to inform the framework for which beans should be created and how.
@AutoConfiguration was introduced in 2.7 with the idea to mark all auto-configurations with its dedicated annotation and move away from spring.factories for auto-configuration imports in 3.0. Usually, @AutoConfiguration classes automatically configure an application based on the dependencies that are present on the classpath. Those classes are generally marked as @ConditionalOnClass and @ConditionalOnMissingBean annotations that detect the presence or absence of specific classes. Other annotations are ConditionalOnBean, ConditionalOnProperty, ConditionalOnMissingClass, ConditionalOnMissingProperty

To create war from spring boot application, in pom.xml, change the packaging to war and extend SpringBootServletInitiliazer and override the configure method which is built on builder framework.  Tjis will be used when run as war.
protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {
		return builder
				.sources(RetinaApplication.class);
				}
Running as war need appcontext in url. so this works http://localhost:8080/appname/api.
	
This will be used when run as a jar.
	public static void main(String[] args) {
		SpringApplication.run(RetinaApplication.class, args);
	}
Running as jar does not need appcontext in url. so this works http://localhost:8080/api.
	
@ResponseBody will tell spring that the output is response body and not the name of the page to be rendered.

to use thymeleaf, we need to have the dependecy for spring boot thymeleaf and we need to return the view name from the controller instead of response body.The same view name(html file) should be present in the templates folder within src/main/resources and will be rendered through ThymeleafViewResolver. we can have something in the view like <span th:text="${name}">this is placeholder</span> and name will be replaced when application is run through server.

to get xml output from controller , we need to have @XmlRootElement annottion on our domain class and then if we pass accept: application/json, it will return json and for application/xml, it will resturn xml. or in the url, if we have http://localhost:8080/api.json it will give json response and http://localhost:8080/api.xml will xml response.

service discovery solutions available are Eureka, Ectd, Consul, Zookeeper, SmartStack.
Eureka provides a lookup server. Made highly available by running multiple copies which replicate the state of registered services. Client services register with eureka by providing host, port, heathcheck url. Client services send heartbeats to eureak. eureka removes services without heartbeat. if you do not have multiple copies of eureka server, it actually gives you a wrning that it can not connect to other eureka servers. To prevent this warning, we use client.registerWithEureka : false and client.fetchRegistry : false in eureka server project. This wil be true in prod env. In Prod env, we will have multiple Urls like client.serviceUrl.defaultZone : http://server:port/eureka, http://server:port/eureka. These multiple URLs can be fetched through config server and with the use of profiles.Eureka server does not persist service registration. It is always in memory (stateful) and that is why , we have multiple instances running in diff availability zones/ regions in production.

which comes first? Eureka or config server
Config first bootstrap (default)
eureka first bootstrap - needs 2 network trips to fetch configuration

https://github.com/kennyk65/Microservices-With-Spring-Student-Files/tree/master

Property configuration in spring boot can be done through
application.properties or application.yaml
env variables - to set profiles
command line parameters
cloud configurations like config server or consul

Bean configuration can be done through
defining bean in deafult application class
defining bean in separate configuration class
xml based configuration
component scanning - any component defined in subpackage where the main application class is located will be scanned byu default unless we mark them to be excluded through enableautoconfiguration exclude like @EnableAutoConfiguration(exclude={DataSourceAutoConfiguration.class}) or using spring.autoconfigure.exclude

spring.profiles in application ymal file - flex configuration based on the environment..active profile can be set using spring.profiles.active in env variable or command line arguments. if the active profile does not match the defined profiles in yaml file, default configuration is used.
spring:
	profiles : dev
server:
	port :8000
---
spring:
	profiles : test
server:
	port :9000
If we set spring.profiles.active to prod, then port used would be 8080.

Spring boot starter is a maven template that contains a collection of all the relevant transitive dependencies that are needed to start a particular functionality.
Like we need to import spring-boot-starter-web dependency for creating a web application. It inculudes dependenices for spring-mvc, spring-web, tomcat.
spring-boot-starter-web includeds embedded tomcat server, jackson for json marshalling which can be useful for rest apis and provide automatic marshalling and unmarshalling, logging library SLF4J and logback, snakeyaml to read yaml file and convert it to properties, starter test with junit

Spring Initializer is a web application that helps you to create an initial spring boot project structure and provides a maven or gradle file to build your code. It solves the problem of setting up a framework when you are starting a project from scratch

spring mvc has template engine starter to provide view resolvers and static file reslvers built in as well (for hmtl fiels, css etc)

thymeleaf allows you to build and style your pages outside the application. you need to include thymeleaf namescape and then use thymeleaf properties like <tr th:each="room:${rooms}">
            <td th:text="${room.number}">Number</td></tr> and then you need to have a controller accepting Model object and you need to add attribute to the model for rooms like @GetMapping
    public String getAllRooms(Model model){
        model.addAttribute("rooms", roomService.getAllRooms()); // this is for thymeleaf 
        return "rooms"; // this refers to the html page name which will be rendered
    }
	
running-setup-logic-on-startup-in-spring
https://www.baeldung.com/running-setup-logic-on-startup-in-spring
We can use Javax's @PostConstruct annotation for annotating a method that should be run once immediately after the bean's initialization.
The InitializingBean Interface - implement the InitializingBean interface and the afterPropertiesSet() method.
ApplicationListener -  create a bean that implements the ApplicationListener<ContextRefreshedEvent> interface and @Override public void onApplicationEvent(ContextRefreshedEvent event). Here we aren't focusing on any particular bean. We're instead waiting for all of them to initialize.
@Bean initMethod Attribute - We can use the initMethod property to run a method after a bean's initialization.
Spring Boot CommandLineRunner - Spring Boot provides a CommandLineRunner interface with a callback run() method. This can be invoked at application startup after the Spring application context is instantiated. multiple CommandLineRunner beans can be defined within the same application context and can be ordered using the @Ordered interface or @Order annotation.
Spring Boot ApplicationRunner - Similar to CommandLineRunner, Spring Boot also provides an ApplicationRunner interface with a run() method to be invoked at application startup. However, instead of raw String arguments passed to the callback method, we have an instance of the ApplicationArguments class. The ApplicationArguments interface has methods to get argument values that are options and plain argument values.

spring security - provides basic authentication by default for all end points other than /js and /css..default user name and password can be see in the logs...form based authentication available with in memory username and passsword by default and can use annotation @enablewebsecurity which disbaled basic authentication and extending websecurityconfigureadapter..oauth authentication with @enableoauthclient and @enableoauthserver annotations

@Endpoint annotation
We can create our own custom actuator endpoints using @Endpoint annotation on a class. Then we have to use @ReadOperation , @WriteOperation , or @DeleteOperation annotations on the methods to expose them as actuator endpoint bean.
https://www.digitalocean.com/community/tutorials/spring-boot-actuator-endpoints

Rabbit MQ in Spring boot ?

JsessionId - You do have to tell the Container that you want to create or use a session, but the Container takes care of generating the session ID, creating a new Cookie object, stuffing the session ID into the cookie, and setting the cookie as part of the response. And on subsequent requests, the Container gets the session ID from a cookie in the request, matches the session ID with an existing session, and associates that session with the current request.

why collection doesn’t extend cloneable and serializable interfaces ? Collection is an interface that specifies a group of objects known as elements. The details of how the group of elements is maintained is left up to the concrete implementations of Collection. For example, some Collection implementations like List allow duplicate elements whereas other implementations like Set don't. A lot of the Collection implementations have a public clone method. However, it does't really make sense to include it in all implementations of Collection. This is because Collection is an abstract representation. What matters is the implementation. The semantics and the implications of either cloning or serializing come into play when dealing with the actual implementation; that is, the concrete implementation should decide how it should be cloned or serialized, or even if it can be cloned or serialized
-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------
-----------------------------Rabbit MQ---------------------------------------------
Java messaing serviceRabbit MQ broker has exchange and queue which helps in sending message from producer to consumers. For example - when order is placed, you need to send mail to customer and then to send shipping details to vendor..each of these things can be done in separate consumer. Order placing service is producer and service sending mail to customer is consumer 1 and sending details to vendor is consumer 2.
Exchange will route the message to queue as message itslef does not have routing details. Need when there are multiple queues otherwise message can be directly sent from producer to queue itself. 4 types of exchange :
Direct exchange matches the key coming in the message to routing key provided at the time of queue binding exactly.
Fanout passes each message coming from the producer to each queue binded with the exchange. It is like publisher subscriber or when you want email to reach to each recipient. Here you need not provide routing key when binding queue to exchange.
Topics matches the key coming the message to the pattern of routing key provided at the time of queue binding. For ex - tv.mobile.ac matches *.mobile.* (Mobile routing key), #.ac (AC routing key) but not *.tv.* (TV routing key). Here * means single character and # means any number of characters
Header matches the header coming in the message to headers provided at the time of queue binding based on x-match any or all. For ex - Message containing header {"item1 = mobile", "item2 = television"} will match Mobile with header = {"x-match = any", "item1 = mobile", "item2 = mob"} and TV with header {"x-match = any", "item1 = tv", "item2 = television"} but not AC queue with header {"x-match = all", "item1 = mobile", "item2 = ac"} .Here you need not provide routing key when binding queue to exchange, but need to provide arguments with key and value pairs.

Default exchange is direct exchange with blank name. All queues are implicilty binded to default exchange using queue name as binding key. You can not bind queue or unbind from default exhange. You can not delete it. 

Instead of queue, we can also bing exchange to another exchange and in that case, based on routing key, the message will be routed to other exchange and queue binded to other exhange. For ex - Direct-exchange can be binded with fanout exchange with binding key as fanout and suppose AC queue is binded to fanout exchange, then message will be delivered to AC queue through direct exchnage.

Need to install rabbit MQ and Erlang(rabbit MQ is written in Erlang programming language). set ERLANG_HOME env variable and add ERLANG_HOME/bin to Path variable
rabbitmq-plugins.bat enable rabbigmt-management
then run rabbitmq-server.bat
then open localhost:15761 and page is opened . enter guest and guest as username and password and rabbitmq managent page opens with exchnages and queues creation and display options

create queue with queue name and you can see properties like total messages, ready messages, unacknowledged message, number of consumers.
To produce message directly to queue, create a appn with amqp-client dependency and then create instance of connection factory, then connection and channel and through channel publish message with exchange as blank, routing key as queue name, properties as null and message string in bytes and then that is shown in ready message in the queue in UI and we can see the exact message also in payload. After publishing the message, need to close the channel and connection. 
channel.basicPublish("","Queue-1",null,msg.getBytes()); // In here default exchange routes the message to Queue with queue name as Queue-1

To produce message to direct exchange - channel.basicPublish("Direct-exchange","mobile",null,msg.getBytes()); // where 1st param is exchange name, 2nd is routing key used to bind queue to exchange
channel.basicPublish("Direct-exchange","fanout",null,msg.getBytes()); // where 1st param is exchange name, 2nd is routing key used to bind fanout exchange to direct exchange
To produce message to fanout exchange - channel.basicPublish("Fanout-exchange","",null,msg.getBytes()); // where 1st param is exchange name, 2nd is routing key used to bind queue to exchange. As we have no routing key, it is blank and should not be null
To produce message to topics exchange - channel.basicPublish("Topics-exchange","tv.mobile.ac",null,msg.getBytes()); // where 1st param is exchange name, 2nd is routing key used to bind queue to exchange
To produce message to headers exchange - channel.basicPublish("Headers-exchange","",br,msg.getBytes()); // where 1st param is exchange name, 2nd is routing key used to bind queue to exchange. As we have no routing key, it is blank and should not be null.3rd is BasicProperties instance
Map<String, Object> headersMap = new HasMap<String, Object>();
headersMap.put("item1", "mobile");
headersMap.put("item2", "television");
BasicProperties br = new BasicProperties();
br = br.builder().headers(headersMap).build();

In the same application, can consumer message again using connection factory, connection and channel. There are DeliveryCallback and CancelCallback functional interfaces with only one method handle. We can use that to basicConsume
like 
DeliveryCallBack deliveryCallBck = (consumerTag, delivery) -> {
String msg = new String (delivery.getBody());
sysout("Msg recived "+msg);
};
channel.basicConsume("Queue-1", true. deliveryCallBck, comsumerTag -> {}); // Queue name, acknoweldegement flag, delivery call back, cancel call back

In case of multiple consumers, messages are recived in round robbin fashion like 1st message to Consumer1, 2nd to Consumer 2, 3rd to Consumer 1, 4th to Consumer 2.
Consumer deals with Queue only and not Exchange.

Sprint boot with Rabbit MQ - rabbit templates and listeners to consume to messages. Need to have dependency for spring-rabbit and spring.rabbitmq.host and spring.rabbitmq.port in application.properties file
rabbit template - to publish message to rabbit mq...we can have rest contoller to which we will pass some param and within the method, we will publish to Rabbit mq using rabbit template
need to have 
@Autowired RabbitTemplate rabbitTemplate;
rabbitTemplate.convertAndSend("Mobile", personObj); - 1st param is routing key, in here we have passed Queue name. convertAndSend uses SimpleMessageConverter which allows message to be string, bytes or serializable and hence the person class must implement serilaizable interface. // In here default exchange routes the message to Queue with queue name as Mobile. This is equivalent to passing blank as exchange name as it internally calls this.exchnage which is set to DEFAULT_EXCHANGE with blank name in rabbit mq jar
rabbitTemplate.convertAndSend("", "Mobile", personObj); // In here default exchange routes the message to Queue with queue name as Mobile.
rabbitTemplate.convertAndSend("Direct-exchange", "mobile", personObj);
rabbitTemplate.convertAndSend("Fanout-exchange", "", personObj);
rabbitTemplate.convertAndSend("Topics-exchange", "tv.mobile.ac", personObj);


Message message = MessageBuilder.withBody(byteMessage).setHeader("item1","mobile").setHeader("item2","television").build(); //byteMessage is person object converted to byte arrray 
rabbitTemplate.send("Headers-exchange", "", message);

Rabbit Listerener - to consume message form rabbit mq. starts listening as sooon as the application is started
need to have annotation with queue name on method like
@Service
public class RabbitMQConsumer {
@RabbitListener(queues="Mobile)
public void getMessage(Person p) { // message is in general a byte[].But if we are sure about data type, we can use the exact class name also and it will be automatically converted. 
// Use this for exchanges other than Headers-exchange. For headers-exchange as message is passed as byte array, we need to get message as bytpe[] and then convert it to person object
// public void getMessage(byte[] message)
sysout(p.getName());
}
}

Sprint MVC with Rabbit MQ - rabbit templates and listeners to consume to messages. Need to have dependency for spring-rabbit and then configuration file with beans for connection factory and rabbit template
@Configuration
public class RabbitMQConfig {
@Bean
public ConnectionFactory connectionFactory() {
	ConnectionFactory factory = new CachingConnectionFactory(); // or new CachingConnectionFactory("localhost", 5672);
	return factory;
}
@Bean
public RabbitTemplate rabbitTemplate() { // Need to have this to use @Autowired annotation for RabbitTemplate in controller class
	return new RabbitTemplate(connectionFactory());
}
@Bean
public SimpleRabbitListenerContainerFactory simpleRabbitListenerContainerFactory() { // Need to have this to use @Autowired annotation for RabbitListener 
	SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory();
	factory.setConnectionFactory(connectionFactory());
	return factory;
}
}

@Service
@EnableRabbit // Needed to have the consumer work, otherwise Rabbit MQ consumer will not work
public class RabbitMQConsumer {
@RabbitListener(queues="Mobile)
public void getMessage(Person p) { // message is in general a byte[].But if we are sure about data type, we can use the exact class name also and it will be automatically converted. 
// Use this for exchanges other than Headers-exchange. For headers-exchange as message is passed as byte array, we need to get message as bytpe[] and then convert it to person object
// public void getMessage(byte[] message)
sysout(p.getName());
}
}

Heroku is a PaaS cloud provider, owned by Salesforce , applications built on java, node.js, python, php, scala can be deployed to it. applications can be deployed on free using free trial account.
 Earlier Pivotal Cloud founbdry was used which was again PaaS and it had rabbit mq service and we can push our jar (spring boot) or war(spring mvc with buildback for server support) to pivotal cloud foundry url
 Creat account and verify it on Heroku. A dashboard page will be shown on login.
Install Heroku CLI and verify by comming cmd and heroku --version
Go to heroku account dashboard, create an application with unique name like infobuzz and under resources add Cloud AMQP to add rabbit mq (free version). Then we will get rabbit mq hostname, username and password and the management page for RabbitMQ to opened using the credentials
In spring boot application.properties, we need to update the spring.rabbitmq.addresses=amqp://url and server.port={PORT:8080}

heroku login
heroku plugins install java - to deploy java applications with heroku
heroku deploy jar myjar-snapshot-1.0.jar app infobuzz
-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------
-----------------------------Unit testing-----------------------------------------
Mockito framework for mocking
@Mock - It is used to create and inject mocked instances.
@Spy - It is used to create a real object and spy on the real object. The @Spy annotation is used to create a real object and spy on that real object. A spy helps to call all the normal methods of the object while still tracking every interaction, just as we would with a mock
@Captor - It is used to create an ArgumentCaptor.
@InjectMocks - It is used to create an object of a class and insert its dependencies.
@RunWith - It is utilized to keep the test clean and improves debugging. It additionally detects the unutilized stubs available in the test and initializes mocks annotated with @Mock annotation.
Some of the limitations of Mockito are as follows:
It cannot mock constructors or static methods or private methods - for that Powermock is used.
It requires Java version 6 plus to run.
It also cannot mock equals (), hashCode() methods.
Mockito.any(Class) can be used in case we need to verify that a method is being called with any argument and not a specific argument.

Read more: https://javarevisited.blogspot.com/2022/07/mockito-interview-questions-with-answers.html#ixzz8Fd83M9bU


Spring Mockmvc framework for controller testing
@RunWith(SpringRunner.class)
@WebMvcTest(HelloWorlController.class)

@Autowired MockMvc mockMvc;
RequestBuilder request = MockMvcRequestBuilders.get("/hello-world").accept(MediaType.APPLICATION_JSON)
MvcResult result = mockMvc.perform(request).andExpect(status().isOk()).andExpect(content().string("hello-world")).andReturn(); -> using resultmatchers for verifying response with andExpect method..content.json() method for json comparsion with space difference and only some fields considered
assertEquals("hello-world", result.getResponse().getContentAsString());

JSONAssert framework for unit testing of json content. Mockito, JSONAssert are all part of spring boot starter test dependency
JSONAssert.assertEquals(exp, actual, strict) -> strict - true /false for fields to be matched strictly. If expected has 2 fields and actual has 3, and strict is true, this will return false

To mock service in controller, we can use MockBean annotation and use mockito like when(mockbeansvc.method()).thenReturn(someValue);
To mock repository in service, we can use Mock annotation and injectmock in the svc and use mockito like when(mockrepo.method()).thenReturn(someValue);  and then call svc method 

Difference between @Mock and @MockBean annotation in Spring Boot
Here are the key differences between @Mock and @MockBean annotations in Spring Boot and Spring Framework are:
1. Context
@Mock is used for mocking objects that are not part of the Spring context, while @MockBean is used for mocking objects that are part of the Spring context.  It is used in plain JUnit tests with the Mockito framework. It is also not aware of the Spring context and is typically used for unit testing isolated components without the need for a full Spring context setup.
2. Integration with Spring Boot
@MockBean is a Spring Boot-specific annotation that provides integration with the Spring Boot Test module, allowing for seamless mocking of Spring beans in Spring Boot applications
3. Initialization
@Mock requires the use of MockitoJUnitRunner or MockitoExtension for initializing the mock objects, while @MockBean is automatically initialized by the Spring Boot Test framework during the test context setup.
4. Bean replacement
@MockBean replaces the actual beans in the Spring context with mock objects during testing, while @Mock does not impact the actual beans in the Spring context.
Read more: https://www.java67.com/2023/04/difference-between-mockitomock-mock-and.html#ixzz8FdjxKoZn


For repository unit testing, @DataJpaTest and @RunWith(SpringRuner.class) can be used ...DataJpaTest creates in memory database, runs data.sql (present in src/test/resources) and creates repository and runs query using it

@SpringBootTest is used for integration test..@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) makes sure that in CI environment, multiple commits do not tend to start the web app on the same port and it chooses any available port. this annoattion launches the entire application and beans are checked and context is loaded.
We can autowrire TestRestTemplate provided by spring boot to make rest calls for getFrObject and then use JsonAssert to verify actual and expected results from the inmemory database H2.
To mock out any external dependencies or database repositories, we can use MockBean annotation.

@TestPropertySource(locations={"classpath:test-conf.properties"})

Frameworks to write asserts/assertions - JSONAsset, Json-path and hamcrest matchers

Hamcrest
		assertThat(numbers, hasSize(3));
		assertThat(numbers, hasItems(12,45));
		assertThat(numbers, everyItem(greaterThan(10)));
		assertThat(numbers, everyItem(lessThan(100)));
		
		assertThat("", isEmptyString());
		assertThat("ABCDE", containsString("BCD"));
		assertThat("ABCDE", startsWith("ABC"));
		assertThat("ABCDE", endsWith("CDE"));
		
AssertJ - allows for chaining and writing predicates
assertThat(numbers).hasSize(3)
						.contains(12,15)
						.allMatch(x -> x > 10)
						.allMatch(x -> x < 100)
						.noneMatch(x -> x < 0);
		
		assertThat("").isEmpty();
		assertThat("ABCDE").contains("BCD")
						.startsWith("ABC")
						.endsWith("CDE");
						
JSONAssert for assertions on json
Json path for better asserts on json like getting first element, getting all ids, getting elements with matching value of name or quanity, getting elements by range of index etc.
String responseFromService = "[" + 
				"{\"id\":10000, \"name\":\"Pencil\", \"quantity\":5}," + 
				"{\"id\":10001, \"name\":\"Pen\", \"quantity\":15}," + 
				"{\"id\":10002, \"name\":\"Eraser\", \"quantity\":10}" + 
				"]";
		
		DocumentContext context = JsonPath.parse(responseFromService);
		
		int length = context.read("$.length()");
		assertThat(length).isEqualTo(3);
		
		List<Integer> ids = context.read("$..id");

		assertThat(ids).containsExactly(10000,10001,10002);
		
		System.out.println(context.read("$.[1]").toString());
		System.out.println(context.read("$.[0:2]").toString());
		System.out.println(context.read("$.[?(@.name=='Eraser')]").toString());
		System.out.println(context.read("$.[?(@.quantity==5)]").toString());
		
To improve the performance of tests, make sure to write tests which avoid launching of spring application context as that takes time.
test names should be menaingful
have utility assert
unit tests should be isolated..like not dependent on database
-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------
-----------------------------Design Pattern-----------------------------------------
https://www.baeldung.com/java-service-locator-pattern
https://www.digitalocean.com/community/tutorials/chain-of-responsibility-design-pattern-in-java
https://www.digitalocean.com/community/tutorials/proxy-design-pattern
https://www.digitalocean.com/community/tutorials/decorator-design-pattern-in-java-example
https://www.digitalocean.com/community/tutorials/adapter-design-pattern-java
https://www.digitalocean.com/community/tutorials/bridge-design-pattern-java
https://www.digitalocean.com/community/tutorials/composite-design-pattern-in-java

https://www.fullstack.cafe/blog/microservices-interview-questions
https://www.interviewbit.com/java-interview-questions/#how-we-can-set-the-spring-bean-scope-and-what-supported-scopes-does-it-have
https://www.interviewbit.com/design-patterns-interview-questions/#bridge-design-pattern
https://www.digitalocean.com/community/tutorials/facade-design-pattern-in-java
https://www.digitalocean.com/community/tutorials/command-design-pattern

https://samirbehara.com/2018/09/10/monolith-to-microservices-using-strangler-pattern/

Builder design pattern - useful for contsructing objects when the constructors have many parameteers or may be many same type of parameteres like String. In this case,instead of calling constructor to create the object, we create the object by adding various fields to it and then building it. In general, you cna have something like builder.addName() and then builder.addAddress(0 etc. But with fluent builders, in the builiding method, you return the instance of builder itself, therby reducing the code to builder.addName().addAdress().
In case of inheritance with fluent builders, theer can be a problem if we return this object in the building method in parent class and child class. To resolve this, we use recursive generics like class PersonBuilder<SELF extends PersonBuilder<SELF>> and then return a method returning self like public self() { return (SELF) this}. The child builder would be like class EmployeeBuilder extends PersonBuilder<EmployeeBuilder> with   protected EmployeeBuilder self(){    return this;}
Faceted builder help in having multiple builders extending from a parent builder class which can be used together to build an object.

Prototype design pattern is used when the Object creation is a costly affair and requires a lot of time and resources and you have a similar object already existing. Prototype pattern provides a mechanism to copy the original object to a new object and then modify it according to our needs. Object copying can be done by extending Cloneable marker interface and overriding the clone method and making it as public. In general, clone creates a shallow copy , so we need to provide implementation for deep copy which can be confusing. Other way of copying is usng Serializable marker interface and then doing SerializationUtils.roundtrip(obj). Third approach is to be deep copy through copy constructors.

https://www.initgrep.com/posts/design-patterns/thread-safety-in-java-singleton-pattern#:~:text=Inner%20Static%20Class%3A%20In%20the,inner%20class%20is%20first%20referenced.

https://www.baeldung.com/string/intern

https://www.digitalocean.com/community/tutorials/flyweight-design-pattern-java - to save memrory , is structural pattern. Flyweight design pattern is used when we need to create a lot of Objects of a class. Since every object consumes memory space that can be crucial for low memory devices, such as mobile devices or embedded systems, flyweight design pattern can be applied to reduce the load on memory by sharing objects.To apply flyweight pattern, we need to divide Object property into intrinsic and extrinsic properties. Intrinsic properties make the Object unique whereas extrinsic properties are set by client code and used to perform different operations

Proxy pattern types - protection proxy - Protection proxy controls access to the original object. Protection proxies are useful when
objects should have different access rights.
https://www.javacodegeeks.com/2015/09/proxy-design-pattern.html

Strategy design pattern - behavioral design pattern. Strategy pattern is used when we have multiple algorithm for a specific task and client decides the actual implementation to be used at runtime. like collections.sort with comparator
https://www.digitalocean.com/community/tutorials/strategy-design-pattern-in-java-example-tutorial

State design pattern is one of the behavioral design pattern. State design pattern is used when an Object change its behavior based on its internal state.
https://www.digitalocean.com/community/tutorials/state-design-pattern-java

Visitor pattern is used when we have to perform an operation on a group of similar kind of Objects. With the help of visitor pattern, we can move the operational logic from the objects to another class. For example, think of a Shopping cart where we can add different type of items (Elements). When we click on checkout button, it calculates the total amount to be paid. Now we can have the calculation logic in item classes or we can move out this logic to another class using visitor pattern
https://www.digitalocean.com/community/tutorials/visitor-design-pattern-java
-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------
-----------------------------Mongo DB----------------------------------------------
MongoDB is a leading NoSQL, classified as a document-oriented database. written in C++ language. pairs each key with a complex data structure named as document. stores document in a binary-encoded format termed as BSON. BSON is an extended format of JSON data model.
The commonly used Data Structures are Document, Graph, Key-value, and Wide column
Document is field value pairs stored in JSON like BSON

-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------
-----------------------------Generative AI-----------------------------------------
generates new content as its primary output
Use cases - Image generation, Video streaming, 
other types of AI - generate content but as a side effect ..Other types include:
Discrimntaive AI - classifying based on existing content
Reactive Machines - used in cars
Limited Memory AI
Narrow Mind AI
Supervised learning
Unsupervised learning - can detect fraud bank transaction
Reincforcement learning - can teach machine how to play a game

most famous tools for generative AI -
Generative Pre trained transformer (GPT) - a natural language model developed by Open AI
large scale, human like text generation, use of transformer architecture
Github CoPilot is a GPT using AI Codex which helps to write code without searching for external solutions
Bing incorporated chatGPT in its search functionality

disdavnatges -
lack of common sense
lack of creativity
no understanding of generated text
biased databases
danger of normalization of mediocrity with creative writing

Text to Image generation services
Midjourney - Mac OS, Closed API, Art centric approach
DALL-E - Windows OS, Open AI, developed by corporation, most superior machine learning algorithm, values technical superiority over design
Stable fusion - Linux OS, Open community, under continuous improvement

Generative Adverserial networks (GANs) - generator and discrimminator like in forgery

-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------
-------------------------------AWS-------------------------------------------------
Region is a cluster of data centers.
Each region can have multiple availability zones (min 3 and max 6). Each AZ contains multiple data centers with reduntant power, networking and connectivity and are connected with high bandwidth and ultra low latency networking to avoid disaster failure.

Choice of region dependens to Compliance to regulatory and legal requirements in country, Availability of service in region, Pricing for the region, Latency 
Global services are IAM, Route 53 DNS service, Cloud front content delivery network, WAF(web appn firewall)
Region specific services are Amazon EC2(Iaas), Elastic Beanstalk(Paas), Lambda(Faas), Rekognition (Saas), AWS CloudShell

IAM - Identity and access management. Global service.
Root user is created on signing in. It should not be shared.
Users can be created by root user. 
Create one IAM user for one physical user
Users can belong to multiple groups or do not belong to a group.
Groups can contain multiple users , but not other groups. 
Users or Groups are created to assign permissions to users via json documents called policies. You should apply least privildege principle.
Policies consist of Version, Id, Statement -> Sid (optional), Effect (Allow/DEny), Principal (User/Account/Role), Action (ListUser, GetUser, PutUser etc), Resource (like S3 bucket), Condition when this policy is applicable (optional)
MFA -> something you know and something you own
MFA devices- virtual MFA device like Google authenticator (works only on phone) and Authy (works on multi device) and both support multiple token on single device. Second type is universal 2nd factor (u2f) security key - yubikey by yubico 3rd party and supports multiple root and iam users using a single key. Hardware key fob mfa device like gemalto and surepassid used in US

To access AWS, 3 options:
AWS management console (protected by password and MFA)
AWS command line interface - protected by access keys..can create scripts to manage resources here..it is open source and available on github
AWS SDK - for use in code, protected by access keys..langauge specific and embedded in appn
AWS CLI is built on aws sdk for python named boton. 
password policy can be set to have n number of letters, char, specical char and to expire passowrd after 90 days and not reuse last 3 passords and change one time password etc
access keys can be created in security permissions for the user by selecting the env for which access key is needed like is it needed for CLI or for SDK etc?
aws --version
aws configure -> To configure CLI with access key and secret access key
aws iam list-users - To list all users, if current access key user is allowed for this action

AWS cloudhsell- region specific, provides full file system with upload and download file options

IAM roles - to assign permissions (oplicies) to services to do work on your behalf. for ex- EC2 instance needs some permission to access AWS. For this we create a new role for AWS service and EC2 and asign permission or policy like IAMReadOnlyAccess. Common IAM roles - EC2 instance roles, Lambda function roles, Roles for cloudformation

IAM Security tools -
IAM credential report (account level)-provides list of all users in account and status of their credentials
IAM access advisor (user level)- provides service permissions granted to user and when they were last accessed. useful to revise policy to least access priviledge.data is tracked for max of 365 days and latest activity changes reflected in 4 hours

To access My Billing dashboard from IAM users who are admin, need to provide IAM user and role access to billing information through root account. After that you can access BILLS, services resulting in the bills, FREE TIER to check how much quota of free tier has been consumeed, BUDGETS to set and budget and get notification like for Zero budget, Monthyl budegt etc and get notificatiuon with 85% of budget is used or forecast is 100% budget use or budget is exceeded

Amazon EC2 - Elastic Compute Cloud - IaaS - consists of
Virtual machines (EC2)
Storing data on virtual drives (EBS)
Load balancing acroos virtual machines Elastic load balancer (ELB)
Scaling services using Auto scaling groups ASG

EC2 sizing and configuration option - can choose
OS - Linux, Mac, Windows - Amazon machine image (AMI) for application and OS image needs to be choosen like AWS Linux, Redhat, Windows etc.
CPU
RAM
Storage space - Netwrok attached (EBS or EFS) or hardware (EC2 instance store)
Network card - speed, public IP address
Firewalls - security groups
Bootstrap script - configured on first launch - using EC2 User data script - can automate tasks like installing software, installing updates, downloading common files from internet etc. Run using root user

EC2 instance type - t2.micro (1vCPU, 1 GB Mem, EBS only storage, low to moderate network performcae), t2.xlarge (3 vCPU, 16 GB mem, EBS only, moderate network perf), c5d.4xlarge, r5.16xlarge, m5.8xlarge
For launching EC2 instance, we choose AMI, Instance type, and define key pair, storage, security group, whether to assign public ip or not and optionally provide user data script.
Key pair used to connect to EC2 instance securely..we can define pair name and pair type (RSA or others) and private key format of .pem (for windows 10, mac, linux ) and .ppk (for windows 8 etc).
Public IPv4 address is assigned (which changes on each restart) and private IPv4 is assigned which is fixed once the instance is running
Security group allow SSH traffic from anywhere or to allow HTTP traffic to instance if web server is configured. Inbound and outbound rules are created. 
by default the volume assigned to EC2 instance is deleted when the instance is deleted.
Instance state can be stopped, terminated, reobbot etc.

7 types of EC2 Instance types -
General Purpose - great for diversity of work loads like web servers and code repositories. Ex- t2.micro is general purpose free tier ec2 instance, balance of storage, compute and memory
Compute Optimized - for compute intensive tasks needing high performance processors like batch workloads, dedicated gaming servers, high performance web servers, high performance computing hpc, machine learning and scientific modelling, media transcoding. Ex-  c4, c5, c6
Memory Optimized - process large data sets in memory like high performance RDBMS/non RDBMS, distributed web scale cache store, in memory DB optimized for BI, real time processing of unstructured data. Ex- R series, X series, High memory, z1d
Storage Optimized - high read write access to large data like OLTP , Relational and no sql DB, Data warehousing, Distributed file systems, Cache for in memory database like Redis. Ex- I series, D series, H series 
Accelerated Computing
Instance features
Measuring Instance Performance

m5.2xlarge
m- instance class
5- generation
2x- size within the instance class

Security group are firewalls on EC2 instances. Can refernce by IP or other security group. Have allow rules. Regulate access to ports, authroized address ranges for IPv4 or IPv6 address, inbound traffic (from other to instance), outbound traffic (other instance to other). They have type (HTTP/ SSH, Custom TCP), Port (80, 22), Protocol (TCP), Source (0.0.0.0), Description.
Bound to region/ VPC
can be attached to multiple instances within the same region/ vpc
by default all inabound traffic is restricted and outbound is allowed
If app timeouts, it is security group issue. If connection refused error, then application error
maintain one security group for SSH access
lives outside EC2 instance. if traffic is blcoked, EC2 does not know about it

To connect to EC2 instance, 3 options :
SSH - works on Mac, Linux, Windows 10 or above..uses .pem file
Putty - works on Windows below 10 and Windows 10 or above..uses .ppk file
EC2 Instance connect - works on all ..for Linux works with Amazon Linux 2 only...EC2 Instance connect is browser based SSH into the machine..does not need .pem or.ppk key..internally generates a temp key file and connects using that
For all these, we need to have security group with inbound rule allowing SSH traffic from anywhere or custom.

command to connect from terminal - we need to place .pem or .ppk key pair file in the same directory from where command is being run and it should have permissions from chmod 400 (make current file read only to prevent accidental overwriting)
ssh -i EC2Tutorial.pem ec2-user@3.250.24.200
here .pem file should not have any space in file name, ec2-user is available by default in Amazon Linux 2, 3.250.24.200 is public IP on EC2 instance which changes on each reboot
after connecting we can issue commands like whoami, ping google.com..to exit type logout or exit

in the EC2 instance, never use aws configure to configure your access key and sceret because anyone using EC2 Instance connect nnect can then retrieve your credentails. Instead attach IAM role to the EC2 instance to run commands like aws iam list-users

EC2 purchasing options -
On demand instance - for short workload, predictable pricing, pay by second...pay by second after first min in mac and linux and pay per hr in windows..high cost but no upfront payment
reserved (1 & 3 years) -  
reserved instance for large workload - can reserve OS, tenacy, instance type, region. discount of 70% comapred to on demand,,have option of no upfront, partial or all upfront payment..can buy or sell on marketplace, scope can be regional or zonal (reserved capcitiy in AZ), suitable for databases
convertible reserved instance for large workload with flexible instances - can change region, OS, scope, tenancy, insatnce type...60% discount
savings plan (1 & 3 years) - commitment to usage, large workload..locked to specific region and instance family like m5 in us-east..but can change instance size like m5large , OS, tenancy (host/dedicated/default)
spot instances - short workload, cheap 90% discount, can be easily lost if your max price you can spend is more than current price..not suitable for critical jobs or databases, suitable for batch jobs, image processing, distributed workload, any job with flexible start and end time, data analysis...if current price is nore than user defined max price, then you have a 2 min grace window within which you can stop or terminate your instance. Spot block reserves some time for your spot instabe (not available after Dec 2022 and can reserve time from 1 to 6 hours within which spot instance will not be reclaimed).. Spot request can specify max price, number of instances, launch specification, valid from and valid to, request type- one time or persistent. One time request spot instance if terminated is gone. But if request is persistent and spot instance goes down, then if the request is valid, it will send another request to launch instance. Spot instance request can be Open, Active, Disabled, Cancelled, Failed, Closed. You can cancel spot instance requests which are open, active or disabled. Cancelling does not terminate insatnce. You should first cancel spot request and then terminate spot instances. Spot fleet = set of spot instances + optional on demand instances. It will try to meet target capacity with price constraints. You can specify multiple launch pool based on instance type, OS, AZ. It will stop launching instance when capcity or price is reached. Stretegies to allocate spot instances can be lowest prices, deiversed (distributed globally), capacity optimized, price capacity optimized
dedicated host - book entire physical server and control instance replacement..most expensive..used in case of regulatory or compliance needs or software with complex licensing model like bring your own license
dedicated instance - hardware dedicated to you but can be shared with instances in same account..no control on instance replacement
capacity reservation - in specific AZ for particular duration..charged whether you run your instance or not


Elastic IP - fixed public IP for EC2 instance. can have max 5 elastic IPs in AWS account. avoid using it, instead use DNS like Route 53 or Load balancer. For SSH into EC2 instance, public IP is used. Once you get Elastic IP from pool of available IPv4 address, you are charged for it. You can associate it to an instance or disassociate it. To not incur any charges, release it.

EC2 Instance placement strategy - using placement groups ..following stratgey for the placement group
Cluster - instances in same hardware, same rack, same AZ..low latency, great failure, increased risk of failure..useful of BigData jobs that need to complete fast
Spread - instances spread across hardware across AZ, max 7 instances per group per AZ..reduced risk of failure, high availability..useful for critical appns where failure should not impact
Partition - instances spread across partitions (which rely on racks) within AZ. Can scale upto 100 instances  per group like Hadoop, Kafka, Cassandra. 7 partitions per AZ. Each partition represnets one rack. Partition failure can affect multiple EC2 instances but not other partitions. EC2 instances access partition information as metadata

ENI - Elastic network interface..logical component in a VPC representing virtual network card..consists of one primary private IPv4 address, secondary IPv4 address, one elastic IP per private IPv4, one public IP , one or more security groups, one mac address..bound to specific AZ..can create separately and attach/move to EC2 instances on the fly..you are not charged for it unless you attach it to an instance..eni created separately does not die when EC2 instance is terminated..by default enis are created with each ec2 instance by AWS and they die once instance is terminated..by separately creating eni, you can associate secondary IP to instance
https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/

EC2 Hibernate - OS boots and user data script is run..in hibernate OS is not stopped..RAM state is written to root EBS volume on stop and loaded from there into EC2 instance memory on start..root EBS volume must be encrypted and have enough space..use cases are long running processes, services that take time to start up..instance can not hibernate for more than 60 days
need to enable hibernation as stop behavior when launching EC2 instance 
ec2 instance RAM must be less than 150 GB, ec2 instance root volume type must be EBS volume, can not be instance store..supports on demand and reserved instance
 
EBS Volume - Elastic block store volume - 
kind of network drive attached to instance (not physical drive , hence latency to communicate with instance)..
persist data after instance is terminated..can be detached from EC2 instance and attached to other..can be mounted to one instance at a time at CCP level..EBS volume can be unattached..one EC2 instance can be attached to multiple EBS volumes..
bound to AZ..can attached EBS volume in an instance in the same AZ as that of EBS volume..to move EBS volume across AZ, need to snapshot first..
need to provision capacity (size in GB and IPOS) and charged based on that..capacity can be increased over time..free tier offers 30 GB of EBS storage of type General purpose (SSD) or magnetic per month..
delete on termination feature controls if EBS volume is terminated when EC2 instance terminates or not..by default root EBS volume is deleted on termination and any other EBS volume is not deleted on termination..but this can be changed from AWS console or AWS CLI

EBS snapshot - backup of EBS volumn at a point of time..to use in disaster recovery
not necessary to detach from instance when creating snapsht but recommended..
can move across AZ or Region..can create volumne from EBS snapshot in a diff region and also can encrypt that
features of EBS snapshot :
EBS snapshot archive - move snapshot to arhive tier..75% cheaper..takes 27 to 72 hrs to restore the archive 
Recycle bin - prevent accidental deletion..retention can be specified from 1 day to 1 year..recycle bin can be specified for EBS snapshots and AMIs
Fast Snapshot Restore (FSR) - force full initialization of snapshot to have no latency on first use..costly

AMI - Amazon machine image
customization of EC2 instance with own software, configuration, OS, environment, helps in faster boot..
built for specific region and can be moved across regions.You can't launch an EC2 instance using an AMI in another AWS Region, but you can copy the AMI to the target AWS Region and then use it to create your EC2 instances.
can launch EC2 instances from : public AMI (provided by AWS), own AMI (make own and maintain), EMI from AWS marketplace
start EC2 instance , customize it (like install httpd apache server in user data script)..stop it for data integrity..build AMI from it, this will also create EBS snapshots..launch instances from custom AMI (and create web pages which will run on httpd server)

EC2 Instance store - hardware disk attached to EC2 instances which are actually hardware..better I/O performance
can be backed up and replicated
lose storage when instance is terminated
data loss if hardware fails
useful for short term storage like buffer, cache, scratch data, temp files

EBS Volume types - 6 types
gp2/gp3 (ssd) - general purpose ssd volumne..balance of price and performance..can be used as boot volume, virtual desktops, dev and test envs
size of 1 GB to 1 TB
baseline of 3000 IOPS, max is 16000 IOPS.
for gp2, size and IOPS are linked..so for 5334 GB, we have max IOPS, 3 IOPS per GB
for gp3, size and IOPS can be increased independently

io1/io2 (ssd) - highest performnce..useful of mission critical low latency and high throughput appns..can be used as boot volume
provisioned iops
appns with more than 16000 IOPS
database workloads with high perf and storage
size of 4 GB to 16 TB..max PIOPS is 64000 for Nitro EC2 Instance and 32000 for others
io2 s more durable and have same price is io1
size and IOPS increased independently
io2 block express - 4 GB to 64 TB, max 256000 IOPS
supports multi EBS attach

st1 (throughput optimized hdd hard disk drive) - low cost hdd - frequently accessed throughput intensive workloads(big data, warehouses, log processing)..can not be boot volume..does not support ebs multi attch..125 GB to 16 TB, max IOPS 500
sc1 (cold hdd) - lowest cost..less frequently accessed workloads....can not be boot volume..does not support ebs multi attch..125 GB to 16 TB..max IOPS 250

vol types characterized by size, throughput, IOPS

EBS multi attach available only for EBS volumnes of io1/io2 family..can be attached to max 16 instances at a time in the same AZ..each instance has full read and write permissions to high performance volume..must use a file system that is cluster aware (not XFS, EXT4 etc.)..useful for appns managing concurrent write operations, high appn availability in clustered linux appns like teradata

when you create encrypted ebs volumne, data is encrypted, data moving bw instance and vol is encrypted, snapshots are encrypted, all volumes created from snapshot are encrypted
snapshots created from unencrypted vol are unencrypted
encryption has minimal latency impact
uses keys from KMS (AES-256)
copying unencrypted snapshot allows encryption, creating volumne from unencrypted snapshot allws tro encrypt the vol on the fly

EFS Elastic File System - managed network file system ..can be mounted on multiple EC2 in multiple AZ..highly availabel, scalable, expensive, pay per use and no capacity planning
used for word press, content management, web serving, data sharing
uses nfsv4.1 protocol
compatible only with Amazon Linux AMI, not windows
can be encrypted using KMS
uses security groups to control access to EFS
uses POSIX file system

EFS performance classes -
EFS scale - 1000s of concurrent EFS clinets, 10GB+ throughput, grow to PB scale automatically
Peformance mode set at EFS creation - general purpose, max IO
Throughput mode - bursting, provisioned, elastic

EFS storage classes -
storage tiers - standard/regional..standard infrequent access - moved in here based on lifecycle policy like move to IA if not accessed for 30 days and move out of IA on first access, low cost to store, cost to retrieve files
availability and durability - standard - multi AZ, used for prod...one zone- one AZ, used in dev


Scalability - 2 types of scalability
Vertical - increase size of instance like t2.micro to t2.large..used in non distributed systems like database, RDS, ElasticCache..limit on how much it can scale
Horizontal - increase number of instances..used in distributed systems like web appns..not all distributed systems can be horizontally scaled..achieved through Auto scaling group and load balancer

High availability - goes hand in hand with horizontal scaling..achieved through Auto scaling group multi AZ and load balancer multi AZ
running app/ system in at least 2 data center/ AZ..run instances of same appn in multi AZ
high availability in case of data center loss
Passive - like RDS Multi AZ
Active - like horizontal scaling


Load balancer -
spread load across multiple downstream instances
expose single point of access(DNS) to your application
handle failure
provide health check)done on a port and route like /health with http protocol and port 80..if response is not OK, means instance is unhealthy)
provide SSL termination HTTPS for website
enforce cookie stickiness
ensure high availability
separate public from private traffic

Elastic load balancer - managed LB by AWS..AWS takes care of upgrades, maintainance, high availability..need to do some configuration
less effort to setup than own LB..costly than own LB
integrates with other AWS services like EC2, EC2 ASG, ECS, AWS certificate manager(ACM), Cloud watch, Route 53, WAF, Global accelerator

Types of load balancers -
Classic Load balancer (CLB, old gen v1 2009)- supports HTTP, HTTPS, TCP, SSL (secure TCP)

Application load balancer (ALB, new gen v2 2016 )- supports HTTP, HTTPS, web socket..operates at layer 7(http)..load balancing to multiple HTTP appns across machines (target groups)..load balancing to multiple appns on same machine( containers)..supports routing from http to https..routing to diff target groups based URL path, URL host, query string..useful for microservices and container based appns..has port mapping feature to redirect to dynamic port in ECS
ALB target groups can be EC2 instances (managed by ASG)-HTTP, IP address (private (can be ip address of on premise machine)), Lambda, ECS tasks(managed by ECS)-HTTP
ALB can route to multiple target groups
health checkes performed at target groups
ALB come with fixed host name
app server(EC2 instance) does not see client IP. Client IP is present in header as X-forwarded-for, port in x-forwarded-port and protocol in x-forwarded-proto
included in free tier
provides static DNS name to be used in appn

Network load balancer (NLB, new gen 2017) - Layer 4, TCP, TLS (secure TCP), UDP - for ultra high performance and handling millions of requests per second...one static IP per AZ, supports assigning elastic IP...not included in free tier
NLB target groups can be EC2 instances, private IP addresses (can be ip address of on premise machine), ALB
health checkes support TCP, HTTP and HTTPS protocols
provides static DNS name and static IP to be used in appn

Gateway load balancer - operates at network layer, layer 3 - IP packets- for security, intrusion detection and prevention, firewall, deep packet inspection systems, payload manipulation and analysis of traffic
based on route table, traffic from users go to GLB and then to fleet of 3rd party virtual security appliances(Taregt group of EC2 instances) and if it is fine, then via GLB, the traffic is sent to application for analysis
combines the functions for transparent netwrok gateway (single entry/exit point for all traffic), load balancer for virtual appliances
uses GENEVE protocol on port 6081
target groups can be EC2 instances, private IP addresses (can be ip address of on premise machine)

Load balancers can be setup as Private/ Internal and Public/ External.

Load balancers will allow straffic on HTTP and HTTPS from anywhere but EC2 instance will allow traffic only from load balancer. So we will link security group of LB to EC2 instance security group..EC2 will still have a separate security group to allow SSH traffic for example. So if we try hitting the public IP of EC2 instance, it will not be possible, but the instance can be reached via LB.

Sticky sessions/ session affinity works with ClassicLB, ALB, NLB..same client is always redirected to the same instance behind LB
uses cookies(except NLB) which has an expiration date which can be setup
may bring imbalance to load over backend EC2 instances
2 types of cookies -
Application based cookies
- custom cookie - generated by application, custom attr set by appn, cookie name must be specified individually for each target group..can not have names like AWSALB, AWSELB, AWSALBAPP, AWSALBTG
- application cookie - generated by LB, cookie name is AWSALBAPP
Duration based cookies- generated by LB..name AWSALB for cookie generated by ALB and AWSELB for cookie generated by CLB, fixed expiry

cross zone load balancing - LB distributes traffic across across EC2 instances distrbuted across multiple AZ
ALB - Cross zone load balancing enabled by default..can disbale it at target group level..no extra charges for inter AZ data transfer
NLB and GLB - disbaled by default, extra charges for inter AZ data
CLB - disabled by default, no extra charges for inter AZ data

SSL certificate allws to encrypt data sent between client and LB (in flight encryption)
SSL - secure socket layer, TLS - Transport layer security ..newer version
public SSL certificates issues by Certficate Authority (CA) like Comodo, Symantec, GoDaddy, GlobalSign, DigitCert
SSL cert has expiration date which can be set ..and can be renewed

Users connect to LB through HTTPS over www..LB talks to EC2 instance through HTTP over private VPC.
LB uses X.509 certificat (SSL. TLS server certificate)..you can manage ceritifcates using AWS certificate manager(ACM)..can upload own certificates
For HTTPS listener, you must specify default cert, optional list of cert for diff domain..client uses SErver name indication (SNI) to specify the hostname they want to reach to..also you can specify security policy to support older version of TLS/SSL legacy clients
SNI allows having multiple SSL certificates in web server..client indicates host name in SSL initial handshake..server will identify certficate based on that..supported by ALB and NLB and cloudfront..not supported by CLB
CLB - supports one SSL certificate..should use multiple CLB to have multiple SSL certificates
ALB and NLB - support listener with multiple SSL certificates..uses SNI

Connection draining - time period when the EC2 instance is de-registrering itself and becoming unhealthy...time to complete in flight request..called connection draining in CLB, de-registration delay in ALB and NLB..can be between 1 to 3600 seconds..default is 300..can be set to low for short requests..ELB is smart enough to not send new requests to de-registering EC2 instance.

ASG-
free, only pay for EC2 instance
scale in or scale out instances
specify min and max no. of instances- min capacity, desired capacity and max capacity
automatically register new instances to LB..for example health check by ELB finds a instance unhealthy or EC2 health check finds insatnce unhealthy, then new instance is created and registered
re-create EC2 instance in case a instance gets unhealthy

ASG attributes
scaling policy - along with cloud watch alarm (ex average CPU usage across EC2 instances is high then scale out instances or scale in )..alarm monitors a metic like average CPU usage or custom metric
min capacity, desired capacity and max capacity
launch template - AMI + Instance type, EC2 user data,SSH key pair, IAM roles for EC2 instances, EBS volumne, security groups, network + subnet information, LB information

Scaling policy type
Dynamic scaling - scheduled actions (based on trends of usage like scale out every friday from 5 to 10 pm), target tracking scaling (based on target like i want average cpu usage to be 40%), simple/step scaling (like when cloud watch alarm is trgiggered for avg cpu usage is >70%, add 2 units, if less than 40%, remove 1 unit)
Predictive scaling - analyze historic data, forecast load and scale based on that..AI powered

Metrics to scale on - CPU utilization, Request count per target,network in/out, any custom metric that you push using cloud watch
after scaling, there is cooldown period of default 5 mins. during cooldown period, ASG will not allow to launch or terminate EC2 instance to allow metrics to stablize..should use ready to launch AMI to reduce time taken to launch EC2 instance and reduce cooldown period

RDS - RElational database service...managed DB servce using Sql
allows you to create databases in cloud managed by AWS including PostGres, MySQL, Microsoft SQL server, Oracle, MariaDB, Aurora (AWS proprietary) 
Adv over deploying DB on EC2 - managed service with
automatic provisioning, OS updates and patching 
continuous backups and store (from 0 no back to 35 days of retention period)..can choose window to take backup
monitoring dashboard
read replicas for improved read
Multi AZ for disaster recovery
Maintenance window for upgrade
backed by EBS volumne (gp2, io1)
scaling both horizontal and vertical
Disadv - can not SSH into RDS instances

RDS provide storage auto scaling
detects when running out of storage space and scales automatically
need to set maximum storage threshold 
automatcially modifies storage if free storage is less than 10% of allocated storage, low storage lasts for at least 5 mins, 6 hours have passed since last modification 
useful of unpredictabale loads
supported in all DB engines - Postgres, Oracle, Mysql, Microsoft SQL server, Maria DB

RDS read replicas - to scale reads from the appn..appn must update connection string to read from replica..can be within AZ, cross AZ or cross region, replication from main DB to read replica is ASYNC and is eventually consistent..appn can get old data if read happended before replication..replicas can be promoted to own DB..can be upto 15 read replicas
useful for reporting application running analytics ..can be used only for SELECT and not INSERT/UPDATE etc.
replication from Master to read replicas cost when the read replica is Cross Region. If the read replica is in cross AZ but within same region, there is no cost. In general, for data transfer between AZ there is cost, but not for managed services like RDS.

RDS Multi AZ for disaster recovery - SYNC replication between Master (AZ A) and Standby instance (AZ B, diff AZ). Same DNS name for Master and Standby for automatic switch to standby in case of failover. Not used for scaling and no manual intervation..failover in case of loss of AZ, network loss, storage loss. Read replicas can be setup as Multi AZ for Disaster recovery.
RDS change from single AZ to Multi AZ does not need downtime..need to just click on Modify. Behind the scenes, a snapshot of Main DB is taken..new DB is restored from snapshot in new AZ..sync replication is established between the DB instances

to connect to RDS Database through client, RDS should be marked as Public and security group should allow traffic from the IP on the sepcified port
you can create read replica from RDS, take snapshot, migrate snapshot, restore to some point in time

RDS - entire database and OS managed by AWS
RDS custom - provides full admin access to underlying OS and Database..Can access the instance using SSH or SSM session manager..install patch, configure settings, enable native features. Available only for Oracle and Microsoft SQL server. Deactivate automation mode before customization and better to take snapshot beforehand

Amazon Aurora - proprietary technology from AWS..not open source..not free tier..Postgres and MySQL both supported as Aurora DB..20% costly than RDS..failover is instantenous and High Avaialbiity native..can have upto 15 replicas..storage increment by 10 GB and upto 128 TB..AWS cloud optimized with 5x preformnace over MySQL on RDS and 3x over Postgres on RDS
6 copies of data across 3 AZ..4 copies needed for write...3 copies need for read..shared volumne storage with 100s of volumnes and self healing, replication and auto expanding..One Master Aurora instances takes writes..automatic failover ..Master +15 read replicas..cross region replication supported 

Aurora DB cluster - client has writer end point to connect to Master which preformed writes on the shared storage volumne. Read replicas can be scaled automatically and load balancing happens at connection level. Reader end point allows client to connect to read replicas.
1 to 35 days of backup retention period and can backtrack to any point in time
On creating aurora database, there is regional cluster created with writer instance and reader instance in diff AZ. The regional cluster has different endpoints for reader instance and write instance. The reader and writer instance also have separate enpoints assigned to them.
We can add read replica auto scaling by defining a auto scaling policy. It can be based on metric like Average CPU utilization of Aurora replicas or Average number of connections of Aurora replicas. When read replica is auto scaled, the endpoint is extended to cover the new aurora instances.
We can have read replicas of diff size like 2 aurora replica of db.r3.large and 2 of db.r3.2xlarge. In that case, for the later instances we can have a custom end point to allow the analytical queries to be sent to those. In this case, instead of having reader end point, you would then have multiple custom end points for your read queries.
WE can add AWS region to Aurora database and make it global if the instance is of appropriate size.

Aurora serverless - automated DB instantiation and scaling based on actual usage..pay per use and cost effective..useful of unpredictable, infrequent, intermittent work loads..no capacity planning needed..client connects to proxy fleet managing aurora instances

Aurora multi master - we can multiple writer inatnces. each aurora instance has both read write capability..different from noraml setup where there is one master node doing write and in case of failover, another read replica is promoted to be new master..there are multiple writer end points which client can connect to..used for continuous write avaialbility

Global Aurora - can be
Aurora cross region read replicas - easy to setup, useful for DR
Aurora gLobal database (recommeneded) - 
1 primary region for read write
upto 5 secondary regions for read only. replication lag is less than 1 second
upto 16 read replicas in each secondary region
decrease in latency
promoting another reaion for DR has RTO of less than 1 MINUTE.
Cross region replication teakes less than 1 SECOND

Aurora machine learning -
add ML based predictions to applictaion using Sql
optimized, simple and secure integration b/w Aurora and AWS ML services
Supported servcies - Amazon Sagmaker (Can be used with any ML model)..Amazon Comprehend (for sentimaent analysis)
useful for predictions, fraud detection, sentiment analysis, product recomemndation, ads targeting
Ex- Client asks what products to buy/ recommended products in sql query to Aurora..Aurora sends some data like user's profile , shopping history to AWS ML services..AWS ML services send predictions like red shirt, blue pant which is then sent to Aurora and then Aurora sends the same as query result.

RDS backup -
Automatic backup - full backup once a day during backup window..transaction log backup every 5 mins..ability to restore 5 mins back in time..from 0 to 35 days of backup retention period (0 means no backup retention/disabled)
Manual backup - Manual DB snapshots...triggered by user..can be retained as long as you want
To save money in case DB is used for 2 hours a day..instead of stopping the DB where you pay for storage, take snapshot of DB and restore when needed

Aurora backup - 
Automatic backup - from 1 to 35 days of backup retention period (can not be disabled)..point in time recovery within the tmeframe
Manual backup - Manual DB snapshots...triggered by user..can be retained as long as you want

RDS and Aurora restore options
RDS/ Aurora backup or snapshots can be restored by creating an new database
MySQL RDS database can be restored from S3 - create backup of on premises database...store backup file on Amazon S3..restore onto new RDS instance running MySQL
MySQL Aurora database can be restored from S3 - create backup of on premises database using Percona Xtrabackup...store backup file on Amazon S3..restore onto new Auroa cluster running MySQL

Aurora Database cloning -
create new Auroa DB cluster from existing one
much faster than backup and restore
uses copy on write protocol- initailly when new cluster is created, it uses the same volume as original cluster (no copying neede and hence fast)..when subsequently updates are made to the existing cluster or new DB cluster, additional storage is allocated and data is copied separately.
useful for production to staging databse creation withouit impacting prod database

RDS and Aurora Security -
Authentication - using username and password, IAM role to connect to DB (IAM authentication)
Can not SSH into instance except RDS custom
Encryption -
At rest - master and replicas encrypted using AWS KMS specified during launch time..if master is unencrypted, read replica will be unencrypted..to encrypt existing unencrypted DB, take snapshot and restore as encrypted
In flight - using TLS and use AWS TLS root certificates at client side
Security groups - to control network access to RDS/ Aurora DB
Audit logs can be enabled and sent to CloudWatch for longer retention

Amazon RDS proxy-
manged service by AWS
allow appns to pool and share DB connections to DB
helps to minimize issues of open connections and timeouts, reduce stress on DB resources of CPU, RAM -> Improved DB efficiency
enforce IAM authenticationa and credentials storage in AWS secrets manager
reduce failover time for RDS and Aurora by 66%
serverless, autocaling, multi AZ
not accessible over public, accessible only within VPC
no need to change code..client connects to proxy instead of DB
Useful for Lambda functions which come and go and can open many connections to DB 
supports RDS (except Oracle) and Aurora

Amazon ElasticCache -
get managed Redis or memcached
cache is inmemory database with high performance and low latency
helps reduce load for read intensive workloads
needs lots of code change
makes appn stateless
AWS takes care of OS patching, setup, conf mgmt, monitoring, failover, recovery, backup
We can create ElasticCahce instances on premises using AWS Outposts
We can have encryption at rest or in transit

ElasticCache Solution Architecture -
DB Cache -> Appn queries ElasticCache. If data found (Cache Hit), it is returned. Else Cache Miss, fetch from RDS and write to Cache..relives load in RDS..cache invalidation strategy to make sure that only current data is there in cache
User Session Store -> User logs into appn..appn writes user session data to ElasticCahe..user hits another instance of appn..instance retrieves the data from cache and user is logged in..makes appn stateless

Redis vs memcached -
Redis is Multi AZ with auto failover, read replicas to have high availability, data durability using AOF persistenece, backup and restore, supports sets and multisets
Memcached - No high availability, No persistence, No backup and restore, Multi threaded architecture, Multiple nodes for partitioning of data (sharding)

ElasticCache security -
IAM authentication only for Redis, else username and password based authentication
IAM policies on ElasticCache only for AWS API level security
Redis Auth -
can set password/token when creating cluster
extra level of security on top of security groups
supports SSL inflight encryption
Memcached - supports SASL based authentication (advanced)

Patterns for ElasticCache -
lazy loading - load data only when not found in cache, all read data in cache, data can become stale
write through - adds or updates data in cache when written to DB, no stale data
session store - store temp user session data in cache and invalidate using time to live (TTL) feature

ElasticCache Redis used for gaming leaderboards (who is first, who is second, thrird). Regis sorted set ensure both uniqueness and element ordering. Each time new element is added, it is ranked in real time and added in correct order. So instead of creating logic in program, redis can be used to get leader board feature.

Important ports:
FTP: 21
SSH: 22
SFTP: 22 (same as SSH)
HTTP: 80
HTTPS: 443

vs RDS Databases ports:
PostgreSQL: 5432
MySQL: 3306
Oracle RDS: 1521
MSSQL Server: 1433
MariaDB: 3306 (same as MySQL)
Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)
 
Oracle RDS database technology does NOT support IAM Database Authentication

DNS - translates human friendly hostnames to ip address..follow hierarichical naming structure
Need to register domain name to domain register like Amazon Route 53, GoDaddy
Zone file contains domain records.
Name server resolves DNS queries like which host name maps to which IP address
http://api.www.example.com. - last . is called root, .com is Top Level Domain(TLD), .example.com is Second level domain (SLD), .www.example.com is Sub domain, api.www.example.com is fully qualified domain name, http is protocol and overall is URL
Web browser wants to access example.com and asks Local DNS server (assigned and managed by your company or assigned by Internet service provider ISP). If it does not know, it asks Root DNS server (managed by ICANN) which says that it knows .com and tells to connect to .com Naming server at 1.2.3.4. Local DNS server then asks TLD DNS server(managed by IANA, barnch of ICANN) if it knows example.com. TLD DNS server tells to connect to example.com Naming server at 5.6.7.8. Local DNS server then asks SLD DNS server(managed Doamin registery like Route 53) if it knows example.com. SLD DNS server tells that IP for example.com is 9.10.11.12. Local DNS server then caches this IP address for example.com with TTL policy

Amazon Route 53 - highly available, scalable, fully managed, Authorartive (customer can update the DNS records) DNS..only amazon service to provide 100% availability SLA..also a domain register..has ability to check the health of resources..53 is a reference to a traditional DNS port.. EC2 instance has public IP..client asks example.com to Amazon Route 53. It provides the public IP address to EC2 instance to client and then client connects to EC2 instance

Route 53 records - how you want to route the traffic for a domain
each record contains 
domain/sub domain name- like example.come
record type - supported are A/ AAAA/ CNAME/ NS and advanced like CAA/ DS/ MX etc.
Routing policy - how route 53 responds to queries
TTL - amount of time the record is cached at DNS resolver

Route 53 record types-
A - maps hostname to IPv4 address
AAAA - maps hostname to IPv6 address
CNAME - maps hostname to another hostname and target domain record type can be A/AAAA. It does not allow to create record for the top node of DNS namespace(zone apex) like..we can create for www.example.com, but not for example.com
NS - name server for the hosted zone which controls how the traffic is routed for the domain

Route 53 hosted zones-
A contained of records which define how the traffic is routed for the domain and subdomain
2 types -
public hosted zone - records which define how the traffic is routed on internet like app1.mydoamin.com
private hosted zone - records which define how the traffic is routed on one or more VPC like app1.mycompany.internal
charged 0.50$ per month per hosted zone

Route 53 records - TTL (time to live)
High TTL like 24 hours - less traffic on Route 53, possibly outdated records
Low TTL like 60 sec - more traffic on Route 53, more cost, for less period of time outdated records, easy to change records
except for alias record, TTL is mandatory of each DNS record

nslookup test.site.com gives the ip address which is mapped
dig test.site.com gives the ip address which is mapped, time to live, record type

CNAME vs Alias
If you want the hostname exposed by an AWS resource like Load balancer/CloudFront to be redirected to domain name, there are2 options
CName - maps hostname to other hostname, but can not be used for root domain, only used for non root domain
Alias - points hostname to other AWS resource..works for both root and non root domain, free of cost, native health check..automatically recognizes changes in the underlying IP address, for ex- exmaple.com can be Alias for the Load balancer DNS name..extension of DNS functionality...Alias record type can be A/AAAA for AWS resources(IPv4/IPv6), you cannot set TTL (set automatically by Route 53)
Route 53 Alias Record targets can be Elastic Load balancers, CloudFront distributions, API gateway, Elastic BeanStalk enviornments, S3 web sites, VPC Interface endpoints, Global accelerator accelerator, Route 53 record in the same hosted zone. Target type can not be EC2 DNS name, hence can not set Alias record for EC2 DNS name.

Route 53 routing policies -
defines how Route 53 responds to DNS queries

following policies supported -
Simple - route traffic to single resource..There can be multiple values returned by Route 53 for the same query, then client randomly chooses one value..If Alias is specified, there can be only one AWS resource specified..can not be associated with health checks

Weighted - controls the % of requests going to each resource...each record is assigned a relative weight (weight of record/ sum of weight of all records)..weight of all records need not be 100..dns records must be of same name and type..can be associated with health checks..use cases - load balancing betweehn regions or testing a new version of appn by sending small % of traffic to it...weight of 0 for record means stop sending tarffic to the reource..if all records have weight of 0 , all records will be returned equally...create multiple records with the same name and diff IP adress and diff weights and based on the wright, the rrcord with highest weight is routed for most of the traffic

Latency based - redirects to resource which has least latency close to us..useful when latency is priority for users..latency is through the traffic between users and AWS region..Geramny users may be redirected to UA EAST..can be associated with health checks..create multiple records with the same name and diff IP adress and diff region like us-east-1, southeast-1 and based on the browser location setting or proxy setting of country, traffic will be routed to the closeset instance

Fail over - if health check on primary EC2 instance fails, traffic is routed by Route 53 to secondary instance (disaster recovery instance)..associated with health check..create 2 records with the same name and diff IP adress, one primary (mandatory to be associated with a health check) and other secondary ..If primary is unhealthy, traffic is routed to secondary

Geolocation - routing based on user location..specify location by country / continent / us state..should create a default record in case of no match of location..use cases : website localization, restrict content distribution, load balancing..can be associated with health checks..diff from latency based as for example for latency based traffic from mexico can be routed to record in US. But for geolocation, it would be routed to default.

Multi Value answer - There can be multiple values returned by Route 53 for the same query, then client randomly chooses one value..But it can be associated with health checks and returns only values for healthy resources..upto 8 healthy records are returned by multi-value query..not a subsitute of ELB, but is used for client side load balancing

GeoProximity (using Route 53 traffic flow feature) - route traffic based on geographic location of user and resource. Resource can be AWS resource (can specify AWS region for that) or non aws on premises resource (can specify latitude and longitude)..shift traffic to resource based on specified bias. To allow more traffic , bias between 1 and 99..to reduce traffic, bias between -1 and -99

IP based - route traffic based on client's IP address..provide a list of CIDR (ip address and corresponding endpoint location)..use cases  : optimized performance, reduce network costs

Route 53 Health checks -
can be used for public or private resources..HTTP health checks are only for public resources..
helpful for automated DNS failover like we have latency based policy and we are redirected to IP address in US, but if that goes down, it should not be used, instead we should be routed to another record.
3 types :
Health checks that monitor an end point (aaplication, server, other AWS resource) - about 15 diff Health checkeres around the world check endpoint..we can define threshold of default 3 seconds or 10 seconds for quick check with more cost..HTTP/HTTPS/TCP protocol are suppoted..If >18% of Health checkers say that endpoint is healthy, it is considered Healthy by Route 53. You can choose from which locations you want the health checkers for Route 53 to use..Health checker will send HTTP request to enpoint, if it returns 2xx or 3xx rsponse, it is considered healthy. Health checker can be configured to read the first 5120 bytes of response to decide pass/fail...end point must allow traffic from Route 53 Health checker IP address

Health check that monitors other health check (calculated health check) - to combine results from multiple health checks in one response..combined using AND/ OR/ NOT..can be specified how many health checks to consider for pass/ fail..Parent health check can monitor upto 256 child health checks..usage : perform maintainance for the website without causing all the health checks to fail

Health check monitoring Cloud watch alarms(full control) ..like alarms on RDS, custom metrics..useful for private resources - Health checkers are outside the VPC and conot access private end points. We create Cloud watch metric and associate Cloud watch alarm and create health check to check the alarm 

Health checks are integated with Cloud watch metrics 

Domain registrar - can be godaddy or amazon registrar..ususlly comes with DNS features..but is different from DNS service...we can buy domain on 3rd party registrar but still use Route 53 as DNS service provided. For this you create a hosted zone in route 53 and update Name server records in 3rd party website to use Route 53 name servers. Vice vera is also possible.

we need to delete the registered domain hosted zone to reduce costs. The records can stay as it is 

Well architected application -
Stateless appn- whatsmytime.com
Cost - using reserved instances (like if 2 need to be available always)..isntead of using elastic IP (which can be max 5 per region per account) using public IP with Route 53
Performance - through multi AZ, ELB
Reliability - using Route 53 to identify the Ip address, using auto scaling groups to auto scale application instead of manually scaling, making ASG and ELB as Multi AZ to prevent disaster failure in any AZ, using ELB health checks, using Alias record between ELB and Route 53 as using A record and TTL (without ELB), if one isnatnce goes down, the client using that instance will find instance as unreachable
Security - thropugh security groups between ELB and EC2 instances..making EC2 instance private instead of public
Opeartional excellence - trhough overall

Stateful appn - myclothes.com
we have client, Route 53, Multi AZ ELB, Multi AZ ASG with EC2 instances in 3 availability zones
client connects to service and Instance 1 and adds shopping cart, but second request goes to insatnce 2 and cart is lost. To resolve 
1) enable stickiness / session affinity at ELB. But if EC2 instance goes down, we lose cart again.
2) another approach would be to have user cookies and intead of EC2 instance storing shopping cart, user keeps shopping cart data in web cookie which is sent with each request..EC2 instance becomes stateless..but HTTP requests become heavier..security risk as cookie content can be altered..cookie validation needs to be done by EC2 instance..cookie can contain upto 4 KB data only.
3) introdue server session..keep session id in web cookie..when client sends shopping cart data to ec2 instance, it keeps the data in ElasticCache and id to retrieve that data is session id. client connects to 2nd EC2 instance and it also stores/retrieves the data from ElasticCache. ElasticCache has submilli second performance, so realy quick. Alternative to ElasticCache is DynamoDB.
Store/ retrive user data like name / address in RDS to which each EC2 instance in diff AZ can connect to . 
To scale reads like browsing data, 1) we can have RDS master for write and RDS read replicas for reads with replication happening between Master and Read replicas. We can have upto 5 read replicas (2) we can have RDS with ealsticCache to store read data. Lazy loading will check if data is found in Cache, if not read from RDS and store in Cache. If found then Cache hit. this will reduce traffic to RDS
To survive disaster recovey, Route 53 is 100% available, ELB is Multi AZ, ASG is Multi AZ, RDS can be Multi AZ with standby instance taking place in case of primary failover and both have same DNS name with sync replication, elasticCache can be Multi AZ
For security, enable all traffic on HTTP to ELB, restrict traffic to EC2 security group from ELB, restrict traffic to ElasticCache security group from EC2 security group, restrict traffic to RDS security group from EC2 security group

Stateful appn - mywordpress.com
we have client, Route 53, Multi AZ ELB, Multi AZ ASG with EC2 instances in 3 availability zones, RDS database to store user data and blogs in mySQL databaase
Instead of RDS, we can use Aurora and get read replicas, Multi AZ and global..using Aurora mySQL
To store user images, we can have EBS volume if there is a single instance in one AZ..but if we have multiple instances in multi AZ, then each will have diff EBS volume and image stored in one EBS volumne in AZ1 will not be available in AZ2 EBS volume. To resolve this, we can use EFS with ENI between EFS and Ec2 instance. EFS creates ENI in each AZ and storage is shared between EC2 instances regardless of AZ. EBS is useful for single instance appn while EFS is useful for distributed appn.

To instantiate applications quickly,
EC2 instances - 
	use Golden AMI - install appn, OS depencies beforehand and launch EC2 instance from Golden AMI
	User data scripts - for dynamci configuration liek URL
	Hybrid - mix Golden AMI and User data - ELastci Bean stalk
RDS Databases - restore from snapshot- have schema and data ready
EBS volumes - restore from snapshot- disk will be formatted and have data

Elastic Bean stalk -
managed service - automated instance configuration, scaling, load balancing, capacity provisioning, health checks
free to use but pay for underlying instances
full control over configuration

components :
application - collection of bean stalk components like environment, version, configuration
version - iteration of application code
environment - collection of aws resources running an application version at a time...tiers (web server envrionment tier which can push message to SQS queue and can be used for web applications and worker tier which pulls message from SQS queue and can scale based on the number of messages in SQS queue and is used for batch or queue processing)..you can create multiple envs (dev, test, prod)
create appn -> upload version -> launch env -> manage env (update version and deploy new version)

supported platforms - Go, Java SE, Java with Tomcat, .Net core on Linux, .Net on Windows, PHP, Python, Node js, Ruby, Packer builder, Single container docker, multi container docker, preconfigured docker, custom lang

deployment modes - single instance with elastic IP (great for dev).Single Instance Mode will create one EC2 instance and one Elastic IP and is in free tier, high availability with load balancer (great for prod)

elastcibeanstalk creates a cloudformation in the backend with stack where multiple events happen

Amazon S3 - infinitely scaling storage
websites use s3 as backbone
AWS services use S3 as an integration

Use cases -
Backup and storage
Disaster recovery
Archival - S3 Glacier - Nasdaq stores 7 years of data in here
Hybrid cloud storage
Application hosting
Media hosting
Data lakes and big data analytics - Sysco uses this
Software delivery
static website

Amazon S3 buckets -
allow to store objects (files) in buckets (directories)
s3 is a global service (buckets created will be visible across all regions) but buckets are created in a region..defined at region level..must have globally unique names across all regions and all accounts
naming convention - no uppercase, no underscore, no IP, 3-63 chars long..must start with lower case or number..must not start with prefix xn-- and must not end with suffix -s3alias

Amazon S3 objects -
files with key
key contains prefix (myfolder1/myfolder2/) and object name(myfile.txt) like s3://my_bucket/myfolder1/myfolder2/myfile.txt
key is full path like s3://my_bucket/myfile.txt or like s3://my_bucket/myfolder1/myfolder2/myfile.txt
no directories within buckets
object values are content of body. can be upto 5 TB (5000GB). for data more than 5 GB, multi part upload must be done
contains metadata (key/value pairs - system or user metdata)
tags - useful for security/liefcyle
version id - if versioning is enabled

Object has a public URL which does not work, but there is a S3 presigned uRL which opens as it contains signature of user accessing the URL within it

Amazon S3 security -
User based - IAM policies - which API calls should be allowed for a specific user from IAM
Resource based - 
Bucket policies - bucket wide rules defined through S3 console..allow cross account access..Json based policies..contains Version..Statement contains SID, Action (Set of API to allow or deny like GetObject), Principal (the user or account to apply the policy to), Effect (Allow/ Deny), Resource (buckets and objects)...this can be used to give public access to objects..grant access to other account..force objects to be encrypted at upload
Object ACL - fine garined..can be disabled
Bucket ACL - less common ,..can be disabled
IAM principal can access S3 object if user IAM permissions allow it OR respurce policy allows it AND there is no explicit deny. Explicit DENY in an IAM Policy will take precedence over an S3 bucket policy.

encryption can be done for s3 objects using encryption keys 
Bucket settings can be done to block public access..can be set at account level

s3 can be used to have static web site hosting through s3bucket -> properties- > enable static web site hosting and upload html file with name index.hmtl (html pages).. to prevent 403 forbidden error, we need to allow public access to s3 bucket...there can be different website url depending on the region (diff of . or -)

Amazon S3 Versioning -
can version files in Amazon S3
versioning enabled at bucket level
version changed to 1,2,3
prveents accidental deletion, allows to restore a version
easy rollback to prev version
any file not versioned prior to enabling versioning has version null
suspending versioning does not delete previous versions
Deleting a version permanently deletes that..If we deleted a file with null verison, it just creates a delete marker with a version id. when we click on show version, we see that delete marker. The file is not shown in web page. To restore the file, we need to delete the delete marker.

Amazon S3 replication -
must enable versioning in source and destination S3 buckets
copying is asynchronous
Buckets can be diff AWS acounts
must give appropriate IAM permissions to S3
2 types- 
CRR (Cross Region Replication) - for compliance, low latency access, replication across accounts
SRR (Same Region Replication) - to replicate data from prod to test env, log aggregation
After enabling replication, only new objects are replicated
S3 batch replication for replication of existing objects and failed replication objects
For deleted objects - can replicate delete markers from source to target via optionsla settings..deltions with a version id not replicated
No chaining of replication - if bucket 1 has replication to bucket 2 which is replicated to bucket 3, objects created in bucket 1 are not replicated to bucket 3

S3 Storage classes -
S3 Standard General Purpose - 99.99% availability, used for frequently accessed data, low latency and high throughout, sustain 2 concurrent facility failures, use cases : big data analytics, mobile and gaming appns, content distribution
S3 Infrequent acccess - less frequently accessed data but with rapid access, lower cost than S3 standard..2 types:
	S3 Standard Infrequent access - 99.9% availability..use cases : disaster recovery, backups
	S3 One zone infrequent access - 99.5% availability..use cases : stroing secondary backup copies of on premise data or data which you can recreate..high durability in single AZ, data lost when AZ is destroyed
S3 Glacier - low cost storage for archiving/backup..pay for storage and retreieval
S3 Glacier Instant Retrieval - millisecond retrieval, useful for data accessed in a quarter..min storage duration of 90 days
S3 Glacier Flexible Retrieval - 3 tiers - Expedited (1 to 5 mins), Standard (3 to 5 hrs), Bulk (5 to 12 hrs and free)..min storage duration of 90 days
S3 Glacier Deep Archive - for long term storage - 2 tiers - Standard (12 hrs), Bulk 48 hrs)..min storage duration of 180 days
S3 Intelligent Tiering - move objects between access tiers based on usage..small monitoring and auto-tiering feee, no cost of data retrieval...tiers - frequent access tier (automatic and default tier), infrequent access tier (automatic and for objects not accessed for 30 days), archive instant access tier (automatic and for objects not accessed for 90 days), archive access tier (optional and configurable from 90 to 700+ days), deep archive access tier (optional and configurable from 180 to 700+ days)
can move between classes manually or using S3 lifecycle configurations

S3 Durability and Availability -
Durability - how many times, object is lost by S3..99.999999999% 11 9's high durability of objects across Multi Azure
same for all storage classes
on stoing 10 million objects, 1 object is lost in 10000 years
Availability -how readily the service is available..depends on storage class
S3 standard has 99.99% availability - not available 53 minutes a year

Transition objects between storage classes can happen in the order (top to all below)
Standard  -> Standard IA -> Intelligent tiering -> One Zone IA -> Glacier Instant Retrieval -> Glacier Flexible Retrieval  -> Glacier Deep Archive
Can be automated through lifecycle rules like move from Standard to Standard IA after 30 days and Glacier Instant after 90 days and Glacier Deep Archive after 180 days etc.

Amazon S3 lifecycle rules -
Transition actions - configure objects to transition to another storage class like move objects to Standard IA 60 days after creation..move to glacier archiving after 6 months
Expiration actions - configure objects to expire/delete after sometime..acess log files can be set to delete after 365 days..incomplete multip part uploads can be deleted..old versions of files if versioning is enabled can be deleted
Rules can be set for certain prefixes like s3://mybucket/mp3/*
Rules can be created for certain object tags like department:finance

S3 analytics runs on S3 bucket and provides recommendations for when to transition objects to appropriate storage classes. Provides recommendations for Standard and Standard IA..not for One zone IA and Glacier..Report is updated daily..provide data between 24 to 48 hours

S3 requester pays -
In general S3 bucket owner pays for the storage cost and the data transfer cost associated with the bucket. But in case of large data sets to be shared with diffreent account, we can have requester pays enabled and then S3 bucket owner pays for storage cost, but netwroking cost is payed by requester. For this, requestor must be authenticated in AWS, can not be anonymous

S3 event notifications - you can react to event happening in S2 by sending notifications to SNS, SQS, lambda
events can be S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication etc.
object name can be filtered like *.jpg
can create as many S3 events, notifictaions typically deliver events in seconds but can take min or longer..event notification can be sent to SQS, lambda functions, SNS
Use case : geenrate thumbnail of image uploaded to S3
For the S3 event notification, instead of IAM role on S3, we use IAM permissions enabled for SNS resource access policy, SQS resource access policy, Lambda Resource policy
All S3 event notifications are sent to Amazon EventBridge form where it can be sent to over 18 AWS servcies as destinations based on rules. We can have advanced filtering with JSON rules on metdata, object name and size. Notifications can be sent to multiple destinations like Step functions, Kinesis streams/Forehose etc. EventBridge has capabilities like Archive, Replay events, Reliable delivery

SQS - Simple Queue Service

S3 Baseline Performance -
S3 automatically scales to high performance with low latency of 100-300 ms
There can be upto 3500 POST/PUT/DELETE/COPY and 5500 GET/HEAD requests per second per prefix in a bucket
there are no limits on number of prefixes in the bucket
example object path => prefix
bucket/folder1/sub1/file => /folder1/sub1/
bucket/folder1/sub2/file => /folder1/sub2/
bucket/1/file => /1/
bucket/2/file => /2/
if we spread reads across all prefixes evenly, there can be 22000 GET/HEAD requests per second in bucket

S3 Performance -
1) Multi part upload - recommended for files > 100MB, must for files > 5 GB
File divided in parts and uploaded parallely and then put togetehr by S3 bucket

2) Transfer acceleration - to increase transfer speed in upload and download..transfer file to AWS edge location (currently over 200) over public www which then forwards data to S3 bucket in target region over private AWS network..comptaible with multi part upload..big file transfer from USA to edge location in USA over public www and then to target S3 bucket in Australia over private AWS

3) Byte range fetches - parallize get by requesting specific byte range..can request only for first XX bytes of header ..partial data retrieval can be done..better resilenece in case of failures..used to speed up download

S3 select and glacier select - retrieve less data using server side filtering..400% faster and 80% cheaper than filtering at client side ..can filter by rows and cols like in SQL statement..less netwrok transfer..less CPU cost client side..like get CSV with S3 select..S3 bucket does server side filtering and send filteered dataset

S3 batch operations-perform bulk operation on multiple S3 objects in single request like
modify object metadata and properties
copy objects between S3 buckets
encrypt unencrypted objects
modify ACLs, tags
Restore objects from S3 Glacier
Invoke lambda function to perfrom custom action on object

Job consists of list of objects, action to perform and optional parameters
S3 batch operation manages retries, sends completion notifications, generate reports, tracks progress
we can use S3 inventory to get object lits and use S3 select to filter the objects 

Amazon S3 Object Encryption

----------------------------------

A Singleton class supports interface implementation, while static classes cannot implement interfaces. The singleton, like any other instance of a class, lives on the heap. To its advantage, a huge singleton object can be lazily loaded whenever required by the application. On the other hand, a static class encompasses static methods and statically bound variables at compile time and is allocated on the stack. It is hard to mock static class for testing.

Creating a static class :
Declaring all constructors of the class to be private.
Providing a static method that returns a reference to the instance. The lazy initialization concept is used to write the static methods.
The instance is stored as a private static variable.

how does hashmap work?
HashMap uses multiple buckets and each bucket points to a Singly Linked List where the entries (nodes) are stored.
Once the bucket is identified by the hash function using hashcode, then hashCode is used to check if there is already a key with the same hashCode or not in the bucket(singly linked list).
If there already exists a key with the same hashCode, then the equals() method is used on the keys. If the equals method returns true, that means there is already a node with the same key and hence the value against that key is overwritten in the entry(node), otherwise, a new node is created and added to this Singly Linked List of that bucket.
If there is no key with the same hashCode in the bucket found by the hash function then the new Node is added to the bucket found.
https://medium.com/@basecs101/internal-working-of-hashmap-in-java-latest-updated-4c2708f76d2c#:~:text=HashMap%20uses%20multiple%20buckets%20and,bucket(singly%20linked%20list).

what is the use of immutable object?
how can you create immutable class in java?
what is meant by transitive dependency in maven? -  if we added JUnit as a dependency in pom.xml under <dependencies> tag. It will download the other hamcrest-core-1.3.jar file and it is under Maven Dependencies, This .jar file can be called Transitive dependency
what are dependency scopes in maven? - https://www.baeldung.com/maven-dependency-scopes
what is meant by provided scope? what will happen if that dependency in provided scope is not provided?
what is benefit of spring framework and spring boot framework?
what is meaning of @springbootannotation?
features of java 8?
what is most challenging issue you have solved in any project? - concurrency issue, negtauve inventory, rca and data fixing
what is the most complicated thing done in current project? - customer mindset management, moving from legacy excel to web appn, automation, product backlog refinement, movement to agile
what are you doing in your current project? what is the tech stack? what do you do funtionally in there not technically?

can top level class be static?
what is the use of static class?
what is the use of static nested class? - can be used in factory pattern to instantiate object with factory methods and the main class constuctor can be made private therby enforcing the use of factory method itself. It can be used for initilizing trhread safe singleton . Refer https://www.initgrep.com/posts/design-patterns/thread-safety-in-java-singleton-pattern#:~:text=Inner%20Static%20Class%3A%20In%20the,inner%20class%20is%20first%20referenced.
can static method access non static variables?
if we pass array to a method, is it pass by value or pass by reference?
program to find second largest number in array?
how are rest apis secured?
how does oauth work?
what is microservice and why is that needed?
what is swagger..what doc is provided in that..can you run service there?


why do want to leave tcs - assigned to change mgmt work while I am grooming myself in microsrevices and somehow mgmt keeps changing and my aspirations are lost in between. so left with no option other than looking for options outside tcs.
why infosys? - good brand name comparable to my current organization, values employees
which quality makes you a good candidate - in past 12 years, high performer in tcs. not only technical but also functional details and what is best for the customer.
what has been your learnings - you failed but you learnt ? - server side calculation
How do you resolve workplace conflicts? - team has team members and both have good ideas..very less instances but thru resolving miscoumminication and scientifically idenityfyinf pros and cons..like svn with jenkins and moving to git
If your team resists your idea, what would you do? - explain scientifically with evidence like for perf mgmt - cahing vs  lazy loading with server side pagination but master data is there which does not change and user has option to change page size to 200 so lazy loading may not be an option
How we can handle the situation when we found code is not complaint before one day of deployment.  - understand reason for non complaince, like if free trail was used for development for a licensed software or may be code is not adhering to security guidelines or may be anything else..better to be honest with customer and let them know about it. It can be that they think they can go ahead with the code as app is intranet based and they are not very much concerned on the misuse with respect to security
Explain any one past issue and its mitigation strategy.  - negative inventory problem..code fix..monitoring query with a point of truth and ensuring that data is not further corrupted till the fix is deployed to prod
Salary expectations and negotiation - in between 40 to 50% increment- promotion recommendation given and i will get in between 20 to 30% increment with promotion..depends on what you can offer ..if i am asking 40 to 50% and you give me 35% increment, I can think about it, but if you give me 15%, it is something I am not looking for
working in pressure - entire tenure worked on dev project with high work load, in agile, we work in sprints with 3 weeks deadline and there is constant pressure

web server vs app server - Web server is useful or fitted for static content.	Whereas application server is fitted for dynamic content.Transactions and connection pooling is not supported.	Transactions and connection pooling is supported.Web Server examples are Apache HTTP Server , Nginx.	Application Servers example are JBoss , Glassfish.
can we have multi threaded appns on both web and appl server - In web servers, multithreading is supported.	While in application server, multithreading is not supported.
what is heap dump and how to capture it
when can memory leak happen
did it happen that appn got stopped before of memory issue
how to solve memory leaks
metaspace and permgenspace
java 8
what is functional interface
terminal operation in java 8
spring what all you have worked on 
diff between prototype and singleton bean
can we have prototype bean inside singleton bean - https://javabeat.net/spring-method-injection/, https://javabeat.net/spring-method-injection/ see @Lookup annotation..https://www.quora.com/Can-singleton-bean-be-injected-into-prototype-bean https://www.logicbig.com/tutorials/spring-framework/spring-core/injecting-singleton-with-prototype-bean.html
what is race condition
how to solve performance issues
procedures and functions
exclude some configuration in spring
how server side pagination was done for quering in each page - https://mysql.rjweb.org/doc.php/pagination
front end experience
scrum master work..what did you do

tech stack - 
What is the difference between an abstract class and an interface in Java? Can you explain the concept of multithreading in Java? How can you create and manage threads? What are the differences between checked and unchecked exceptions in Java? Give examples of each. How does Java handle memory management? Explain the concept of garbage collection. What is the difference between method overloading and method overriding in Java? What are the access modifiers in Java (e.g., public, private, protected, default)? Explain their visibility and usage.

Design Car Rental System- 1. create a method to get all cars which matched the filter criteria. 2. create a method to get all cars which are available for booking b/w 2 given dates. 3. create a method to book a car and it should handle concurrency. ie 2 users shouldn't be able to book same car for same dates if they try to book at same time.

Reverse a String using minimum space

test cases
linked list 
scenario based problems
data structures
design patterns
hashing working

https://www.ambitionbox.com/interviews/newgen-software-technologies-interview-questions/java-developer

Clustered index - https://www.spotlightcloud.io/blog/when-to-use-clustered-or-non-clustered-indexes-in-sql-server#:~:text=A%20clustered%20index%20is%20an,on%20a%20primary%20key%20column.
Thread dump vs heap dump - https://www.baeldung.com/java-heap-thread-core-dumps
usage of thread dump - for analyzing one thread blocking another thread, race condition may be
trigger
constraint - primary, foreign , not null, unique
foreign key importance
find max salary for each dept. table has emp , salary and dept columns

remove duplicate rows from a table
delete from t
 where rowid IN ( select rid
                    from (select rowid rid, 
                                 row_number() over (partition by 
                         companyid, agentid, class , status, terminationdate
                                   order by rowid) rn
                            from t)
                   where rn <> 1);
				   
				   DELETE FROM your_table
WHERE rowid not in
(SELECT MIN(rowid)
FROM your_table
GROUP BY column1, column2, column3);

app slow after deployment
performance how to solve
how to check if query is optimized
what to do for low performing associate
what is delivery timeline could not be met
what is deployment diagram and architecture diagarm
phases in sdlc
servlet programming how to m

what is lock in database and how to avoid it
what is critical path in project
what is feature traceability matrix - for impact analysis
program to work with multi dimensional array
what is use of microservice and rest api
what is controller - controller and rest controller diff and MVC architecture
what are 4 pillars of performance tuning - db tuning to optimize query, if query returns millions of rows then lazy loading or pagination, reduce number of queries like reslting from hibernate N+1 prpblem, UI loading solve, master data management issue
what is benchmark for saying if any query is optimized - depends on synchronous or asynchronous call. for synchronous call, depends on project. in general, it should be that query should return response in seconds.

difference between arraylist and linkedlist java - arraylist allows to access element at particular position easily O(1), but linked list would have to travserse the entire list O(n). LinkedList is fast for appending or deleting large elements at the ends of a list O(1), but slow to access a specific element. ArrayList is fast for accessing a specific element but can be slow to add to either end, and especially slow to delete in the middle as it requires shifting all the elements only for removal from the end it is O(1). Arraylist is suitable for cache as for linkedlist nodes may be spread all over the ram.
https://stackoverflow.com/questions/322715/when-to-use-linkedlist-over-arraylist-in-java
https://www.w3schools.com/java/java_linkedlist.asp
The ArrayList class has a regular array inside it. When an element is added, it is placed into the array. If the array is not big enough, a new, larger array is created to replace the old one and the old one is removed.The LinkedList stores its items in "containers." The list has a link to the first container and each container has a link to the next container in the list. To add an element to the list, the element is placed into a new container and that container is linked to one of the other containers in the list.

Write program to filter employees using java stream
int[] arr = {1,2,3};
    Arrays.stream(arr).boxed().sorted(Collections.reverseOrder()).forEach(System.out::println);
	books.stream().filter(obj -> obj.getAuthor().equals("Udayan Khattry")).collect(Collectors.toList());
	List<Integer> list = Arrays.asList(10, 20, 8);
	list.stream().sorted().forEach(System.out::println);
	
Spring Boot has various exceptions for different scenarios.
BeanCreationException is Spring trying to inject a bean that doesn’t exist in the context. NoSuchBeanDefinitionException
UnsatisfiedDependencyException gets thrown when, as the name suggests, some bean or property dependency isn’t satisfied. UnsatisfiedDependencyException: Error creating bean with name ‘purchaseDeptService’: Unsatisfied dependency expressed through constructor parameter 0 (bean was not injected due to missing annotation). dependency cannot be resolved when there’s more than one bean that satisfies it. To solve this, we may want to add @Qualifier to distinguish between the repositories:
DataIntegrityViolationException - like setting not null value to null, deleting child before parent etc.
DataAccessException - for database access errors
HttpMessageNotReadableException - for errors in HTTP message conversion
MethodArgumentNotValidException - for validation errors in request parameters
NoSuchBeanDefinitionException - for errors in bean configuration
UnauthorizedException - for authentication and authorization errors
https://reflectoring.io/spring-boot-exception-handling/

Adding port number as 0 for a service in Spring boot - Spring Boot simply asks the OS to give it a available port, and the OS determines what port to hand out based on its own configuration. The range for random ports depends on a couple of factors:
All port numbers are 16-bit unsigned integers, so the highest possible value is 65535.
Ports below 1024 are considered privileged ports on Unix-based systems (including macOS and Linux). Normal users can't open sockets on these ports, so typically, Spring Boot will not use them.
Ports from 1024 to 49151 are known as the User or Registered Ports, and ports from 49152 to 65535 are the Dynamic or Private Ports. These are typically the ports Spring Boot might choose from.
If you are not running Spring Boot with root or administrator privilege, then the random port is likely to be between 1024 and 65535 (this is not a strict rule and can be different based on your OS configuration).
https://stackoverflow.com/questions/54950710/spring-boot-app-actually-running-on-port-0-instead-of-random
It doesn't actually start it on port 0, it starts it on a random port. In your eureka server you will see that it is in port 0. So if you have problems communicating with each other, it is because in every microservice you start with random port would have to configure in your application.yml a preferIpAddress to find it by ip and not by hostname:

Impact of changing annotations on spring boot classes - can define scope of bean, may be @EnableCcahing, make some method @Cacheable (value ="name", key=""). Make some bean as dependent bean and govern bean creation, apply aspect orieneted progamming to some annotation like @service or @repository
Changing annotations on Spring Boot classes can have significant impact on the application.
Changing annotations can affect the behavior of the application
Annotations can control the lifecycle of beans, security, caching, etc.
Removing or modifying annotations can cause runtime errors
Adding new annotations can introduce new functionality
Annotations can affect the performance of the application
aop - https://www.digitalocean.com/community/tutorials/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations - using aspectj dependency +@aspect class(responsible for handling cross cutting concern like logging) + joinpoint (the point in the application such as method exection, exception handling) +pointcut (expression used to match joinput to find if advice is to be applied or not) +@advice (actions on join point like @before, @after etc) 
@Aspect
public class EmployeeAnnotationAspect {

	@Before("@annotation(com.journaldev.spring.aspect.Loggable)")
	public void myAdvice(){
		System.out.println("Executing myAdvice!!");
	}
}

javascript difference between splice and slice
slice returns a piece of the array but it doesn't affect the original array. splice changes the original array by removing, replacing, or adding values and returns the affected values.
https://stackoverflow.com/questions/37601282/javascript-array-splice-vs-slice

Given array of objects [{model: apple , price: 2000}, {model : apple , price : 1000}, {model: samsung , price: 500}] Return output as {apple : 1000, samsung :500} Ie flatten the array to map if model is same return min price
  public static void m7() {
    ObjectPrice[] arr = {new ObjectPrice("apple" , 2000), new ObjectPrice("apple" , 1000), new ObjectPrice("samsung" , 500)};
    HashMap<String, Integer> output = new HashMap<String, Integer> ();
    List list = Arrays.asList(arr).stream().map(obj -> {

      Integer price = obj.getPrice();
      if (output.containsKey(obj.getModel())) {
        Integer existingPrice = output.get(obj.getModel());
        if (price> existingPrice) {
          price = existingPrice;
        }
        output.replace(obj.getModel(), price);
      } else {
        output.put(obj.getModel(), price);
      }

      return output;
    }).collect(Collectors.toList());
    list.stream().distinct().forEach(System.out::println);

  }
  
 Given employee class find 10th Max. Salary java
 https://medium.com/@anil.java.story/second-highest-salary-using-java-streams-api-d8b5eb8051b5
 Arrays.asList(arr).stream().sorted(new Comparator<ObjectPrice> () {
      @Override
      public int compare(ObjectPrice o1, ObjectPrice o2) {
        return o1.getPrice().compareTo(o2.getPrice());
      }
    }).collect(Collectors.toList()).get(1)
	
Lazy loading of beans in spring boot - @Lazy annotation on specific bean or for all the beans through spring.main.lazy-initialization:true
faster startup time but may be some problems are known in production. Lazy loading is a technique used to postpone the initialization of an object until it is actually needed. Lazy loading allows beans to be created only when needed instead of creating them all at application startup.

SQL find 10 max salary from employee table , can use offset and limit Postgresql function

Thread pool executer fire 3 method in parallel and wait for all 3 to complete - using join

@async annotation usage how to make async api

Atomic Integer in java - thread safe mutable integer. Normal Integer is immuitable and thread safe, but int is mutable and to make it thread safe you need to use synchronized keyword. Hence Atmoic Integer is used.. It uses compare and swap feature of OS (compareandset methods). Integer is immutable and therefore is thread-safe by default, no synchronized or volatile needed. However, the downside of immutability is that the value can never change. If you need a changing value use AtomicInteger or some other method or volatile/synchronized keywords.
private AtomicInteger count = new AtomicInteger(0);
public int updateCounter() {
    return count.incrementAndGet();
}
https://stackoverflow.com/questions/38846976/what-is-the-difference-between-atomic-integer-and-normal-immutable-integer-class#:~:text=Integers%20are%20object%20representations%20of,asigning%20a%20value%20to%20variable.
https://jenkov.com/tutorials/java-concurrency/compare-and-swap.html
https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/atomic/package-summary.html

how is metaspace better than permgenspace java
PermGen Space											MetaSpace
PermGen always has a fixed maximum size.				Metaspace by default auto increases its size depending on the underlying OS.
Contiguous Java Heap Memory								Native Memory(provided by underlying OS)
Max size can be set using XX:MaxPermSize				Max size can be set using XX:MetaspaceSize
Comparatively ineffiecient Garbage collection. 			Comparatively effiecient Garbage collection. Deallocate class data concurrently and not during GC pause.
Frequent GC pauses and no concurrent deallocation.
Limited to MaxPermSize – default ~64M - 85M
Once exhausted throws OutOfMemoryError "PermGen space".
Restart with larger MaxPermSize.

The maximum metaspace size can be set using the -XX:MaxMetaspaceSize flag, and the default is unlimited, which means that only your system memory is the limit. The -XX:MetaspaceSize tuning flag defines the initial size of metaspace If you don’t specify this flag, the Metaspace will dynamically re-size depending of the application demand at runtime.

what does foreach internally does in java 8?
We use forEach to iterate over a collection and perform a certain action on each element. The action to be performed is contained in a class that implements the Consumer interface and is passed to forEach as an argument.
The Consumer interface is a functional interface (an interface with a single abstract method). It accepts an input and returns no result.
The enhanced for-loop is an external iterator, whereas the new forEach method is internal. Internal Iterator — forEach manages the iteration in the background and leaves the programmer to just code what is meant to be done with the elements of the collection. method only needs to know what is to be done, and all the work of iterating will be taken care of internally. External iterators mix what and how the loop is to be done. Enumerations, Iterators and enhanced for-loop are all external iterators (remember the methods iterator(), next() or hasNext()?). In all these iterators, it’s our job to specify how to perform iterations.

Difference Between Collection.stream().forEach() and Collection.forEach()
Collection.forEach() uses the collection’s iterator (if one is specified), so the processing order of the items is defined. In contrast, the processing order of Collection.stream().forEach() is undefined. Many collections (e.g. ArrayList or HashSet) shouldn’t be structurally modified while iterating over them. If an element is removed or added during an iteration, we’ll get a ConcurrentModification exception.Furthermore, collections are designed to fail fast, which means that the exception is thrown as soon as there’s a modification.Similarly, we’ll get a ConcurrentModification exception when we add or remove an element during the execution of the stream pipeline. However, the exception will be thrown later.

make arraylist read only in java 8
An ArrayList can be made read-only easily with the help of Collections.unmodifiableList() method. This method takes the modifiable ArrayList as a parameter and returns the read-only unmodifiable view of this ArrayList.

primary annotation in spring
https://www.baeldung.com/spring-primary

cache control and pragma expires in header
Pragma is the HTTP/1.0 implementation and cache-control is the HTTP/1.1 implementation of the same concept. They both are meant to prevent the client from caching the response. Older clients may not support HTTP/1.1 which is why that header is still in use. Pragma is the HTTP/1.0 implementation and cache-control is the HTTP/1.1 implementation of the same concept. They both are meant to prevent the client from caching the response. Older clients may not support HTTP/1.1 which is why that header is still in use.

spring security which class is to be extended and which method to be overridden
WebSecurityConfigurerAdapter and override configure method to disbale cors or csrf or permit request to which all role applicable. cors is cross origin resource sharing to allow communication between which domains 

for in memory authentication which spring security class to be extended
authenticationmanagerbuilder and inmemoryauthentication method
@EnableWebSecurity
public class config extends WebSecurityConfigurerAdapter { 
  
    // Adding the roles 
    @Override
    protected void configure(AuthenticationManagerBuilder auth) throws Exception { 
        auth.inMemoryAuthentication() 
                .withUser("Zack") 
                .password("aayush") 
                .roles("admin_role") 
                .and() 
                .withUser("GFG") 
                .password("Helloword") 
                .roles("student"); 
    } 
  
    // Configuring the api 
    // according to the roles. 
    @Override
    protected void configure(HttpSecurity http) throws Exception { 
        http. 
                httpBasic() 
                .and() 
                .authorizeRequests() 
                .antMatchers("/delete").hasRole("admin_role") 
                .antMatchers("/details").hasAnyRole("admin_role","student") 
                .and() 
                .formLogin(); 
    } 
  
    // Function to encode the password 
    // assign to the particular roles. 
    @Bean
    public PasswordEncoder getPasswordEncoder(){ 
        return NoOpPasswordEncoder.getInstance(); 
    } 
}

Write a program to find common prefix.
String arr[] = {"armstrong", "armor", "archer"}

How can we handle NosuchElementFound Exception.

["Dff56", "68ghb77", "jgg366"]
Program to remove string from above array elements & calculate sum.

https://www.glassdoor.com/Interview/Coforge-Interview-Questions-E36319.htm

https://www.geeksforgeeks.org/overriding-in-java/
co variant return types -  it is possible to have different return types for an overriding method in the child class, but the child’s return type should be a subtype of the parent’s return type. The overriding method becomes variant with respect to return type.
It helps to avoid confusing type casts present in the class hierarchy and thus making the code readable, usable and maintainable.
We get the liberty to have more specific return types when overriding methods.
Help in preventing run-time ClassCastExceptions on returns

can stream be used for map java
Map (HashMap, TreeMap, etc.) is not a collection inheriting from Collection. Stream is not supported for Map. If you want to stream a Map, you must first use the entrySet() method to turn the Map into a Set, which is a Collection type.
Map<Integer, String> map = new HashMap();
Stream<Map.Entry<Integer, String>> stream = map.entrySet().stream();

how does stream opearte in java
stream does pipeline processing on a datasource which is a collection like array or arraylist etc. It has various intermediate opeations which can tranform the stream into another stream and terminal opeartion which evaluet the stream and return the output. Intermediate opeartions are lazy in nature and called only when terminal operation is called.
Java streams represent a pipeline through which data will flow and the functions to operate on the data. A pipeline in this instance consists of a stream source, followed by zero or more intermediate operations, and a terminal operation.

differerence between map and flatmap
 map emits one item for each entry in the list and flatMap is basically a map + flatten operation. Both map and flatMap can be applied to a Stream<T> and they both return a Stream<R>. The difference is that the map operation produces one output value for each input value, whereas the flatMap operation produces an arbitrary number (zero or more) values for each input value.what flattening a stream consists in, consider a structure like [ [1,2,3],[4,5,6],[7,8,9] ] which has "two levels". Flattening this means transforming it in a "one level" structure : [ 1,2,3,4,5,6,7,8,9 ]
 
can we throw some custom exception during stream processing
From a stream processing, we can throw a RuntimeException. It is meant to be used if there is a real problem, the stream processing is stopped; Or if we don't want to stop the whole processing, we only need to throw a caught Exception. Then it has to be handled within the stream.
https://www.javaadvent.com/2020/12/exceptions-and-streams.html
Checked exceptions don’t play nice with the Java Stream API as the normal functional interface methods do not declare throwing checked exception in theri signature. So any checked excception needs to be catched within the stream processing and converted to run time exception and thrown later
Use @SneakyThrows (Lombok) or Unchecked (jOOL) to get rid of checked exceptions with Streams
Consider Try (vavr) ..concept of either containg either the element or the exception whenever you want to collect the errors occurring for an element instead of terminating the Stream.
https://dzone.com/articles/exception-handling-in-java-streams

can we use functional interface for anonymous class?
Lambdas are limited to functional interfaces. If you want to create an instance of an abstract class, you can do it with an anonymous class, but not a lambda. Similarly, you can use anonymous classes to create instances of interfaces with multiple abstract methods.
lambdas != anonymous inner classes
Lambdas in Java replace many of the common uses of anonymous inner classes. The result is much more compact, readable, and obvious code. No, the implementation of lambdas is not based on anonymous inner classes.In an anonymous class, this keyword refers to the anonymous class itself. But in the case of a lambda expression, this refers to its enclosing class.
We can also declare member variables in an anonymous class, which isn’t possible in the case of a lambda expression. Thus, an anonymous class can have a state. Variables declared inside the lambda expression act as local variables. Both of them, though, have access to member variables of the enclosing class.On compilation, for each anonymous class, a separate class file gets generated. in the case of a lambda expression, an invokedynamic instruction is added to the class file. This opcode instruction helps to figure out the functional interface method to be called. The lambda expression is better compared to the anonymous class in terms of performance. This is because an anonymous class leads to an extra class file on the compilation, which takes additional time during class loading and verification during runtime.
The performance of lambda expressions is better because the invokedynamic instruction lately binds the lambda expression to the functional interface method.
https://www.baeldung.com/java-lambdas-vs-anonymous-class#:~:text=Even%20in%20the%20case%20of,there%20are%20differences%20between%20them.

difference between lambda expression and method reference
both can be used in place of other but there's a subtle difference between lambdas and method references. For instance, compare: myInstance::getStuff and () -> myInstance.getStuff()
This will mostly be considered equivalent. But. If myInstance happens to be null, the lambda will throw a null pointer when the lambda gets evaluated, but the method reference will throw right away when trying to access the reference.
https://stackoverflow.com/questions/24487805/lambda-expression-vs-method-reference
https://www.reddit.com/r/java/comments/xlrth2/subtle_difference_between_lambda_and_method/?rdt=45745

can aggragetor pattern be used with api gateway microservices
https://www.java67.com/2023/01/aggregator-microservice-pattern-in-java.html
https://waytoeasylearn.com/learn/api-gateway-pattern/ ocleot api gateway in .Net has request aggregation feature
https://anjireddy-kata.medium.com/api-gateway-pattern-in-microservices-spring-cloud-api-gateway-85d7f0576af6

Serialization is the conversion of the state of an object into a byte stream which we can then save to a database or transfer over a network.; deserialization does the opposite.Classes that are eligible for serialization need to implement a special marker interface, Serializable.static fields belong to a class (as opposed to an object) and are not serialized. Also, note that we can use the keyword transient to ignore class fields during serialization.
When a class implements the java.io.Serializable interface, all its sub-classes are serializable as well. Conversely, when an object has a reference to another object, these objects must implement the Serializable interface separately, or else a NotSerializableException will be thrown.
The JVM associates a version (long) number with each serializable class. We use it to verify that the saved and loaded objects have the same attributes, and thus are compatible on serialization.
https://www.baeldung.com/java-serialization

when to use component scan in spring boot
https://reflectoring.io/spring-component-scanning/


isolation levels in database?
default isolation level in Oracle
how is transaction handling done in spring application..explain through isolation level
how is transaction handling done in mciroservices
outbox pattern in microservices
Cred pattern
singleton bean with prototype bean how to handle
BACK END job heavy opeartion and DB is locked and client not able to pull reports - write instance and read replicas..how to ensure replication between write instance and read replicas

scopes of spring beans - singleton, prototype, request, session, application
features of java 8 each in detail - stream processing, lambda expressions, method references, default methods, optionsl class, metaspace and permgenspace, local data and instant
what is collections framework - arraylist, linkedlist, hashset, treeset
diff between primary and qualifier annotation
how is exception handling done in spring boot- on each controller class and on global level, controller advice annotation
what all do you know about design patterns
serialization and deserialization

Why string is immutable , is it possible to write our own immutable class? - string is immutable as we are using that to storename username, password and being immutable, they are thread safe, so another thread can not change its value in between. Secondly, JVM does string interning allocating single memory for a copy of a string literal in the string pool. This helps in memory optimization and performance improvement and cached values are returned.  https://www.baeldung.com/java-string-immutable#:~:text=Being%20immutable%20automatically%20makes%20the,across%20multiple%20threads%20running%20simultaneously.
difference between functional programming and object oriented programming?
What is the real time scenario , where you are using the circuit breaker ? - https://medium.com/geekculture/design-patterns-for-microservices-circuit-breaker-pattern-276249ffab33
How you previously handling microservices deployment?
How you implementing and validating the JWT token ? - see above and also https://www.viralpatel.net/java-create-validate-jwt-token/
Which one is best constructor injection or Setter injection?
Have you configured saga design pattern in your project?
Write own singleton class and secure it in multi threading environment?
Write code to find repeated characters in string with count using java 8?
Get third highest salary in given employee list use java 8?
In microservices environment , if we deploy in cloud for every instance port number always changing right . So, how you going to call your instance int his case ?
How you securing your microservices? - use https, secure your secrets, use oauth client crdentails flow, write secure code https://www.okta.com/resources/whitepaper/8-ways-to-secure-your-microservices-architecture/
Have you involved in CI/CD process ?
Do you know about orchestration and choreography design patterns?
Why we need Autowire in Spring ?
Difference between @Configuration and @Component annotations?
Can you tell me what are the design patterns you worked?
How to prevent sql injection?
Find count of pairs whose sum is k in an array

https://javadevelopersguide.blogspot.com/2014/08/tavant-interview-questions.html

In Java, a covariant return type allows a subclass method to return a more specific type than the return type of the overridden method in the superclass. This means that if a method is overridden in a subclass, then the return type of the overriding method can be a subtype of the return type of the overridden method.